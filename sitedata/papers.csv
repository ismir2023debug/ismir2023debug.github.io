uid,day,session,position,paper_presentation,long_presentation,title,abstract,primary_author,primary_email,authors_and_affil,authors,author_emails,primary_subject,secondary_subject,SpecialTrack,abstract_short,StudentAuthor,StudentAuthor,AwardNominee,pdf_name,pdf_path,video,poster_pdf,thumbnail,slides_pdf,channel_url,slack_channel
103,1,1,0,Virtually,TRUE,Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model,"Lyric interpretations can help people understand songs and their lyrics quickly, and can also make it easier to manage, retrieve and discover songs efficiently from the growing mass of music archives. In this paper we propose BART-fusion, a novel model for generating lyrics interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder. We employ a cross-modal attention module to incorporate the audio representation into the lyrics representation to help the pre-trained language model understand the song from an audio perspective, while preserving the language model's original generative performance. We also release the Song Interpretation Dataset, a new large-scale dataset for training and evaluating our model. Experimental results show that the additional audio information helps our model to understand words and music better, and to generate precise and fluent interpretations. An additional experiment on cross-modal music retrieval shows that interpretations generated by BART-fusion can also help people retrieve music more accurately than with the original BART.",Yixiao Zhang,ldzhangyx@outlook.com,"Yixiao Zhang (Centre for Digital Music, Queen Mary University of London)*; Junyan Jiang (Music X Lab, NYU Shanghai, MBZUAI); Gus Xia (Music X Lab, NYU Shanghai, MBZUAI); Simon Dixon (Centre for Digital Music, Queen Mary University of London)","Zhang, Yixiao*; Jiang, Junyan; Xia, Gus; Dixon, Simon",ldzhangyx@outlook.com*; jj2731@nyu.edu; gxia@nyu.edu; s.e.dixon@qmul.ac.uk,MIR fundamentals and methodology -> lyrics and other textual data,"Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",No,"In this paper we propose BART-fusion, a novel model for generating lyrics interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder.",Yes,Yes,Yes,000001.pdf,https://archives.ismir.net/ismir2022/paper/000001.pdf,https://drive.google.com/open?id=1jjVL9ryCaUaHxdKU6uA51OSK5KuHajsk,https://drive.google.com/open?id=12pEH1F8X97iyUyzkvMtSxGBxPmvp5u89,https://drive.google.com/open?id=1RX-gNH83IsTWPWhmrm8MbkJG1UMlMNTO,,https://slack.com/app_redirect?channel=C04CY0MCR4H,p1-01-zhang
46,1,1,1,Virtually,FALSE,Toward postprocessing-free neural networks for joint beat and downbeat estimation,"Recent deep learning-based models for estimating beats and downbeats are mainly composed of three successive stages---feature extraction, sequence modeling, and post processing. While such a framework is prevalent in the scenario of sequence labeling tasks and yields promising results in beat and downbeat estimations, it also indicates a shortage of the employed neural networks, given that the post-processing usually provides a notable performance gain over the previous stage. Moreover, the assumption often made for the post-processing is not suitable for many musical pieces. In this work, we attempt to improve the performance of joint beat and downbeat estimation without incorporating the post-processing stage. By inspecting a state-of-the-art approach, we propose reformulations regarding the network architecture and the loss function. We evaluate our model on various music data and show that the proposed methods are capable of improving the baseline approach without the aid of a post-processing stage.",Tsung-Ping Chen,tearfulcanon@iis.sinica.edu.tw,"Tsung-Ping Chen (Institute of Information Science, Academia Sinica, Taiwan)*; Li Su (Institute of Information Science, Academia Sinica, Taiwan)","Chen, Tsung-Ping*; Su, Li",tearfulcanon@iis.sinica.edu.tw*; lisu@iis.sinica.edu.tw,Musical features and properties,"Musical features and properties -> rhythm, beat, tempo",No,"We propose a post-processing-free deep learning model to tackle the joint beat and downbeat estimation task, and achieve a state-of-the-art performance on the Ballroom dataset.",No,Yes,No,000002.pdf,https://archives.ismir.net/ismir2022/paper/000002.pdf,https://drive.google.com/open?id=1NUkdMAyYMortu8ssU8WwasTMUPt7zEby,https://drive.google.com/open?id=1z_cJqeicGRJRs3ET12i1LK9mgcliKCKH,https://drive.google.com/open?id=1KPYQUt8mo5JhaiX287vyitUPdF_xb7TG,,https://slack.com/app_redirect?channel=C04CMQU7CPN,p1-02-chen
37,1,1,2,Virtually,FALSE,Music Translation: Generating Piano Arrangements in Different Playing Levels,"We present a novel task of ""playing level conversion"": generating a music arrangement in a target difficulty level, given another arrangement of the same musical piece in a different level. For this task, we create a parallel dataset of piano arrangements in two strictly well-defined playing levels, annotated at individual phrase resolution, taken from the song catalog of a piano learning app.

In a series of experiments, we train models that successfully modify the playing level while preserving the musical 'essence'. We further show, via an ablation study, the contributions of specific data representation and augmentation techniques to the model's performance.

In order to evaluate the performance of our models, we conduct a human evaluation study with expert musicians. The evaluation shows that our best model creates arrangements that are almost as good as ground truth examples. Additionally, we propose MuTE, an automated evaluation metric for music translation tasks, and show that it correlates with human ratings.",Matan Gover,matangover@gmail.com,Matan Gover (Simply)*; Oded Zewi (Simply),"Gover, Matan*; Zewi, Oded",matangover@gmail.com*; oded@joytunes.com,MIR tasks -> music generation,Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> musical style and genre,No,We trained a model to re-arrange piano songs to a different playing level while preserving the musical essence. Experts musicians rated the outputs of our model to be almost as good as ground truth reference examples.,No,Yes,No,000003.pdf,https://archives.ismir.net/ismir2022/paper/000003.pdf,https://drive.google.com/open?id=1vnrRdzLG0EjjbSHkv0fR7DMy1O-STkVA,https://drive.google.com/open?id=1Pj2q7_K2PhZQKvYemSIyLKh6or6d2V4w,https://drive.google.com/open?id=1zbK7SGgSxTlJXBJ4G1zV_O4kZU8hXM15,https://docs.google.com/presentation/d/14AemWMvDbMHhaQ2jFDxfzjzHFfhaHiFL6ZpNHncDsps/edit?usp=sharing,https://slack.com/app_redirect?channel=C04D92FRX5W,p1-03-gover
287,1,1,3,Virtually,FALSE,Scaling Polyphonic Transcription with Mixtures of Monophonic Transcriptions,"Automatic Music Transcription (AMT), in particular the problem of automatically extracting notes from audio, has seen much recent progress via the training of neural network models on musical audio recordings paired with aligned ground-truth note labels.  However, progress is currently limited by the difficulty of obtaining such note labels for natural audio recordings at scale.  In this paper, we take advantage of the fact that for monophonic music, the transcription problem is much easier and largely solved via modern pitch-tracking methods.  Specifically, we show that we are able to combine recordings of real monophonic music (and their transcriptions) into artificial and musically-incoherent mixtures, greatly increasing the scale of labeled training data.  By pretraining on these mixtures, we can use a larger neural network model and significantly improve upon the state of the art in multi-instrument polyphonic transcription.  We demonstrate this improvement across a variety of datasets and in a ``zero-shot'' setting where the model has not been trained on any data from the evaluation domain.",Ian Simon,iansimon@google.com,"Ian Simon (Google Research, Brain Team)*; Joshua Gardner (Google Research, Brain Team); Curtis Hawthorne (Google Research, Brain Team); Ethan Manilow (Google Research, Brain Team); Jesse Engel (Google Research, Brain Team)","Simon, Ian*; Gardner, Joshua; Hawthorne, Curtis; Manilow, Ethan; Engel, Jesse",iansimon@google.com*; jpgard@cs.washington.edu; fjord@google.com; ethanm@u.northwestern.edu; jesseengel@google.com,MIR tasks -> music transcription and annotation,"Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing; MIR fundamentals and methodology -> web mining, and natural language processing",No,We improve upon the state of the art in multi-instrument music transcription by pretraining a large neural network model on mixtures of automatically-transcribed monophonic recordings from the wild.,No,Yes,No,000004.pdf,https://archives.ismir.net/ismir2022/paper/000004.pdf,https://drive.google.com/open?id=1mhRr6iciJlhkZpDOsaNCVhKc-SdmjM3Z,https://drive.google.com/open?id=1y1pEnQ0xy_xo5cIoRLbTgXOy9wbRc57P,https://drive.google.com/open?id=1-5-gfWb2dYm6A7jFDjTyZ5t8qLXPwtvx,,https://slack.com/app_redirect?channel=C04CK88202F,p1-04-simon
100,1,1,4,"In-person, in Bengaluru",FALSE,Attention-based audio embeddings for query-by-example,"An ideal audio retrieval system efficiently and robustly recognizes a short query snippet from an extensive database. However, the performance of well-known audio fingerprinting systems falls short at high signal distortion levels. This paper presents an audio retrieval system that generates noise and reverberation robust audio fingerprints using the contrastive learning framework. Using these fingerprints, the method performs a comprehensive search to identify the query audio and precisely estimate its timestamp in the reference audio. Our framework involves training a CNN to maximize the similarity between pairs of embeddings extracted from clean audio and its corresponding distorted and time-shifted version. We employ a channel-wise spectral-temporal attention mechanism to capture salient time indices and spectral bands in the CNN features. The attention mechanism enables the CNN to better discriminate the audio by giving more weight to the salient spectral-temporal patches in the signal. Experimental results indicate that our system is efficient in computation and memory usage while being more accurate, particularly at higher distortion levels, than competing state-of-the-art systems and scalable to a larger database. ",Anup Singh,anup.singh@ugent.be,"Anup Singh (IDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium, Department of Electrical Engineering, Indian Institute of Technology Kanpur, India)*; Kris Demuynck (IDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium); Vipul Arora (Department of Electrical Engineering, Indian Institute of Technology Kanpur, India)","Singh, Anup*; Demuynck, Kris; Arora, Vipul",anup.singh@ugent.be*; Kris.Demuynck@UGent.be; vipular@iitk.ac.in,MIR tasks -> fingerprinting,MIR tasks -> indexing and querying,No,"CNN enhanced by spectral-temporal attention mechanism generates discriminative robust audio embeddings, useful for query-by-example applications.",Yes,Yes,No,000005.pdf,https://archives.ismir.net/ismir2022/paper/000005.pdf,https://drive.google.com/open?id=1G96VBsQfpdqjnGhdFlj8ZA1U0l3D8q_d,https://drive.google.com/open?id=1LzpJd3xJpw2Nv5S4kOCbpkvupENNH9_9,https://drive.google.com/open?id=12mad31jT1ycN8UOpq0lWeMPrVI80ezo3,https://docs.google.com/presentation/d/1yYEjTjfA8erqyyyAS0j-PC-qVZFhiDjn_fzhu27hrOA/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CMQUQM8U,p1-05-singh
61,1,1,5,Virtually,FALSE,SIATEC-C: Computationally efficient repeated pattern discovery in polyphonic music,"The use of point-set representations of music enable repeated pattern discovery
to be performed on polyphonic music. The discovery of patterns containing polyphony is also enabled by the use of point-set representations. The SIA and SIATEC algorithms discover repeated patterns in point-sets by
computing maximal translatable patterns and their translational equivalence classes.
While the algorithms are relatively efficient, their application to larger pieces
of music is not viable due to quadratic space complexity.
This paper introcudes a novel algorithm, SIATEC-C, for repeated pattern discovery in point-set representations of music. The algorithm discovers repeated patterns and finds all of their occurrences, while
running with subquadratic space complexity. The algorithm can also provide significant running
time improvements over the comparable SIATEC algorithm.
The computational performance of the algorithm is compared with SIATEC. The accuracy of the algorithm
is also evaluated on the JKU-PDD data set.",Otso Björklund,otso.bjorklund@helsinki.fi,Otso Björklund (University of Helsinki)*,"Björklund, Otso*",otso.bjorklund@helsinki.fi*,MIR tasks -> pattern matching and detection,MIR fundamentals and methodology -> symbolic music processing,No,A novel algorithm for computationally efficient repeated pattern discovery in polyphonic symbolic music that enables larger data sets than before.,Yes,No,No,000006.pdf,https://archives.ismir.net/ismir2022/paper/000006.pdf,https://drive.google.com/open?id=1TsSHHxL4DJ2OuYyUyDpVkF1CwwEj2Lt8,https://drive.google.com/open?id=1DFqkRIunJGAHGYWIAIbJrIwg0dLNPaC2,https://drive.google.com/open?id=1aKycjVCV_TI_2mlIJ5cD7RFsFhXAwaZR,,https://slack.com/app_redirect?channel=C04CKB84P4J,p1-06-björklund
109,1,1,6,"In-person, in Bengaluru",FALSE,Tailed U-Net: Multi-Scale Music Representation Learning,"Self-supervised learning has steadily been gaining traction in recent years. In music information retrieval (MIR), one promising recent application of self-supervised learning is the CLMR framework (contrastive learning of musical representations). CLMR has shown good performance, achieving results on par with state-of-the-art end-to-end classification models, but it is strictly an encoding framework. It suffers the characteristic limitation of any encoder that it cannot explicitly combine multi-timescale information, whereas a characteristic feature of human audio perception is that we tend to perceive all frequencies simultaneously. To this end, we propose a generalization of CLMR that learns to extract and explicitly combine representations across different frequency resolutions, which we coin the tailed U-Net (TUNe). TUNe architectures combine multi-timescale information during a decoding phase, similar to U-Net architectures used in computer vision and source separation, but have a tail added to reduce sample-level information to a smaller pre-defined number of representation dimensions. The size of the decoding phase is a hyperparameter, and in the case of a zero-layer decoding phase, TUNe reduces to CLMR. The best TUNe architectures, however, require less training time to match CLMR performance, have superior transfer learning performance, and are competitive with state-of-the-art models even at dramatically reduced dimensionalities.
",Marcel A Vélez Vásquez,marcel.velezv@gmail.com,"Marcel A Vélez Vásquez (Music Cognition Group · Institute for Logic, Language, and Computation · University of Amsterdam)*; John Ashley Burgoyne (Music Cognition Group · Institute for Logic, Language, and Computation · University of Amsterdam)","Vélez Vásquez, Marcel A*; Burgoyne, John Ashley",marcel.velezv@gmail.com*; j.a.burgoyne@uva.nl,Domain knowledge -> machine learning/artificial intelligence for music,"Domain knowledge -> representations of music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> musical affect, emotion and mood; Musical features and properties -> musical style and genre; Musical features and properties -> representations of music",No,"We propose TUNe, a generalization of the CLMR framework for music representation learning, inspired by the U-Net architectures used for source separation, that includes a decoding phase in addition to the CLMR encoding steps, thereby enabling multi-time-scale integration and better parameter efficiency without seriously impacting state-of-the-art performance.",Yes,Yes,No,000007.pdf,https://archives.ismir.net/ismir2022/paper/000007.pdf,https://drive.google.com/open?id=1kQbuoxXuO1i1oOevGfvLLGJxYzmzx346,https://drive.google.com/open?id=1Gi0Zi7lOvfBM8EVNHvHJ6pUh9VXiwEs7,https://drive.google.com/open?id=1DW5rwje8Zjy9MlVC_cYzi7FBnS8GXYLX,https://drive.google.com/open?id=1c6XiB8VqfQMPFyocInoigXpBjRQgR1wk,https://slack.com/app_redirect?channel=C04CCP2DG5C,p1-07-vásquez
85,1,1,7,Virtually,FALSE,DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation,"A vocoder is a conditional audio generation model that converts acoustic features such as mel-spectrograms into waveforms. Taking inspiration from Differentiable Digital Signal Processing (DDSP), we propose a new vocoder named SawSing for singing voices. SawSing synthesizes the harmonic part of singing voices by filtering a sawtooth source signal with a linear time-variant finite impulse response filter whose coefficients are estimated from the input mel-spectrogram by a neural network. As this approach enforces phase continuity, SawSing can generate singing voices without the phase-discontinuity glitch of many existing vocoders. Moreover, the source-filter assumption provides an inductive bias that allows SawSing to be trained on a small amount of data. Our evaluation shows that SawSing converges much faster and outperforms state-of-the-art generative adversarial network- and diffusion-based vocoders in a resource-limited scenario with only 3 training recordings and a 3-hour training time.",Da-Yi Wu,ericwudayi2@gmail.com,"Da-Yi Wu (Academia Sinica)*; Wen-Yi Hsiao (Taiwan AI Labs); Fu-Rong Yang (National Tsing Hua University); Oscar D Friedman (470 Music Group); Warren Jackson (PARC); scott bruzenak (470 Music Group); Yi-Wen  Liu (National Tsing Hua University); Yi-Hsuan Yang (Academia Sinica, Taiwan AI Labs)","Wu, Da-Yi*; Hsiao, Wen-Yi; Yang, Fu-Rong; Friedman, Oscar D; Jackson, Warren; bruzenak, scott; Liu, Yi-Wen ; Yang, Yi-Hsuan",ericwudayi2@gmail.com*; s101062219@gmail.com; fjbcrs34@gmail.com; oscar@foursevenzero.com; jackson@parc.com; scott@noisecastleiii.com; ywliu@ee.nthu.edu.tw; yang@citi.sinica.edu.tw,MIR tasks -> music synthesis and transformation,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music generation; Musical features and properties -> timbre, instrumentation, and singing voice",No,"Based on differentiable digital signal processing, we propose a lightweight and training-efficient sawtooth-based substractive singing vocoder that can reconstruct the audio waveform from a mel spectrogram with comparable quality to state-of-the-art neural network-based vocoders.",No,No,No,000008.pdf,https://archives.ismir.net/ismir2022/paper/000008.pdf,https://drive.google.com/open?id=1SDxWncOrKS39FNOKv09dnFGFUWN_gLZp,https://drive.google.com/open?id=1C7YKJs6kRSEAV9QHikcDd2yPpMgeqVMo,https://drive.google.com/open?id=1cf0bz_-8AOJ3ylqDPbXmvkIocK0QZgtP,,https://slack.com/app_redirect?channel=C04C4QPEXC7,p1-08-wu
117,1,1,8,"In-person, in Bengaluru",FALSE,Equivariant self-supervision for musical tempo estimation,"Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. 
Contrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. 
In this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling.
Our experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. 
As an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community.",Elio Quinton,elio.quinton@gmail.com,Elio Quinton (Universal Music Group)*,"Quinton, Elio*",elio.quinton@gmail.com*,Musical features and properties -> representations of music,"Musical features and properties -> rhythm, beat, tempo",No,"In this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data.",No,No,No,000009.pdf,https://archives.ismir.net/ismir2022/paper/000009.pdf,https://drive.google.com/open?id=1vhS9TpAf5qRwlP6rJXTbrYkZcB6D1TqZ,https://drive.google.com/open?id=1V7heVpCGbctSX1GJA-TOQWPIH5MQ0OKj,https://drive.google.com/open?id=1PQ7_Ir1nW8-Z13OwBIRHVpqm_huprFNl,https://drive.google.com/open?id=1ZCUvEzuQjkw-5Sk5HfkxDfyvrwTK4Ere,https://slack.com/app_redirect?channel=C04CGCUJQJZ,p1-09-quinton
90,1,1,9,Virtually,FALSE,How Music features and Musical Data Representations Affect Objective Evaluation of Music Composition: A Review of CSMT Data Challenge 2020,"Tools and methodologies for distinguishing computer-generated melodies from human-composed melodies have a broad range of applications from detecting copyright infringement through the evaluation of generative music systems to facilitating transparent and explainable AI. This paper reviews a data challenge on distinguishing computer-generated melodies from human-composed melodies held in association with the Conference on Sound and Music Technology (CSMT) in 2020. An investigation of the submitted systems and the results are presented first. Besides the structure of the proposed models, the paper investigates two important factors that were identified as contributors to good model performance: the specific music features and the music representation used. Through an analysis of the submissions, important melody-related music features have been identified. Encoding or representation of the music in the context of neural network modes has also been found to significantly impact system performance through an experiment where the top-ranked system was re-implemented with different input representations for comparison purposes. Besides demonstrating the feasibility of developing an objective music composition evaluation system, the investigation presented in this paper also reveals some important limitations of current music composition systems opening opportunities for future work in the community.",Yuqiang Li,Yuqiang.li19@student.xjtlu.edu.cn,Yuqiang Li (Xi’an Jiaotong-Liverpool University)*; Shengchen Li (Xi’an Jiaotong-Liverpool University); George Fazekas (Queen Mary University of London),"Li, Yuqiang*; Li, Shengchen; Fazekas, George",Yuqiang.li19@student.xjtlu.edu.cn*; Shengchen.li@xjtlu.edu.cn; george.fazekas@qmul.ac.uk,"Evaluation, datasets, and reproducibility -> evaluation methodology","Evaluation, datasets, and reproducibility -> evaluation metrics; Musical features and properties -> melody and motives; Musical features and properties -> representations of music; Musical features and properties -> rhythm, beat, tempo",No,Pitch-based music features and appropriate input representation are vital in developing neural network models for identifying computer-generated melodies.,Yes,Yes,No,000010.pdf,https://archives.ismir.net/ismir2022/paper/000010.pdf,https://drive.google.com/open?id=1Z3JKyr8si0qTBYch4VK9dQyUEBvY3iJa,https://drive.google.com/open?id=1BJ40v5h1FPGquMXF7c2VVHm2P0EM98R1,https://drive.google.com/open?id=1E2V2AqnYPwxxlPCA_h8p3ia0vGnavqQN,,https://slack.com/app_redirect?channel=C04CK86BGAX,p1-10-li
153,1,1,10,"In-person, in Bengaluru",FALSE,YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations,"Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.",Eunjin Choi,jech@kaist.ac.kr,"Eunjin Choi (Graduate School of Culture Technology, KAIST, South Korea)*; Yoonjin Chung (Graduate School of AI, KAIST, South Korea); Seolhee Lee (Graduate School of Culture Technology, KAIST, South Korea); JongIk Jeon (Department of Industrial Design, KAIST, South Korea); Taegyun Kwon (Graduate School of Culture Technology, KAIST, South Korea); Juhan Nam (Graduate School of Culture Technology, KAIST, South Korea)","Choi, Eunjin*; Chung, Yoonjin; Lee, Seolhee; Jeon, JongIk; Kwon, Taegyun; Nam, Juhan",jech@kaist.ac.kr*; yoonjin.chung@kaist.ac.kr; seolhee_lee@kaist.ac.kr; matji@kaist.ac.kr; ilcobo2@kaist.ac.kr; juhan.nam@kaist.ac.kr,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Applications -> gaming, augmented/virtual reality; Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation; Musical features and properties -> musical affect, emotion and mood",No,We propose new emotion tag-annotated multi-instrumental symbolic game music dataset.,Yes,Yes,No,000011.pdf,https://archives.ismir.net/ismir2022/paper/000011.pdf,https://drive.google.com/open?id=1J4-anjMGCpgeGR5zPPkuiTeZpfr7hj6E,https://drive.google.com/open?id=15qp-jjYoLk1RKK0o7SyFhcNArFXNgGCc,https://drive.google.com/open?id=1EwEe1u1LnfB5mpcws3bQQTg7zADEAupl,https://docs.google.com/presentation/d/1YKM2Kilw1iBoMgeZ1QukO062zx-8rzl8jM6HgfRnnaA/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CKB936LS,p1-11-choi
133,1,1,11,Virtually,FALSE,Detecting Symmetries of All Cardinalities With Application to Musical 12-Tone Rows,"Popularized by Arnold Schoenberg in the mid-20th century, the method of twelve-tone composition produces musical compositions based on one or more orderings of the equal-tempered chromatic scale. The work of twelve-tone composers is famously challenging to traditional Western tonal and structural sensibilities; even so, group theoretic approaches have determined that 10% of certain composers’ works contain a highly unusual classical symmetry of music. We extend this result by revealing many symmetries that were previously undetected in the works of Schoenberg, Webern, and Berg. Our approach is computational rather than group theoretic, scanning each composition for symmetries of many different cardinalities. Thus, we capture partial symmetries that would be overlooked by more formal means. Moreover, our methods are applicable beyond the narrow scope of twelve-tone composition. We achieve our results by first extending the group-theoretic notion of symmetry to encompass shorter motives that may be repeated and reprised in a given composition, and then comparing the incidence of these symmetries between the work of composers and the space of all possible 12-tone rows. We present four candidate hierarchies of symmetry and show that in each model, between 75% and 95% of actual compositions contained high levels of internal symmetry.",Anil Venkatesh,avenkatesh@adelphi.edu,"Anil Venkatesh (Department of Mathematics and Computer Science, Adelphi University)*; Viren Sachdev (Department of Mathematics and Computer Science, Adelphi University)","Venkatesh, Anil*; Sachdev, Viren",avenkatesh@adelphi.edu*; virensachdev@mail.adelphi.edu,Domain knowledge -> computational music theory and musicology,"Domain knowledge -> representations of music; Musical features and properties -> melody and motives; Musical features and properties -> structure, segmentation, and form",No,"Use computational methods to scan a composition for symmetry at many different resolutions, then assemble these measurements into a single robust symmetry score.",No,Yes,No,000012.pdf,https://archives.ismir.net/ismir2022/paper/000012.pdf,https://drive.google.com/open?id=1nfO9PGQU-rShoT5ZfgvQsNdoCzVDrR-B,https://drive.google.com/open?id=1jmL7YfNTUBFOB6Et1wSw_5Qxs15U-Cx5,https://drive.google.com/open?id=1Mq_kbHOiQTUVRLRpZRpABkPThOrrMHi3,,https://slack.com/app_redirect?channel=C04D92GPTL0,p1-12-venkatesh
234,1,1,12,"In-person, in Bengaluru",FALSE,The power of deep without going deep? A study of HDPGMM music representation learning,"In the previous decade, Deep Learning (DL) has proven to be one of the most effective machine learning methods to tackle a wide range of Music Information Retrieval (MIR) tasks. It offers highly expressive learning capacity that can fit any music representation needed for MIR-relevant downstream tasks. However, it has been criticized for sacrificing interpretability. On the other hand, the Bayesian nonparametric (BN) approach promises similar positive properties as DL, such as high flexibility, while being robust to overfitting and preserving interpretability. Therefore, the primary motivation of this work is to explore the potential of Bayesian nonparametric models in comparison to DL models for music representation learning. More specifically, we assess the music representation learned from the Hierarchical Dirichlet Process Gaussian Mixture Model (HDPGMM), an infinite mixture model based on the Bayesian nonparametric approach, to MIR tasks, including classification, auto-tagging, and recommendation. The experimental result suggests that the HDPGMM music representation can outperform DL representations in certain scenarios, and overall comparable.",Jaehun Kim,j.h.kim@tudelft.nl,Jaehun Kim (Delft University of Technology)*; Cynthia C. S. Liem (Delft University of Technology),"Kim, Jaehun*; Liem, Cynthia C. S.",j.h.kim@tudelft.nl*; c.c.s.liem@tudelft.nl,Musical features and properties -> representations of music,"Applications -> music recommendation and playlist generation; Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> MIR tasks; MIR tasks -> automatic classification; MIR tasks -> similarity metrics",No,"Bayesian nonparametric models, such as the HDPGMM, can learn an effective music representation that can outperform deep-learned representations in certain scenarios and is overall comparable.",No,Yes,No,000013.pdf,https://archives.ismir.net/ismir2022/paper/000013.pdf,https://drive.google.com/open?id=1OKpPNQA3fv3oE7wp6YtMAxaTGTGYPFHu,https://drive.google.com/open?id=1fR3Y9ikhCmtYhWan0_kIqEyZmhpQr-ul,https://drive.google.com/open?id=1iv7g-YI6ZV6UgbRa8neYU3Hgj0lHpEVR,https://docs.google.com/presentation/d/1PdDRilp3BOxaG2xSn2U9GQFF52kwg9GqkyZZVhCgf3k/edit?usp=sharing,https://slack.com/app_redirect?channel=C04C4QQLQHM,p1-13-kim
322,1,1,13,Virtually,FALSE,Pop Music Generation with Controllable Phrase Lengths,"Research on music generation using deep learning has attracted more attention; in particular, Transformer-based models have succeeded in generating coherent musical pieces. Recently, an increasing number of studies have focused on phrases that are smaller musical units, and several studies have addressed phrase-level control. In this study, we propose a method for sequentially generating a piece that enables the control of each phrase length and, consequently, the length of the entire piece. We added PHRASE and a new event, BAR COUNTDOWN, which indicates the number of bars remaining in the phrase, to the existing event-based music representations. To reflect user input indicating the phrase lengths of the piece being generated, we used an autoregressive generation model that adds these two events to the generated event-token sequence based on the user input and uses it as input for the next time step. Subjective listening tests revealed that the pieces generated by our methods possessed designated phrase lengths and ended naturally at the determined length.",Daiki Naruse,naruse@mi.t.u-tokyo.ac.jp,"Daiki Naruse (The University of Tokyo, Japan)*; Tomoyuki Takahata (The University of Tokyo, Japan); Yusuke Mukuta (The University of Tokyo, Japan, RIKEN, Japan); Tatsuya Harada (The University of Tokyo, Japan, RIKEN, Japan)","Naruse, Daiki*; Takahata, Tomoyuki; Mukuta, Yusuke; Harada, Tatsuya",naruse@mi.t.u-tokyo.ac.jp*; takahata@mi.t.u-tokyo.ac.jp; mukuta@mi.t.u-tokyo.ac.jp; harada@mi.t.u-tokyo.ac.jp,MIR tasks -> music generation,"Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; Musical features and properties -> representations of music; Musical features and properties -> structure, segmentation, and form",No,We propose music generation models that can control the length of each phrase and the entire piece.,Yes,Yes,No,000014.pdf,https://archives.ismir.net/ismir2022/paper/000014.pdf,https://drive.google.com/open?id=1Vzij-z2_GQnhf2pjWhJFr8Abf8Zyel5o,https://drive.google.com/open?id=17QxnZvs4cgAwXZXQlFXEWV40zKsY2Y8T,https://drive.google.com/open?id=1_nK4C4awvigUxpDU74xagmolynRW1Wac,,https://slack.com/app_redirect?channel=C04CK8884BD,p1-14-naruse
169,1,1,14,"In-person, in Bengaluru",FALSE,Exploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation,"While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",Yen-Tung  Yeh,ytsrt66589@gmail.com,"Yen-Tung  Yeh (Academia Sinica, National Taiwan University)*; Yi-Hsuan Yang (Academia Sinica, Taiwan AI Labs); Bo-Yu Chen (Academia Sinica, National Taiwan University)","Yeh, Yen-Tung *; Yang, Yi-Hsuan; Chen, Bo-Yu",b06611042@ntu.edu.tw*; yang@citi.sinica.edu.tw; bernie40916@gmail.com,MIR tasks -> music generation,Domain knowledge -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation; Musical features and properties -> representations of music,No,We find that using a general audio classifier to constrain the feature space of the discrminator by the idea of Projected GAN holds the promise of improving the training of GAN-based deep generative models for music.,Yes,Yes,No,000015.pdf,https://archives.ismir.net/ismir2022/paper/000015.pdf,https://drive.google.com/open?id=1CtRVNZ7FPPKD_GEB_jMdS0gh4EFEphoZ,https://drive.google.com/open?id=1K56Pf8bBlUZ7R0iCX1gR6vDM1-iz4c_G,https://drive.google.com/open?id=1YwOACtnBvrwrXbLhSNyfaqrUh2AQcSoq,https://drive.google.com/open?id=18REiXkw6NMEcehaok7qlq8NktLUHQ4Wm,https://slack.com/app_redirect?channel=C04CMQVEKSQ,p1-15-yeh
306,1,1,15,"In-person, in Bengaluru",FALSE,Modeling the rhythm from lyrics for melody generation of pop songs,"Creating a pop song melody according to pre-written lyrics is a typical practice for composers. A computational model of how lyrics are set as melodies is important for automatic composition systems, but an end-to-end lyric-to-melody model would require enormous amounts of paired training data. To mitigate the data constraints, we adopt a two-stage approach, dividing the task into lyric-to-rhythm and rhythm-to-melody modules. However, the lyric-to-rhythm task is still challenging due to its multimodality. In this paper, we propose a novel lyric-to-rhythm framework that includes part-of-speech tags to achieve better text-setting, and a Transformer architecture designed to model long-term syllable-to-note associations. For the rhythm-to-melody task, we adapt a proven chord-conditioned melody Transformer, which has achieved state-of-the-art results. Experiments for Chinese lyric-to-melody generation show that the proposed framework is able to model key characteristics of rhythm and pitch distributions in the dataset, and in a subjective evaluation, the melodies generated by our system were rated as similar to or better than those of a state-of-the-art alternative.",daiyu zhang,daiyu.zhang@bytedance.com,Daiyu Zhang (ByteDance)*; Ju-Chiang Wang (ByteDance); Katerina Kosta (ByteDance); Jordan B. L. Smith (ByteDance); Shicen Zhou (ByteDance),"zhang, daiyu*; Wang, Ju-Chiang; Kosta, Katerina; Smith, Jordan B. L.; Zhou, Shicen",daiyu.zhang@bytedance.com*; ju-chiang.wang@bytedance.com; katerina.kosta@bytedance.com; jordan.smith@bytedance.com; zhoushicen@bytedance.com,MIR tasks -> music generation,MIR fundamentals and methodology -> multimodality,No,"We study the lyrics-to-melody generation task and propose a novel framework to improve lyrics-to-rhythm sub-task using part-of-speech information for text-setting, and a Compound Word representation for the note encoder, relative attention as transformer decoder.",No,Yes,No,000016.pdf,https://archives.ismir.net/ismir2022/paper/000016.pdf,https://drive.google.com/open?id=1_MyDbDMkhm5zguPcbYdhhixKbvPi690L,https://drive.google.com/open?id=1wbQvC1h9DCL0xK5fEa5Ta7DAIXlxYuPv,https://drive.google.com/open?id=1Uc9UyW1mNMb1_plCea6YzIvJvD547x2f,https://docs.google.com/presentation/d/14LMYWAaZawFgwrLx7bXWL9724SFP5sx0BSjcHq6IkkU/edit?usp=sharing,https://slack.com/app_redirect?channel=C04C4QRA7T9,p1-16-zhang
217,1,2,0,"In-person, in Bengaluru",TRUE,Visualization for AI-Assisted Composing,"We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers.",Simeon Rau,simeon.rau@visus.uni-stuttgart.de,"Simeon Rau (VISUS, University of Stuttgart, Germany)*; Frank Heyen (VISUS, University of Stuttgart, Germany); Stefan Wagner (ISTE, University of Stuttgart, Germany) ; Michael Sedlmair (VISUS, University of Stuttgart, Germany)","Rau, Simeon*; Heyen, Frank; Wagner, Stefan; Sedlmair, Michael",Simeon.Rau@visus.uni-stuttgart.de*; frank.heyen@visus.uni-stuttgart.de; stefan.wagner@iste.uni-stuttgart.de; Michael.Sedlmair@visus.uni-stuttgart.de,Human-centered MIR,Applications -> music composition; Human-centered MIR -> music interfaces and services; MIR tasks -> similarity metrics; Musical features and properties -> representations of music,No,Visualization can support AI-assisted composition.,Yes,No,Yes,000017.pdf,https://archives.ismir.net/ismir2022/paper/000017.pdf,https://drive.google.com/open?id=1iwMm4pJoaaX9CiFzRMWt6gisY-ixIs1H,https://drive.google.com/open?id=1b1B4uejvzqP1k9azq7kPUpTVrqYEHmEH,https://drive.google.com/open?id=1cUT34kayLNSRJwp3_wya0FXUBIXHx7x_,https://drive.google.com/open?id=1FqRqwKOq8aPOPiJ3xCUkLHUicNL8FowR,https://slack.com/app_redirect?channel=C04CKB9G3AN,p2-01-rau
305,1,2,1,Virtually,FALSE,Retrieving musical information from neural data: how cognitive features enrich acoustic ones,"Various features – from low-level acoustics, to higher-level statistical regularities, to memory associations – contribute to the experience of musical enjoyment and pleasure. Recent work suggests that musical surprisal, that is, the unexpectedness of a musical event given its context, may directly predict listeners’ experiences of pleasure and enjoyment during music listening. Understanding how surprisal shapes listeners’ preferences for certain musical pieces has implications for music recommender systems, which are typically content- (both acoustic or semantic) or metadata-based. Here we test a recently developed computational algorithm, called Dynamic-Regularity Extraction (D-REX), that uses Bayesian inference to predict the surprisal that humans experience while listening to music. We demonstrate that the brain tracks musical surprisal as modeled by D-REX by conducting a decoding analysis on the neural signal (collected through magnetoencephalography) of participants listening to music. Thus, we demonstrate the validity of a computational model of musical surprisal, which may remarkably inform the next generation of recommender systems. In addition, we present an open-source neural dataset which will be available for future research to foster approaches combining MIR with cognitive neuroscience, an approach we believe will be a key strategy in characterizing people’s reactions to music.  ",Ellie Bean Abrams,ea84@nyu.edu,"Ellie Bean Abrams (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University, Department of Psychology, New York University)*; Eva Muñoz Vidal (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University, Department of Psychology, New York University); Claire Pelofi (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University); Pablo Ripollés (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University, Department of Psychology, New York University)","Abrams, Ellie Bean*; Muñoz Vidal, Eva; Pelofi, Claire; Ripollés, Pablo",ellie.abrams@nyu.edu*; elm8254@nyu.edu; cp2830@nyu.edu; pripolles@nyu.edu,Domain knowledge -> cognitive MIR,"Applications -> music recommendation and playlist generation; Human-centered MIR -> personalization; Musical features and properties -> musical affect, emotion and mood",No,"Musical expectation and surprisal, which have been shown to predict liking and pleasure, is encoded at the brain level as modeled by a new Bayesian computational model. This has implications for music recommender systems.",Yes,No,No,000018.pdf,https://archives.ismir.net/ismir2022/paper/000018.pdf,https://drive.google.com/open?id=151IWNyIDcSD92I4YbjtbFabc1YobsK4X,https://drive.google.com/open?id=1jQBDLr4IaH20ym-U5jfXoHni3GC8xUDL,https://drive.google.com/open?id=1OkcvT7xUU8tzVHvUF5hCZ3SiO5Iulsvm,,https://slack.com/app_redirect?channel=C04CMQWFYBW,p2-02-abrams
72,1,2,2,"In-person, in Bengaluru",FALSE,Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention,"We propose Beat Transformer, a novel Transformer encoder architecture for joint beat and downbeat tracking. Different from previous models that track beats solely based on the spectrogram of an audio mixture, our model deals with demixed spectrograms with multiple instrument channels. This is inspired by the fact that humans perceive metrical structures from richer musical contexts, such as chord progression and instrumentation. To this end, we develop a Transformer model with both time-wise attention and instrument-wise attention to capture deep-buried metrical cues. Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity. Experiments demonstrate a significant improvement in demixed beat tracking over the non-demixed version. Also, Beat Transformer achieves up to 4% point improvement in downbeat tracking accuracy over the TCN architectures. We further discover an interpretable attention pattern that mirrors our understanding of hierarchical metrical structures.",Jingwei Zhao,jzhao@u.nus.edu,"Jingwei Zhao (Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)*; Gus Xia (Music X Lab, NYU Shanghai, MBZUAI); Ye Wang (School of Computing, NUS, Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)","Zhao, Jingwei*; Xia, Gus; Wang, Ye",jzhao@u.nus.edu*; gxia@nyu.edu; wangye@comp.nus.edu.sg,Musical features and properties,"Applications -> music retrieval systems; Domain knowledge -> machine learning/artificial intelligence for music; Musical features and properties -> rhythm, beat, tempo",No,A novel beat and downbeat tracking system that is based on Transformer and enhanced by music demixing.,Yes,Yes,No,000019.pdf,https://archives.ismir.net/ismir2022/paper/000019.pdf,https://drive.google.com/open?id=1Q0LsWMOByUsHhXlkArzV0pB0tSlxmos-,https://drive.google.com/open?id=1DZ3voqJPBwMMe2AQRcvkcOS36iInirKG,https://drive.google.com/open?id=1xfyHMhPnqYcs3PpUGX_biIKMpNMrTQL2,https://docs.google.com/presentation/d/1YdoslAYlF1L6kdvJwL83owFQEtYCYcU4/edit?usp=sharing&ouid=115661709969705131614&rtpof=true&sd=true,https://slack.com/app_redirect?channel=C04CY0M36E5,p2-03-zhao
151,1,2,3,Virtually,FALSE,Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning,"We propose a system for rendering a symbolic piano performance with flexible musical expression. It is necessary to actively control musical expression for creating a new music performance that conveys various emotions or nuances. However, previous approaches were limited to following the composer's guidelines of musical expression or dealing with only a part of the musical attributes. We aim to disentangle the entire musical expression and structural attribute of piano performance using a conditional VAE framework. It stochastically generates expressive parameters from latent representations and given note structures. In addition, we employ self-supervised approaches that force the latent variables to represent target attributes. Finally, we leverage a two-step encoder and decoder that learn hierarchical dependency to enhance the naturalness of the output. Experimental results show that our system can stably generate performance parameters relevant to the given musical scores, learn disentangled representations, and control musical attributes independently of each other.",Seungyeon Rhyu,rsy1026@snu.ac.kr,"Seungyeon Rhyu (Music and Audio Research Group, Seoul National University, South Korea)*; Sarah Kim (Krust Universe, South Korea); Kyogu Lee (Music and Audio Research Group, Seoul National University, South Korea, Graduate School of AI, AI Institute, Seoul National University, South Korea)","Rhyu, Seungyeon*; Kim, Sarah; Lee, Kyogu",rsy1026@snu.ac.kr*; estelle.kim@krustuniverse.com; kglee@snu.ac.kr,MIR tasks -> music generation,Musical features and properties -> expression and performative aspects of music,No,We propose a CVAE-based system for generating a symbolic piano performance with flexible musical expression.,Yes,No,No,000020.pdf,https://archives.ismir.net/ismir2022/paper/000020.pdf,https://drive.google.com/open?id=1n_HZ-Kgko68D0YoROBida8ITvuXoG7gb,https://drive.google.com/open?id=1pbzok9hZv7VqxQ-T4wwMA66IpVOLMl-w,https://drive.google.com/open?id=1-YtYLwtg6K029Cx5XidtQG6jC2kg5x-M,,https://slack.com/app_redirect?channel=C04CK86SZ2P,p2-04-rhyu
118,1,2,4,"In-person, in Bengaluru",FALSE,Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts,"As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed personalized auto-taggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, these context-aware auto-tagger could be only used by assuming that the context class is explicitly provided by the user. In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time first by leveraging personalized music auto-taggers, and second by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a personalized context-aware music retrieval system is feasible, but the performance suffers in the case of new users, new tracks or when the number of context classes increases. ",Karim M. Ibrahim,karim.ibrahim@telecom-paris.fr,"Karim M. Ibrahim (LTCI, Télécom Paris, Institut Polytechnique de Paris, Deezer Research)*; Elena V. Epure (Deezer Research); Geoffroy Peeters (LTCI, Télécom Paris, Institut Polytechnique de Paris); Gaël Richard (LTCI, Télécom Paris, Institut Polytechnique de Paris)","Ibrahim, Karim M.*; Epure, Elena V.; Peeters, Geoffroy; Richard, Gaël",karim.m.ibraheem@gmail.com*; eepure@deezer.com; geoffroy.peeters@telecom-paris.fr; gael.richard@telecom-paris.fr,MIR tasks -> automatic classification,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; Human-centered MIR -> personalization; Human-centered MIR -> user behavior analysis and mining; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; Musical features and properties -> musical affect, emotion and mood",No,"For contextual recommendation, we can predict when a situation is happening based on the device data, while tagging the tags with their potential situations in a peronalized manner.",No,Yes,No,000021.pdf,https://archives.ismir.net/ismir2022/paper/000021.pdf,https://drive.google.com/open?id=1Q5wzXTun1KIoKdLnhZ-xJP736VQRhfef,https://drive.google.com/open?id=1cS7-ehKq1zxEizn1sWvGGZitdr8hAS64,https://drive.google.com/open?id=1TgmfGOK_n1KFUMe-X1eMIaiL1lsEGqKM,https://drive.google.com/open?id=1pT2vyDLSW6XGf4PNgvfhrRu7hdwD4IG3,https://slack.com/app_redirect?channel=C04CKB8PDEE,p2-05-ibrahim
164,1,2,5,Virtually,FALSE,Jukedrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE,"This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio- domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",Yueh-Kao Wu,yk.lego09@gmail.com,Yueh-Kao Wu (Academia Sinica)*; Ching-Yu Chiu (National Cheng Kung University); Yi-Hsuan Yang (Taiwan AI Labs),"Wu, Yueh-Kao*; Chiu, Ching-Yu; Yang, Yi-Hsuan",yk.lego09@gmail.com*; x2009971@gmail.com; yang@citi.sinica.edu.tw,MIR tasks -> music generation,"Domain knowledge -> machine learning/artificial intelligence for music; Musical features and properties -> rhythm, beat, tempo",No,We show that deep generative models are able to improvise the drum part directly in the audio domain to play along to a user-provided drumless recording.,No,No,No,000022.pdf,https://archives.ismir.net/ismir2022/paper/000022.pdf,https://drive.google.com/open?id=1g_E3V-GrV16hpaKTk4IAbvlK8_FgJTF8,https://drive.google.com/open?id=1kzWgTqJOxZyovhRzsrSMi42bQT5NC_CZ,https://drive.google.com/open?id=1y-TG4fD3WmNiIvaNdnV5imS3N1_FEyTD,,https://slack.com/app_redirect?channel=C04CMQVCF2Q,p2-06-wu
147,1,2,6,Virtually,FALSE,Learning Hierarchical Metrical Structure Beyond Measures,"Music contains hierarchical structures beyond beats and measures. While hierarchical structure annotations are helpful for music information retrieval and computer musicology, such annotations are scarce in current digital music databases. In this paper, we explore a data-driven approach to automatically extract hierarchical metrical structures from scores. We propose a new model with a Temporal Convolutional Network-Conditional Random Field (TCN-CRF) architecture. Given a symbolic music score, our model takes in an arbitrary number of voices in a beat-quantized form, and predicts a 4-level hierarchical metrical structure from downbeat-level to section-level. We also annotate a dataset using RWC-POP MIDI files to facilitate training and evaluation. We show by experiments that the proposed method performs better than the rule-based approach under different orchestration settings. We also perform some simple musicological analysis on the model predictions. All demos, datasets and pre-trained models are publicly available on Github.",Junyan Jiang,jj2731@nyu.edu,"Junyan Jiang (Music X Lab, NYU Shanghai, MBZUAI)*; Daniel Chin (Music X Lab, NYU Shanghai, MBZUAI); Yixiao Zhang (Music X Lab, NYU Shanghai, Centre for Digital Music, QMUL); Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)","Jiang, Junyan*; Chin, Daniel; Zhang, Yixiao; Xia, Gus",jj2731@nyu.edu*; daniel.chin@nyu.edu; ldzhangyx@outlook.com; gxia@nyu.edu,"Musical features and properties -> structure, segmentation, and form","Domain knowledge -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> pattern matching and detection; Musical features and properties; Musical features and properties -> rhythm, beat, tempo",No,We propose a new data-driven TCN-CRF model to extract a 4-layer hierarchical metrical structure beyond measures from symbolic music.,Yes,No,No,000023.pdf,https://archives.ismir.net/ismir2022/paper/000023.pdf,https://drive.google.com/open?id=1jWSzDrSgqMBs91Mi0uRIGMR0BU3YuuNw,https://drive.google.com/open?id=1WBja87MuYEsYuAh2FtPfQzTL6aDNg2Hg,https://drive.google.com/open?id=1bJUUFdMLR9-GDKCyTxtNuIHzsSOZHav7,,https://slack.com/app_redirect?channel=C04D92GTCQG,p2-07-jiang
204,1,2,7,Virtually,FALSE,Mid-level Harmonic Audio Features for Musical Style Classification,"The extraction of harmonic information from musical audio is fundamental for several music information retrieval tasks. In this paper, we propose novel harmonic audio features based on the perceptually-inspired tonal interval vector space, computed as the Fourier transform of chroma vectors. Our contribution includes mid-level features for musical dissonance, chromaticity, dyadicity, triadicity, diminished quality, diatonicity, and whole-toneness. Moreover, we quantify the perceptual relationship between short- and long-term harmonic structures, tonal dispersion, harmonic changes, and complexity. Beyond the computation on fixed-size windows, we propose a context-sensitive harmonic segmentation approach. We assess the robustness of the new harmonic features in style classification tasks regarding classical music periods and composers. Our results align with, slightly outperforming, existing features and suggest that other musical properties than those in state-of-the-art literature are partially captured. We discuss the features regarding their musical interpretation and compare the different feature groups regarding their effectiveness for discriminating classical music periods and composers.",Francisco C. F. Almeida,up201909574@fe.up.pt,"Francisco C. F. Almeida (Univ. Porto, Faculty of Engineering & INESC TEC)*; Gilberto Bernardes (Univ. Porto, Faculty of Engineering & INESC TEC); Christof Weiss (International Audio Laboratories Erlangen)","Almeida, Francisco C. F.*; Bernardes, Gilberto; Weiss, Christof",up201909574@fe.up.pt*; gba@fe.up.pt; christof.weiss@audiolabs-erlangen.de,Musical features and properties,"MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> musical style and genre",No,Perceptually-inspired mid-level harmonic features computed as the DFT of chroma vectors have shown to outperform existing style classification models of classical music periods and composers.,Yes,Yes,No,000024.pdf,https://archives.ismir.net/ismir2022/paper/000024.pdf,https://drive.google.com/open?id=1vc8HTUSlxOdplTBk3cIzHSyfxJa8hNEs,https://drive.google.com/open?id=1jm3Xip_D1P9V1Y4XFCuzEvYTL-1i6Qyj,https://drive.google.com/open?id=1b2UHoWcv9d8Bp5uC42rYmfW1MM87_6gW,,https://slack.com/app_redirect?channel=C04CCP3735L,p2-08-almeida
113,1,2,8,"In-person, in Bengaluru",FALSE,Distortion Audio Effects: Learning How to Recover the Clean Signal,"Given the recent advances in music source separation and automatic mixing, removing audio effects in music tracks is a meaningful step toward developing an automated remixing system. This paper focuses on removing distortion audio effects applied to guitar tracks in music production. We explore whether effect removal can be solved by neural networks designed for source separation and audio effect modeling.
Our approach proves particularly effective for effects that mix the processed and clean signals. The models achieve better quality and significantly faster inference compared to state-of-the-art solutions based on sparse optimization. We demonstrate that the models are suitable not only for declipping but also for other types of distortion effects. By discussing the results, we stress the usefulness of multiple evaluation metrics to assess different aspects of reconstruction in distortion effect removal.",Giorgio Fabbro,giorgio.fabbro@sony.com,"Johannes Imort (RWTH Aachen University, Germany); Giorgio Fabbro (Sony Europe B.V., Stuttgart, Germany)*; Marco A Martinez Ramirez (Sony Group Corporation, Tokyo, Japan); Stefan Uhlich (Sony Europe B.V., Stuttgart, Germany); Yuichiro Koyama (Sony Group Corporation, Tokyo, Japan); Yuki Mitsufuji (Sony Group Corporation, Tokyo, Japan)","Imort, Johannes; Fabbro, Giorgio*; Martinez Ramirez, Marco A; Uhlich, Stefan; Koyama, Yuichiro; Mitsufuji, Yuki",johannes.imort@rwth-aachen.de; giorgio.fabbro@sony.com*; marco.martinez@sony.com; stefan.uhlich@sony.com; Yuichiro.Koyama@sony.com; Yuhki.Mitsufuji@sony.com,MIR fundamentals and methodology -> music signal processing,"Applications -> performance, and production; Domain knowledge -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation; MIR tasks -> sound source separation",No,"Recovering the clean signal from clipped or overdriven guitar signals can be formulated as a filtering problem that can be solved efficiently with neural networks originally designed for source separation, achieving high quality and inference speed in contrast to previous solutions based on sparse optimization.",Yes,No,No,000025.pdf,https://archives.ismir.net/ismir2022/paper/000025.pdf,https://drive.google.com/open?id=16u1SK0_iqfzqbIvY1TlfQ_yWRG1yYJov,https://drive.google.com/open?id=14-dTdMOLibNXOCFe3UFUjgLlT9tGacJc,https://drive.google.com/open?id=1Kmlvn7_i2OpBDk_4OQV3D_w0gsUpAY5v,https://drive.google.com/open?id=1RGCm9Y9C5Q-tojlt8g-cJbM2kd7kMytn,https://slack.com/app_redirect?channel=C04C4QPMVK9,p2-09-imort
33,1,2,9,Virtually,FALSE,End-to-End Full-Page Optical Music Recognition for Mensural Notation,"Optical Music Recognition (OMR) systems typically consider workflows that include several steps, such as staff detection, symbol recognition, and semantic reconstruction. However, fine-tuning these systems is costly due to the specific data labeling process that has to be performed to train models for each of these steps. In this paper, we present the first segmentation-free full-page OMR system that receives a page image and directly outputs the transcription in a single step. This model requires only the annotations of full score pages, which greatly alleviates the task of manual labeling. The model has been tested with early music written in mensural notation, for which the presented approach is especially beneficial. Results show that this methodology provides a solution with promising results and establishes a new line of research for holistic transcription of music score pages.",Antonio Ríos-Vila,arios@dlsi.ua.es,"Antonio Ríos-Vila (U.I for Computer Research, University of Alicante, Spain)*; José M. Iñesta (U.I for Computer Research, University of Alicante, Spain); Jorge Calvo-Zaragoza (U.I for Computer Research, University of Alicante, Spain)","Ríos-Vila, Antonio*; Inesta, Jose M.; Calvo-Zaragoza, Jorge",arios@dlsi.ua.es*; inesta@dlsi.ua.es; jcalvo@dlsi.ua.es,MIR tasks -> optical music recognition,Domain knowledge -> machine learning/artificial intelligence for music; MIR tasks -> music transcription and annotation,No,This paper presents an evolution of the statee-of-the-art OMR recongition systems to segmentation-free end-to-end full music page transcriptions.,Yes,Yes,No,000026.pdf,https://archives.ismir.net/ismir2022/paper/000026.pdf,https://drive.google.com/open?id=1ChqPLzsVay8fH5Ve-E4FDbvWTFZnauyD,https://drive.google.com/open?id=1bqt_DrqYkabIYJiZ-pOOasCXEN6ZKXGu,https://drive.google.com/open?id=1bJg7r0R-M-_JaQby-2Ui4Ii70iBFPKUG,,https://slack.com/app_redirect?channel=C04CKB7SXH8,p2-10-ríos-vila
188,1,2,10,"In-person, in Bengaluru",FALSE,Mel Spectrogram Inversion with Stable Pitch,"Vocoders are models capable of transforming a low-dimensional spectral representation of an audio signal, typically the mel spectrogram, to a waveform. Modern speech generation pipelines use a vocoder as their final component. Recent vocoder models developed for speech achieve high degree of realism, such that it is natural to wonder how they would perform on music signals. Compared to speech, the heterogeneity and structure of the musical sound texture offers new challenges. In this work we focus on one specific artifact that some vocoder models designed for speech tend to exhibit when applied to music: the perceived instability of pitch when synthesizing sustained notes. We argue that the characteristic sound of this artifact is due to the lack of horizontal phase coherence, which is often the result of using a time-domain target space with a model that is invariant to time-shifts, such as a convolutional neural network. 

We propose a new vocoder model that is specifically designed for music. Key to improving the pitch stability is the choice of a shift-invariant target space that consists of the magnitude spectrum and the phase gradient. We discuss the reasons that inspired us to re-formulate the vocoder task, outline a working example, and evaluate it on musical signals. Our method results in 60% and 10% improved reconstruction of sustained notes and chords with respect to existing models, 
using a novel harmonic error metric.",Bruno Di Giorgi,bdigiorgi@apple.com,Bruno Di Giorgi (Apple)*; Mark Levy (Apple); Richard Sharp (Apple),"Di Giorgi, Bruno*; Levy, Mark; Sharp, Richard",bdigiorgi@apple.com*; mark_levy@apple.com; richard_sharp@apple.com,MIR tasks -> music synthesis and transformation,MIR fundamentals and methodology -> music signal processing,No,"When inverting the Mel Spectrogram to audio, reconstruction of musical notes and chords can be improved by explicitly modelling the phase gradient spectrum.",No,Yes,No,000027.pdf,https://archives.ismir.net/ismir2022/paper/000027.pdf,https://drive.google.com/open?id=1A8OcItwfN9P0OKLQU122KjwNVDrjx0KK,https://drive.google.com/open?id=1jpiR9Y-p0POeWMv3pHDo723_kLmK7Wgb,https://drive.google.com/open?id=1TWl73QhOmqsG_OWF3x_3c295dZm0ZfCk,https://drive.google.com/open?id=1eSJQbz0xm1bmjMx4ZR-YnT9vdcaTryq1,https://slack.com/app_redirect?channel=C04CCP34MK8,p2-11-di-giorgi
119,1,2,11,Virtually,FALSE,Latent feature augmentation for chorus detection,"In this paper, we introduce LA-Chorus, a chorus detection model based on latent feature augmentation and ResNet FPN architecture. Our contributions in LA-Chorus are three-fold. Firstly, we propose a method for implicitly augmenting chorus data in the latent space during the train7 ing stage. Compared to augmentations on audio surfaces such as time stretching and pitch shifting, latent augmentations indicate changes at a higher level in original audio, thereby increasing the diversity and sufficiency in training. Second, we apply Feature Pyramid Network (FPN) to generate additional embeddings from low dimension to high dimension, consequently achieving a multi-scale training paradigm. Lastly, we release Di-Chorus, a new open-source dataset of diverse genres and languages for the community of music structure analysis. In conjunction with other public datasets, we conduct comprehensive ex18 periments to evaluate the performance of LA-Chorus compared to other state-of-the-art models, which demonstrate the out-performance of LA-Chorus and the effectiveness of proposed latent feature augmentation.",Xingjian Du,duxingjian.real@bytedance.com,Xingjian Du (ByteDance)*; Huidong Liang (ByteDance); Yuan Wan (ByteDance AI Lab); Yuheng Lin (ByteDance AI Lab); Ke Chen (University of California San Diego); Bilei Zhu (ByteDance AI Lab); Zejun Ma (Bytedance),"Du, Xingjian*; Liang, Huidong; Wan, Yuan; Lin, Yuheng; Chen, Ke; Zhu, Bilei; Ma, Zejun",duxingjian.real@bytedance.com*; lianghuidong@bytedance.com; wanyuan.0626@bytedance.com; linyuheng.53@bytedance.com; knutchen@ucsd.edu; zhubilei@bytedance.com; mazejun@bytedance.com,MIR tasks -> music transcription and annotation,,No,LATENT FEATURE AUGMENTATION FOR CHORUS DETECTION,No,No,No,000028.pdf,https://archives.ismir.net/ismir2022/paper/000028.pdf,https://drive.google.com/open?id=1N9zeHi5tkvYKhjv0r16xgSBS5GzXW5bH,https://drive.google.com/open?id=1F5J93qfLB_enOWdzi3lGneBYrmVdAQCj,https://drive.google.com/open?id=1617ycXzfATdFwdsM-Y9uusY2gYN3H1wT,,https://slack.com/app_redirect?channel=C04CKB8Q2GJ,p2-12-du
218,1,2,12,Virtually,FALSE,AccoMontage2: A Complete Harmonization and Accompaniment Arrangement System,"We propose AccoMontage2, a system capable of doing full-length song harmonization and accompaniment arrangement based on a lead melody. Following AccoMontage, this study focuses on generating piano arrangements for popular/folk songs and it carries on the generalized template-based retrieval method. The novelties of this study are twofold. First, we invent a harmonization module (which AccoMontage does not have). This module generates structured and coherent full-length chord progression by optimizing and balancing three loss terms: a micro-level loss for note-wise dissonance, a meso-level loss for phrase-template matching, and a macro-level loss for full piece coherency. Second, we develop a graphical user interface which allows users to select different styles of chord progression and piano texture. Currently, chord progression styles include Pop, R&B, and Dark, while piano texture styles include several levels of voicing density and rhythmic complexity. Experimental results show that both our harmonization and arrangement results significantly outperform the baselines. Lastly, we release AccoMontage2 as an online application as well as the organized chord progression templates as a public dataset. ",Li Yi,ly1387@nyu.edu,"Li Yi (Music X Lab, NYU Shanghai, MBZUAI)*; Haochen Hu (Music X Lab, NYU Shanghai, MBZUAI); Jingwei Zhao (Institute of Data Science, NUS); Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)","Yi, Li*; Hu, Haochen; Zhao, Jingwei; Xia, Gus",ly1387@nyu.edu*; hh1933@nyu.edu; jzhao@u.nus.edu; gxia@nyu.edu,MIR tasks,"Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> MIR tasks; Human-centered MIR -> music interfaces and services; MIR tasks -> music generation; MIR tasks -> music synthesis and transformation",No,"We propose AccoMontage2, a system capable of doing full-length song harmonization and accompaniment arrangement based on a lead melody.",Yes,Yes,No,000029.pdf,https://archives.ismir.net/ismir2022/paper/000029.pdf,https://drive.google.com/open?id=1OF59ctqQTEilC-oSKuIscvN7FUVH-ghA,https://drive.google.com/open?id=1D_f5QafclHSYkfYDYibajD2JTiLF9Xly,https://drive.google.com/open?id=1QAHUKa6RmcjyHSpHhgnf9MOoGm43K3hx,,https://slack.com/app_redirect?channel=C04D92HF0QY,p2-13-yi
285,1,2,13,"In-person, in Bengaluru",FALSE,Supervised and Unsupervised Learning of Audio Representations for Music Understanding,"In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks.

We show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs.

Within the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning---and in some cases, supervised learning---for music understanding.

We also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality.
",Matthew C McCallum,matt.c.mccallum@gmail.com,"Matthew C McCallum (SiriusXM, USA)*; Filip Korzeniowski (SiriusXM, USA); Sergio Oramas (SiriusXM, USA); Fabien Gouyon (SiriusXM, USA); Andreas Ehmann (SiriusXM, USA)","McCallum, Matthew C*; Korzeniowski, Filip; Oramas, Sergio; Gouyon, Fabien; Ehmann, Andreas",matt.c.mccallum@gmail.com*; fkorzeniowski@pandora.com; soramas@pandora.com; fgouyon@pandora.com; aehmann@pandora.com,Musical features and properties -> representations of music,Applications -> digital libraries and archives; Applications -> music retrieval systems; Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; MIR tasks -> automatic classification,No,"Audio representations learned from large-scale expert annotated music data achieve state-of-the-art performance for a wide range of music understanding / labelling tasks, furthermore, representations learned using unsupervised learning on the same set of audio achieves state-of-the-art performance for unsupervised and sometimes supervised learning. This is done in a method that is practical for application to industry-scale music catalogs.",No,Yes,No,000030.pdf,https://archives.ismir.net/ismir2022/paper/000030.pdf,https://drive.google.com/open?id=1z-1-ViOy301bJ-nrvqtdPe8LgdLIS-5A,https://drive.google.com/open?id=16LQ3MZJb4HZrbe6fPohKNkCBtar9ZXp4,https://drive.google.com/open?id=1ihOKFFuBiC0wc8ANilHWHhwfcly5c7fe,https://drive.google.com/open?id=1ZnxJLQgIV5j21aehIJHbdxrlmIgt-1ku,https://slack.com/app_redirect?channel=C04D92J316U,p2-14-mccallum
320,1,2,14,"In-person, in Bengaluru",FALSE,Generating Coherent Drum Accompaniment with Fills and Improvisations,"Creating a complex work of art like music necessitates profound creativity. With recent advancements in Deep Learning and powerful models such as Transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments – Piano, Guitar, Bass and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its relatively low representation in the training data. We propose a novelty function that represents the extent of improvisation in a specific bar relative to its neighbors. We train a model to detect improvisation positions from the melodic accompaniment tracks. Finally, we use a novel BERT inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music.",Rishabh A Dahale,dahalerishabh1@iitb.ac.in,"Rishabh A Dahale (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)*; Vaibhav Vinayak Talwadker (Department of Electrical Engineering, Indian Institute of Technology Bombay, India); Preeti Rao (Department of Electrical Engineering, Indian Institute of Technology Bombay, India); Prateek Verma (Stanford University)","Dahale, Rishabh A*; Talwadker, Vaibhav Vinayak; Rao, Preeti; Verma, Prateek",dahalerishabh1@iitb.ac.in*; talwadkerv@gmail.com; prao@ee.iitb.ac.in; prateekv@stanford.edu,MIR tasks -> music generation,"Musical features and properties -> structure, segmentation, and form",No,This paper highlights the drawbacks of language models in capturing rare but important musical events (drum fills and improvisations) and proposes a system to mitigate this issue.,Yes,No,No,000031.pdf,https://archives.ismir.net/ismir2022/paper/000031.pdf,https://drive.google.com/open?id=1aINKqGpGW86_tANRiw6zTH5iz9GbqTB6,https://drive.google.com/open?id=1L1Jz73TIk7MMWrwaOETWEjfJaOktROEY,https://drive.google.com/open?id=14M6XklTLCbkVR9I2hcstszVGEoaOAFty,https://docs.google.com/presentation/d/1W9phzb04iLpOd4wFUABpXXX73Ij1NL1s/edit?usp=sharing&ouid=106338110260173212503&rtpof=true&sd=true,https://slack.com/app_redirect?channel=C04CGCW7N4V,p2-15-dahale
231,1,2,15,"In-person, in Bengaluru",FALSE,Bottlenecks and solutions for audio to score alignment research,"Although audio to score alignment is a classic Music Information Retrieval problem, it has not been defined uniquely with the scope of musical scenarios representing its core. The absence of a unified vision makes it difficult to pinpoint its state-of-the-art and determine directions for improvement. To get past this bottleneck, it is necessary to consolidate datasets and evaluation methodologies to allow comprehensive benchmarking. In our review of prior work, we demonstrate the extent of variation in problem scope, datasets, and evaluation practices across audio to score alignment research. To circumvent the high cost of creating large-scale datasets with various instruments, styles, performance conditions, and musician proficiency from scratch, the research community could generate ground truth approximations from non-audio to score alignment datasets which include a temporal mapping between a music score and its corresponding audio. We show a methodology for adapting the Aligned Scores and Performances dataset, created originally for beat tracking and music transcription. We filter the dataset semi- automatically by applying a set of Dynamic Time Warping based Audio to Score Alignment methods using out-of-the-box Chroma and Constant-Q Transform extraction algorithms, suitable for the characteristics of the piano performances of the dataset. We use the results to discuss the limitations of the generated ground truths and data adaptation method. While the adapted dataset does not provide the necessary diversity for solving the initial problem, we conclude with ideas for expansion, and identify future directions for curating more comprehensive datasets through data adaptation, or synthesis.",Alia Morsi,alia.morsi@upf.edu,"Alia Morsi (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)*; Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)","Morsi, Alia*; Serra, Xavier",alia.morsi@upf.edu*; xavier.serra@upf.edu,MIR fundamentals and methodology -> music signal processing,"Domain knowledge -> representations of music; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> indexing and querying",No,"Current audio to score alignment (ASA) methods have reached a glass ceiling. We believe this is likely to remain unless a change occurs in how we define the problem and how we evaluate it, which would be possible with having varied data representing many practical ASA scenarios. We provide a use case of adapting the ASAP dataset to be used for ASA, and highlight how the generated weak alignments could be used to create further data to covering varied scenarios.",Yes,Yes,No,000032.pdf,https://archives.ismir.net/ismir2022/paper/000032.pdf,https://drive.google.com/open?id=1WyBwLn2_jKPDXXk8papoV2DcbBZV5O0u,https://drive.google.com/open?id=1gBfXdMY_HhoxT2BZ_IRikmQvTsbFkqc1,https://drive.google.com/open?id=1vvQDqpLXH_87yqXRYBSwQMwdnDWPYzOK,https://docs.google.com/presentation/d/1l7ZscAbAYGc2q-mwASJMna-mDAO4Lh6BJEAEu8XEySA/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CMQX0LG4,p2-16-morsi
83,2,3,0,"In-person, in Bengaluru",TRUE,Raga Classification From Vocal Performances Using Multimodal Analysis  ,"Work on musical gesture and embodied cognition suggests a rich complementarity between audio and movement information in musical performance. Pose estimation algorithms now make it possible (in contrast to Motion Capture)  to collect rich movement information from unconstrained performances of indefinite length. Vocal performances of Indian art music  offer the opportunity to carry out multimodal analysis using this information, combing musician’s body movements (i.e. pose and gesture data) with audio features. In this work we investigate raga identification from 12 s excerpts from a dataset of 3 singers and 9 ragas using the combination of audio and visual representations that are each semantically salient on their own.  While gesture based classification is relatively weak by itself, we show that combining latent representations from the pre-trained unimodal networks can surpass the already high performance obtained by audio features. ",Preeti Rao,prao@ee.iitb.ac.in,"Martin Clayton (Department of Music, Durham University, United Kingdom); Preeti Rao (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)*; Nithya Shikarpur (Department of Electrical Engineering, Indian Institute of Technology Bombay, India); Sujoy Roychowdhury (Department of Electrical Engineering, Indian Institute of Technology Bombay, India); Jin Li (Department of Music, Durham University, United Kingdom)","Clayton, Martin; Rao, Preeti*; Shikarpur, Nithya Nadig; Roychowdhury, Sujoy; Li, Jin",martin.clayton@durham.ac.uk; prao@ee.iitb.ac.in*; snnithya@gmail.com; 214077004@iitb.ac.in; j.lixjtu@gmail.com,Domain knowledge -> computational ethnomusicology,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> automatic classification",Yes,Vision based methods help exploit singer gestures to complement melody-based classification.,No,Yes,Yes,000033.pdf,https://archives.ismir.net/ismir2022/paper/000033.pdf,https://drive.google.com/open?id=11wQvhr8uswrb9_o_6im3M_ndtHk6p_Xl,https://drive.google.com/open?id=1g9Vo9_o7IBXIl1z_Jm5mGZ7HQG6csZBH,https://drive.google.com/open?id=1uEqTDoKqUYOrmWWqCZJte_fufywP83Ab,https://drive.google.com/open?id=1wSMKi9zpsJJd1zn0A1oRP7oI2IxffB2w,https://slack.com/app_redirect?channel=C04CY0M7ERF,p3-01-rao
165,2,3,1,"In-person, in Bengaluru",TRUE,Traces of Globalization in Online Music Consumption Patterns and Results of Recommendation Algorithms,"Music streaming platforms allow users to enjoy music from all over the globe.
Such opportunity speeds up cultural exchange between different countries, a process often associated with globalization. While such an exchange could lead to more diverse music consumption, empirical evidence on its influence on online music consumption is limited. Besides, the extent to which music recommender systems foster exchange or amplify globalization in music remains an understudied problem.

In this paper, we present findings from an empirical study to detect traces of globalization in domestic vs. foreign online music consumption. Besides, we investigate if popular recommendation algorithms, specifically ItemKNN and 
NeuMF, are prone to amplifying globalization processes. Our experiments on Last.fm listening data show nuanced patterns of globalization in music consumption. We observe a strong position of US music in all considered countries. In countries such as Sweden, Great Britain, or Brazil, US music shows various levels of coexistence with domestic music. We find that Finland is least influenced by US music, while greatly consuming and 'exporting' domestic music.  With respect to recommendation algorithms, ItemKNN tends to recommend domestic music to users of many countries, while NeuMF contributes to accelerating globalization and shifting balance towards dominance of US music on the market.",Oleg Lesota,oleglesota@gmail.com,Oleg Lesota (Johannes Kepler University)*; Emilia Parada-Cabaleiro (Johannes Kepler University Linz); Stefan Brandl (Johannes Kepler University Linz); Elisabeth Lex (TU Graz); Navid Rekabsaz (JKU); Markus Schedl (Johannes Kepler University Linz),"Lesota, Oleg*; Parada-Cabaleiro, Emilia; Lex, Elisabeth; Rekabsaz, Navid; Brandl, Stefan; Schedl, Markus",oleglesota@gmail.com*; emipc86@hotmail.com; elisabeth.lex@tugraz.at; navid.rekabsaz@jku.at; stefan.brandl@jku.at; markus.schedl@jku.at,Human-centered MIR -> user behavior analysis and mining,Applications -> music recommendation and playlist generation; Philosophical and ethical discussions -> legal and societal aspects of MIR,Yes,"Globalization patterns of online music consumption are not homogeneous across countries, music recommender systems contribute to cultural globalization.",Yes,No,Yes,000034.pdf,https://archives.ismir.net/ismir2022/paper/000034.pdf,https://drive.google.com/open?id=1LXaANmzwMqXYUMtuA1hfHrdT-lBs056n,https://drive.google.com/open?id=11NiZccrqg7WmNy7IQcJiuAzGoq-WpZYp,https://drive.google.com/open?id=1-8zl2BbnZs3HrkLEymXn3loWxzdLGMHX,https://drive.google.com/open?id=16jgSQyC7XFfkWYKNb_jgKPYvWeHMN4hF,https://slack.com/app_redirect?channel=C04D92H4F96,p3-02-lesota
303,2,3,2,Virtually,FALSE,Network Analyses for Cross-Cultural Music Popularity,"Anglo-American popular culture has been said to be intricately connected to global popular culture, both shaping and being shaped by popular trends worldwide, yet few research has examined this issue empirically. Our research quantitatively maps the extent of these cultural influences in popular music consumption, by using network analyses to explore cross-cultural popularity in music from 30 countries corresponding to 6 cultural regions (N = 4863 unique songs over six timepoints from 2019-2021). Using Top100 charts from these countries, we constructed a network based on the co-occurrence of songs in charts, and used eigencentrality as an indicator of cross-cultural song popularity. We then compared the country-of-origin of the artists, arousal music features, and socioeconomic indicators. Songs from artists with Anglo-American backgrounds tended to have higher eigencentrality overall, and mixed effects regressions showed that eigencentrality was negatively associated with danceability, and positively associated with spectral energy, and the migrant population of the country (of the charts). Next, using community detection, we observed 11 separate 'communities' in the network. Most communities appeared to be limited by region/culture, but Anglo-American music seemed disproportionally able to transcend cultural boundaries far beyond their geographical borders. We also discuss implications pertaining to cultural hegemony, and the effectiveness of our method in estimating cross-cultural popularity.",Kongmeng Liew,liew.kongmeng@is.naist.jp,Kongmeng Liew (Nara Institute of Science and Technology)*; Vipul Mishra (Nara Institute of Science and Technology); Yangyang Zhou (Nara Institute of Science and Technology); Elena V. Epure (Deezer Research); Romain Hennequin (Deezer Research); Shoko Wakamiya (Nara Institute of Science and Technology); Eiji Aramaki (Nara Institute of Science and Technology),"Liew, Kongmeng*; Mishra, Vipul; Zhou, Yangyang; Epure, Elena V.; Hennequin, Romain; Wakamiya, Shoko; Aramaki, Eiji",liew.kongmeng@is.naist.jp*; vipulmishra2053@gmail.com; zhou.yangyang.zr4@is.naist.jp; eepure@deezer.com; rhennequin@deezer.com; wakamiya@is.naist.jp; aramaki@is.naist.jp,Domain knowledge -> computational music theory and musicology,Applications -> digital libraries and archives; Domain knowledge -> computational ethnomusicology; Philosophical and ethical discussions -> legal and societal aspects of MIR,Yes,"We quantify the pervasiveness of Anglo-American music in Top100 charts from 100 countries, and look at correlates from socio-economic indicators and simple MIR-based arousal features.",No,No,No,000035.pdf,https://archives.ismir.net/ismir2022/paper/000035.pdf,https://drive.google.com/open?id=1lvSF-lwH7TjyzsdlMVvqzHAIVOKaNFXQ,https://drive.google.com/open?id=1tY74PkqPSXwcUAlSyhhEZgzsTWshZqqi,https://drive.google.com/open?id=1JcFbhDCb5H7fqktu7HJngVEiausZ7f_i,,https://slack.com/app_redirect?channel=C04CCP41U0N,p3-03-liew
51,2,3,3,Virtually,FALSE,Three related corpora in Middle Byzantine music notation and a preliminary comparative analysis,"The Middle Byzantine notation (MBn) is used to capture the plainchant melodies of eastern Orthodox Christian music from the middle of the 12th century until 1814. In the context of this research, we study the evolution of a subgenre of Byzantine music known as Heirmologic. We present three Heirmologic corpora spanning the periods before, during and after the 16th century. We discuss the challenges we faced during the digitisation process, and the steps we took to overcome them. For the analysis of the three corpora, we apply the three methods, namely notational texture, melodic arch similarity, and Jensen-Shannon distances of Markovian models, the second of which is novel and inspired by the idea of melodic arches. Through these methods, we aim at highlighting the differences of the corpora in order to obtain an outline of the evolution of the subgenre. We observe that the post 16th century Heirmologic pieces are more similar to the 16th century ones, while there is a greater difference with the pre 16th century pieces. This indicates that the 16th century constitutes a turning point in the melodic features of the Heirmologic subgenre.",Polykarpos Polykarpidis,polykarpospolykarpidis@gmail.com,Polykarpos Polykarpidis (National and Kapodistrian University of Athens)*; DIONYSIOS KALOFONOS (Independent researcher); Dimitrios Balageorgos (National and Kapodistrian University of Athens); Christina Anagnostopoulou (National and Kapodistrian University of Athens),"Polykarpidis, Polykarpos*; KALOFONOS, DIONYSIOS; Balageorgos, Dimitrios; Anagnostopoulou, Christina",polykarpospolykarpidis@gmail.com*; peitemou@gmail.com; dbalageorgos@music.uoa.gr; chrisa@music.uoa.gr,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",Domain knowledge -> computational ethnomusicology; Domain knowledge -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> similarity metrics; Musical features and properties -> representations of music,Yes,A novel dataset in Middle Byzantine music notation.,Yes,Yes,No,000036.pdf,https://archives.ismir.net/ismir2022/paper/000036.pdf,https://drive.google.com/open?id=11_bDg_L2_UVMJ8-B9gNf_bKwoapY_s2W,https://drive.google.com/open?id=12jRG0khHbiWj4JmVlUefVw_4r3vvzfCu,https://drive.google.com/open?id=1_0B0WZi06iQa7m_LULRcvn0F9rEFY7b4,,https://slack.com/app_redirect?channel=C04CY0LRXKK,p3-04-polykarpidis
48,2,3,4,Virtually,FALSE,Playing Technique Detection by Fusing Note Onset Information in Guzheng Performance,"The Guzheng is a kind of traditional Chinese instruments with diverse playing techniques. Instrument playing techniques (IPT) play an important role in musical performance. However, most of the existing works for IPT detection show low efficiency for variable-length audio and provide no assurance in the generalization as they rely on a single sound bank for training and testing. In this study, we propose an end-to-end Guzheng playing technique detection system using Fully Convolutional Networks that can be applied to variable-length audio. Because each Guzheng playing technique is applied to a note, a dedicated onset detector is trained to divide an audio into several notes and its predictions are fused with frame-wise IPT predictions. During fusion, we add the IPT predictions frame by frame inside each note and get the IPT with the highest probability within each note as the final output of that note. We create a new dataset named GZ_IsoTech from multiple sound banks and real-world recordings for Guzheng performance analysis. Our approach achieves 87.97% in frame-level accuracy and 80.76% in note-level F1-score, outperforming existing works by a large margin, which indicates the effectiveness of our proposed method in IPT detection.",Dichucheng Li,21210240219@m.fudan.edu.cn,Dichucheng Li (Fudan University)*; Yulun Wu (Fudan University); Qinyu Li (Sichuan Conservatory Of Music); Jiahao Zhao (Fudan University); Yi Yu (NII); Fan Xia (Sichuan Conservatory of Music ); Wei Li (Fudan University),"Li, Dichucheng*; Wu, Yulun; Li, Qinyu; Zhao, Jiahao; Yu, Yi; Xia, Fan; Li, Wei",21210240219@m.fudan.edu.cn*; 20110240018@fudan.edu.cn; 935362804@qq.com; 20210240306@fudan.edu.cn; yiyu@nii.ac.jp; 409769992@qq.com; weili-fudan@fudan.edu.cn,MIR tasks -> music transcription and annotation,"Evaluation, datasets, and reproducibility -> MIR tasks; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks; Musical features and properties -> expression and performative aspects of music; Musical features and properties -> representations of music",Yes,We propose an end-to-end guzheng playing technique detection system fusing note onset information and create a new dataset for guzheng performance analysis.,Yes,Yes,No,000037.pdf,https://archives.ismir.net/ismir2022/paper/000037.pdf,https://drive.google.com/open?id=1Gp00ewNuGQ3FVY27TT-rv4RHrnL2q2Mx,https://drive.google.com/open?id=14M-JDQqI1LVEW7Uwe_yVhb1P3P1hMOEt,https://drive.google.com/open?id=1YT7fAyYnHpnOLh-igf9cJ0ksqMkwhBNA,,https://slack.com/app_redirect?channel=C04D92FURNU,p3-05-li
111,2,3,5,"In-person, in Bengaluru",FALSE,KDC: an open corpus for computational research of dastgāhi music,"Iranian dastgāhi music is considered as the classical repertory of contemporary Iran. In the 19th century, the melodic modes that developed during its long history were grouped in categories, each of them known as dastgāh. The dastgāhi system presents unique features, that have been object of musicological study since its inception. However, computational methods for its research are still scarce, due in good part to the lack of open, well curated corpora. The aim of the KUG Dastgāhi Corpus (KDC) is to contribute to the development of computational corpus driven research for this tradition. KDC is created following the FAIR principles, and in close collaboration with performers and scholars, who contribute to it with annotations and qualitative evaluations. Besides presenting the first version of KDC, in this paper we explore the possibilities that Iranian dastgāhi music offers to computational research. In order to test the performance of state-of-the-art technologies applied to this music tradition, we present preliminary results for several analytical tasks, and discuss thei opportunities and limitations learnt in the process.",Babak Nikzat,b.nikzat@kug.ac.at,Babak Nikzat (University of Music and Performing Arts Graz)*; Rafael Caro Repetto (Kunstuniversität Graz),"Nikzat, Babak*; Caro Repetto, Rafael",b.nikzat@kug.ac.at*; rafael.caro-repetto@kug.ac.at,Domain knowledge -> computational ethnomusicology,"Domain knowledge -> computational music theory and musicology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Musical features and properties -> melody and motives",Yes,"A new open corpus for the computational research of Iranian dastgahi music is introduced, possibilities and limitations for computational methods are explored, and a preliminary test of state of the art technologies is curried out.",No,No,No,000038.pdf,https://archives.ismir.net/ismir2022/paper/000038.pdf,https://drive.google.com/open?id=12CPNBBB11-Yg32qGYCn-BwL6pPHuoxSI,https://drive.google.com/open?id=12M_y7UPwqF6bdOB35RbwPs9owXe6oB1B,https://drive.google.com/open?id=1jf16ypILamWbpUvcxInEZbMbFCoKAo3K,https://docs.google.com/presentation/d/1ansMwdJX_7mxeKXAWV2K-2sJYnC6Bu5J4I4ns3eRoXs/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CY0MEP41,p3-06-nikzat
148,2,3,6,Virtually,FALSE,Inaccurate Prediction or Genre Evolution? Rethinking Genre Classification,"The existing MIR research on genre classification primarily focuses on how to classify a song into the “correct” genre while downplaying the fact that genres mutate over time and in response to social change in terms of their musical properties. Songs claiming the same genre can sound very different if they are released years apart, and genres may revive musical traditions from the past. In this paper, I show that the performance of genre classifiers fluctuates as genres evolve. Unsatisfactory performance of the classifiers may not indicate algorithmic flaws but rather the change of genre characteristics. I demonstrate this by studying the case of Chinese Hip-Hop music. Specifically, I collected and analyzed 69,427 songs from four genres (Hip-Hop, Pop, Rock, and Folk) released on a Chinese music platform between 2009 and 2019. Using classifiers trained from the songs in different year cohorts to predict the genre of all the songs, I show how genre classifiers can be used to detect the stylistic shift in Hip-Hop that happened during this period. The paper thus offers a novel, sociological perspective on contending with the much-challenged idea of improving genre classification accuracy for its own sake. However, instead of questioning the effort, I argue that MIR research on genre classification can be helpful for studying genre as a social construct and cultural phenomenon if the pursuit of prediction performance and the cultural meaning of inaccurate prediction are carefully balanced.",Ke Nie,knie@ucsd.edu,"Ke Nie (University of California, San Diego)*","Nie, Ke*",knie@ucsd.edu*,Philosophical and ethical discussions -> legal and societal aspects of MIR,"Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> evaluation metrics; MIR tasks -> automatic classification; Musical features and properties -> musical style and genre",Yes,The paper contends that unsatisfactory predictions of MIR based genre classifiers may not indicate algorithmic flaws but rather the evolution of genre.,Yes,Yes,No,000039.pdf,https://archives.ismir.net/ismir2022/paper/000039.pdf,https://drive.google.com/open?id=1N_CcdvwxUOHU_fGO6b0TRhaPS4JZPZ3n,https://drive.google.com/open?id=1XTV9bCJ5QZe2-FOgDldCVUKB7soWMU_j,https://drive.google.com/open?id=1LS4URElVU1MG_JDFyp88qQCK29D6JPo_,,https://slack.com/app_redirect?channel=C04C4QQ0WCX,p3-07-nie
334,2,3,7,"In-person, in Bengaluru",FALSE,In Search of Sañcāras: Tradition-informed Repeated Melodic Pattern Recognition in Carnatic Music,"Carnatic Music is a South Indian art and devotional music practice in which melodic patterns (motifs and phrases), known as sañcāras, play a crucial structural and expressive role. We demonstrate how the combination of transposition invariant features learnt by a Complex Autoencoder (CAE) and predominant pitch tracks extracted using a Frequency-Temporal Attention Network (FTA-Net) can be used to annotate and group regions of variable-length, repeated, melodic patterns in audio recordings of multiple Carnatic Music performances. These models are trained on novel/expert-curated datasets of hundreds of Carnatic audio recordings and the extraction process tailored to account for the unique characteristics of sañcāras in Carnatic Music. Experimental results show that the proposed method is able to identify 54% of all sañcaras annotated by a professional Carnatic vocalist. Code to reproduce and interact with these results is available online.",Thomas Nuttall,thomas.nuttall@upf.edu,"Thomas Nuttall (Music Technology Group, Universitat Pompeu Fabra, Barcelona)*; Genís Plaja-Roglans (Music Technology Group, Universitat Pompeu Fabra, Barcelona); Lara Pearson (Max Planck Institute for Empirical Aesthetics); Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona)","Nuttall, Thomas*; Plaja-Roglans, Genís; Pearson, Lara; Serra, Xavier",thomas.nuttall@upf.edu*; genis.plaja@upf.edu; lara.pearson@ae.mpg.de; xavier.serra@upf.edu,MIR tasks -> pattern matching and detection,"Domain knowledge -> computational ethnomusicology; Domain knowledge -> computational music theory and musicology; Musical features and properties -> melody and motives; Musical features and properties -> timbre, instrumentation, and singing voice",Yes,Computational methods that consider each step of the research pipeline within the context of the musical tradition studied can achieve state-of-the-art results on musicologically relevant tasks such as melodic pattern recognition.,No,Yes,No,000040.pdf,https://archives.ismir.net/ismir2022/paper/000040.pdf,https://drive.google.com/open?id=1TeKu8Xips2y7bmwPOUjA6K4ilwOadhet,https://drive.google.com/open?id=1ISIt3wM8Q8pKZhKbhVWg-Q7ISr-ECRTD,https://drive.google.com/open?id=1ysRgRh7oUb0piTR7uRQCpIiwQjgdSMlM,https://docs.google.com/presentation/d/1Vz8enBgiS9J2AqjH1hMP16CYmyHcqqYlnhQP1L0zGoc/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CGCW91RT,p3-08-nuttall
63,2,3,8,Virtually,FALSE,Automatic Chinese National Pentatonic Modes Recognition Using Convolutional Neural Network,"Chinese national pentatonic modes, with five tones of Gong, Shang, Jue, Zhi and Yu as the core, play an essential role in traditional Chinese music culture. After the early twentieth century, with the development of new Chinese music, the ancient Chinese theory of scales gradually developed into a new pentatonic modes theory under the influence of western music. In this paper, we briefly introduce our self-built CNPM (Chinese National Pentatonic Modes) Dataset, then design residual convolutional neural network models to identify which TongGong system the mode belongs, the pitch of tonic, the mode pattern and the mode type from audio signals, in combination with musical domain knowledge. We use both single-task and multi-task models with three strategies for identification, and compare them with a simple template-based baseline method. In experiments, we use seven accuracy metrics to evaluate the models. The results on identifying both the tonic pitch and the pattern of mode correctly achieve an average accuracy of 69.65%. As an initial research on automatic Chinese national pentatonic modes recognition, this work will contribute to the development of multicultural music information retrieval, computational ethnomusicology and five-tone music therapy.",Wei Li,weili-fudan@fudan.edu.cn,Zhaowen Wang (Central Conservatory of Music); Mingjin Che (Sichuan Conservatory of Music); Yue Yang (Central Conservatory of Music); Wen wu  Meng  (Sichuan Conservatory of Music); Qinyu Li (Sichuan Conservatory Of Music); Fan Xia (Sichuan Conservatory of Music ); Wei Li (Fudan University)*,"Wang, Zhaowen; Che, Mingjin; Yang, Yue; Meng , Wen wu ; Li, Qinyu; Xia, Fan; Li, Wei*",wzw@mail.ccom.edu.cn; 993393523@qq.com; YewYang@mail.ccom.edu.cn; 1056844091@qq.com; 935362804@qq.com; 409769992@qq.com; weili-fudan@fudan.edu.cn*,MIR tasks -> automatic classification,"Domain knowledge -> computational ethnomusicology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Musical features and properties",Yes,An initial research on automatic Chinese national pentatonic modes recognition from audio signals.,Yes,Yes,No,000041.pdf,https://archives.ismir.net/ismir2022/paper/000041.pdf,https://drive.google.com/open?id=1SmG4hcog1aGDre-ai_mBVSkNiURd2mky,https://drive.google.com/open?id=1OO5QLt-Ll_rSK65ElTU1VpnnsWY5oTdI,https://drive.google.com/open?id=1IPqfItMzfupr0WxPdq9AT3RPOH6Q9ZCW,,https://slack.com/app_redirect?channel=C04CCP1UBPY,p3-09-wang
81,2,3,9,Virtually,FALSE,Teach Yourself Georgian Folk Songs Dataset: A Annotated Corpus Of Traditional Vocal Polyphony,"New datasets of non-Western traditional music contribute to the development of knowledge in MIR and allow computational techniques to inform ethnomusicology. We present an annotated dataset of traditional vocal polyphony from two regions of the Republic of Georgia with disparate musical characteristics. The audio for each song consists of four polyphonic recordings of one performance from different microphones. We present a process and workflow that we use to annotate the dataset, which takes advantage of the salience of individual voices in each recording. The process results in an $f_0$ estimate for each vocal part.",David Gillman,dgillman@ncf.edu,David Gillman (New College of Florida)*; Uday Goyat (Georgia Institute of Technology); Atalay Kutlay (New College of Florida),"Gillman, David*; Kutlay, Atalay; Goyat, Uday",dgillman@ncf.edu*; atalay.kutlay18@ncf.edu; udaygoyat45@gmail.com,MIR tasks -> music transcription and annotation,"Applications -> music heritage and sustainability; Evaluation, datasets, and reproducibility -> evaluation metrics; Musical features and properties -> timbre, instrumentation, and singing voice",Yes,"A dataset of Georgian vocal polyphony annotated by means of a new process, and a web interface to view the annotated data.",No,Yes,No,000042.pdf,https://archives.ismir.net/ismir2022/paper/000042.pdf,https://drive.google.com/open?id=1z3l8KSI2W3EubdKicfBRsxFPbXc3iHhP,https://drive.google.com/open?id=1WCurTzvut5fdWEAi4GvkWuH2b6SNwqzM,https://drive.google.com/open?id=1iUO9AOV5BYS29bZeqMAmIv7b4WNeX7CP,https://drive.google.com/open?id=1neYS7m4C_lqHrBNbMMr-a86AGe7EW3aQ,https://slack.com/app_redirect?channel=C04CKB8CK0A,p3-10-gillman
272,2,3,10,Virtually,FALSE,Adapting meter tracking models to Latin American music,"Beat and downbeat tracking models have improved significantly in recent years with the introduction of deep learning methods. However, despite these improvements, several challenges remain. Particularly, the adaptation of available models to underrepresented music traditions in MIR is usually synonymous with collecting and annotating large amounts of data, which is impractical and time-consuming. Transfer learning, data augmentation, and fine-tuning techniques have been used quite successfully in related tasks and are known to alleviate this bottleneck. Furthermore, when studying these music traditions, models are not required to generalize to multiple mainstream music genres but to perform well in more constrained, homogeneous conditions. In this work, we investigate simple yet effective strategies to adapt beat and downbeat tracking models to two different Latin American music traditions and analyze the feasibility of these adaptations in real-world applications concerning the data and computational requirements. Contrary to common belief, our findings show it is possible to achieve good performance by spending just a few minutes annotating a portion of the data and training a model in a standard CPU machine, with the precise amount of resources needed depending on the task and the complexity of the dataset.",Lucas S Maia,lucas.maia@smt.ufrj.br,Lucas S Maia (Federal University of Rio de Janeiro)*; Martín Rocamora (Universidad de la República); Luiz W P  Biscainho (UFRJ); Magdalena Fuentes (New York University),"Maia, Lucas S*; Rocamora, Martín; Biscainho, Luiz W P ; Fuentes, Magdalena",lucas.maia@smt.ufrj.br*; rocamora@fing.edu.uy; wagner@smt.ufrj.br; mgfuenteslujambio@gmail.com,"Musical features and properties -> rhythm, beat, tempo",Applications -> music heritage and sustainability; Domain knowledge -> computational ethnomusicology; Domain knowledge -> machine learning/artificial intelligence for music; Human-centered MIR -> personalization,Yes,"We investigate simple yet effective strategies to adapt beat and downbeat tracking models to two different Latin American music traditions, and analyze the feasibility of these adaptations in real-world applications concerning the data and computational requirements.",Yes,No,No,000043.pdf,https://archives.ismir.net/ismir2022/paper/000043.pdf,https://drive.google.com/open?id=1bphH5JcIcq-4v8AMWlIk81snYc2fVe7J,https://drive.google.com/open?id=1iPl1-HQiUcCQERIUEZEp7NLRPrD77gjZ,https://drive.google.com/open?id=1AJYM4KL8DOtgytZjToJusjiV8ibKJ-hP,,https://slack.com/app_redirect?channel=C04CMQW7C5A,p3-11-maia
235,2,3,11,"In-person, in Bengaluru",FALSE,Critiquing Task- versus Goal-oriented Approaches: A Case for Makam Recognition,"Computational Musicology and Music Information Retrieval (MIR) address the core musical question under study from a different perspective, often a combination of top-down vs. bottom-up approaches. However, the evaluation metrics for MIR tend to capture the model accuracy in terms of the goal. For instance, mode (melodic framework) recognition is implemented with a goal to evaluate and compare melodic analysis approaches, but it is worth investigating if at all it lends itself as one befitting proxy task. In this work, we aim to review whether the model actually learns the task it is intended for. This is particularly relevant in non-Eurogenetic music repertoires where the grammatical rules are rather prescriptive. We employ methodologies that combine domain-knowledge and data-driven optimizations as a possible way for a comprehensive understanding of these relationships. This is tested on Makam which is one of the understudied corpora in MIR. We evaluate an array of feature-engineering methods on the largest mode recognition dataset curated for Ottoman-Turkish makam music, composed of 1000 recordings in 50 makams. We adapted the time-delayed melody surfaces (TDMS) feature, which in combination with support vector machine (SVM) classifier yields 77.2% recognition accuracy, comparable to the current state-of-the-art. We also address (ethno)musicology-driven tasks with a view to gathering deeper insights into this music, such as tuning, intonation, and melodic similarity. We aim to propose avenues to extend the study to makam characterization over the mere goal of recognizing the mode, to better understand the (dis)similarity space and other plausible musically interesting facets.",Kaustuv Kanti Ganguli,kkg3@nyu.edu,Kaustuv Kanti Ganguli (Zayed University)*; Sertan Şentürk (Kobalt Music Group / Independent Researcher); Carlos Guedes (NYU Abu Dhabi),"Ganguli, Kaustuv Kanti*; Şentürk, Sertan; Guedes, Carlos",kkg3@nyu.edu*; sertan.senturk@upf.edu; carlos.guedes@nyu.edu,Domain knowledge -> computational ethnomusicology,"Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> MIR tasks; MIR tasks -> automatic classification; Philosophical and ethical discussions -> philosophical and methodological foundations",Yes,"Amid the goal-oriented MIR problems, we critique whether the computational model learns the musical task it is intended for; a case for makam recognition highlights the musicological insights that we care about, more than the classification accuracy.",No,Yes,No,000044.pdf,https://archives.ismir.net/ismir2022/paper/000044.pdf,https://drive.google.com/open?id=16w-ry78fw4S-5HEUvnJcYXEWsia3F5UN,https://drive.google.com/open?id=1-0vY1Qu69VcngK5sKnq0B8zExsegxTjw,https://drive.google.com/open?id=1t_pw1VcjjbJJXzJtdJsKy6B-HCF6aQDO,https://drive.google.com/open?id=1j4mWP1EDhqDazRS2DX-93uoe77wQAtBY,https://slack.com/app_redirect?channel=C04CGCVGXKP,p3-12-ganguli
245,2,3,12,"In-person, in Bengaluru",FALSE,A Dataset for Greek Traditional and Folk Music: Lyra,"Studying under-represented music traditions under the MIR scope is crucial, not only for developing novel analysis tools, but also for unveiling musical functions that might prove useful in studying world musics. This paper presents a dataset for Greek Traditional and Folk music that includes 1570 pieces, summing in around 80 hours of data. The dataset incorporates YouTube timestamped links for retrieving audio and video, along with rich metadata information with regards to instrumentation, geography and genre, among others. The content has been collected from a Greek documentary series that is available online, where academics present music traditions of Greece with live music and dance performance during the show, along with discussions about social, cultural and musicological aspects of the presented music. Therefore, this procedure has resulted in a significant wealth of descriptions regarding a variety of aspects, such as musical genre, places of origin and musical instruments. In addition, the audio recordings were performed under strict production-level specifications, in terms of recording equipment, leading to very clean and homogeneous audio content. In this work, apart from presenting the dataset in detail, we propose a baseline deep-learning classification approach to recognize the involved musicological attributes. The dataset, the baseline classification methods and the models are provided in public repositories. Future directions for further refining the dataset are also discussed.",Charilaos Papaioannou,cpapaioan@mail.ntua.gr,"Charilaos Papaioannou (School of ECE, National Technical University of Athens)*; Ioannis Valiantzas (Department of Music Studies, National and Kapodistrian University Of Athens); Theodore Giannakopoulos (NCSR Demokritos); Maximos Kaliakatsos-Papakostas (Athena RC); Alexandros Potamianos (National Technical University of Athens)","Papaioannou, Charilaos*; Valiantzas, Ioannis; Giannakopoulos, Theodore; Kaliakatsos-Papakostas, Maximos; Potamianos, Alexandros",cpapaioan@mail.ntua.gr*; ival@music.uoa.gr; tyianak@iit.demokritos.gr; maximos@athenarc.gr; potam@central.ntua.gr,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",Domain knowledge -> computational ethnomusicology,Yes,"This paper presents a novel dataset for Greek Traditional and Folk music that includes 1570 pieces, summing in around 80 hours of data, as well as a deep learning baseline classification approach along with the respective results.",Yes,Yes,No,000045.pdf,https://archives.ismir.net/ismir2022/paper/000045.pdf,https://drive.google.com/open?id=1gOXyVuIhiESwav2kfQLKX2DQ1S_96lUJ,https://drive.google.com/open?id=1Xt4yhB5xGArAHL6EKpe1OUABxTQkHwfA,https://drive.google.com/open?id=1vApZRJ_aVN-q1dwzDfi_045vK77OCNse,https://drive.google.com/open?id=1Z4e-MstRd2iNjI9IG-XAtgzYxJlXRrLd,https://slack.com/app_redirect?channel=C04CGCVLJMB,p3-13-papaioannou
296,2,3,13,"In-person, in Bengaluru",FALSE,Analysis and detection of singing techniques in repertoires of J-POP solo singers,"In this paper, we focus on singing techniques within the scope of music information retrieval research. We investigate how singers use singing techniques using real-world recordings of famous solo singers in Japanese popular music songs (J-POP). First, we built a new dataset of singing techniques. The dataset consists of 168 commercial J-POP songs, and each song is annotated using various singing techniques with timestamps and vocal pitch contours. We also present descriptive statistics of singing techniques on the dataset to clarify what and how often singing techniques appear. We further explored the difficulty of the automatic detection of singing techniques using previously proposed machine learning techniques. In the detection, we also investigate the effectiveness of auxiliary information (i.e., pitch and distribution of label duration), not only providing the baseline. The best result achieves 40.4% at macro-average F-measure on nine-way multi-class detection. We provide the annotation of the dataset and its detail on the appendix website.",Yuya Yamamoto,s2130507@s.tsukuba.ac.jp,Yuya Yamamoto (University of Tsukuba)*; Juhan Nam (KAIST); Hiroko Terasawa (University of Tsukuba),"Yamamoto, Yuya*; Nam, Juhan; Terasawa, Hiroko",s2130507@s.tsukuba.ac.jp*; juhan.nam@kaist.ac.kr; terasawa@slis.tsukuba.ac.jp,Musical features and properties -> expression and performative aspects of music,"Applications -> performance, and production; Domain knowledge -> computational music theory and musicology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> automatic classification; Musical features and properties -> timbre, instrumentation, and singing voice",Yes,"We annotated singing techniques on real-world j-pop vocal recordings, and conducted on singing technique analysis and identification.",Yes,No,No,000046.pdf,https://archives.ismir.net/ismir2022/paper/000046.pdf,https://drive.google.com/open?id=1jz37KNjy-TKBSwd01jMxO8GwqA44ic4R,https://drive.google.com/open?id=1PrxS8bfAamdo3l3xmSdO9NSEnW9Av-ke,https://drive.google.com/open?id=1NevCH-gs0euK96cskq0bicLEmHnZMGAk,https://docs.google.com/presentation/d/1vqwSbeNetO-MxkhLKZXhsanl7pWIw6vnYhKaL_LNIi4/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CMQWDUUU,p3-14-yamamoto
335,2,4,0,"In-person, in Bengaluru",TRUE,Performance MIDI-to-score conversion by neural beat tracking,"Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S .",Lele Liu,lele.liu@qmul.ac.uk,Lele Liu (Queen Mary University of London)*; Qiuqiang Kong (ByteDance); Veronica Morfi (Queen Mary University of London); Emmanouil Benetos (Queen Mary University of London),"Liu, Lele*; Kong, Qiuqiang; Morfi, Veronica; Benetos, Emmanouil",lele.liu@qmul.ac.uk*; kongqiuqiang@bytedance.com; g.v.morfi@qmul.ac.uk; emmanouil.benetos@qmul.ac.uk,MIR tasks -> music transcription and annotation,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo",No,"We propose a method using convolutional-recurrent neural networks for performance MIDI-to-score conversion by tracking beats, quantised note times, key signatures, time signatures, and hand parts in a note sequence.",Yes,No,Yes,000047.pdf,https://archives.ismir.net/ismir2022/paper/000047.pdf,https://drive.google.com/open?id=1_UAm2QXD12IhwzmaR2nCHqBclWZj0Dd3,https://drive.google.com/open?id=1iBziT4mXvY8sj3Q3zYvSiAs3mdRzRFDJ,https://drive.google.com/open?id=1ZSJreE-WDJPheZ6_mcFMMwun8GnzPNQx,https://docs.google.com/presentation/d/1Z8tfP14MoqEEaLKfn7NlJKByP08AImYR64Snv4F1RV8/edit?usp=sharing,https://slack.com/app_redirect?channel=C04C4QRE9P1,p4-01-liu
66,2,4,1,Virtually,FALSE,Symbolic Music Loop Generation with Neural Discrete Representations,"Since most of music has repetitive structures from motifs to phrases, repeating musical ideas can be a basic operation for music composition. The basic block that we focus on is conceptualized as loops which are essential ingredients of music. Furthermore, meaningful note patterns can be formed in a finite space, so it is sufficient to represent them with combinations of discrete symbols as done in other domains. In this work, we propose symbolic music loop generation via learning discrete representations. We first extract loops from MIDI datasets using a loop detector and then learn an autoregressive model trained by discrete latent codes of the extracted loops. We show that our model outperforms well-known music generative models in terms of both fidelity and diversity, evaluating on random space. Our code and supplementary materials are available at https://github.com/sjhan91/Loop_VQVAE_Official.",Sangjun Han,sj.han@lgresearch.ai,Sangjun Han (LG AI Research)*; Hyeongrae Ihm (LG AI Research); Moontae Lee (University of Illinois at Chicago); Woohyung Lim (LG AI Research),"Han, Sangjun*; Ihm, Hyeongrae; Lee, Moontae; Lim, Woohyung",sj.han@lgresearch.ai*; hrim@lgresearch.ai; moontae@uic.edu; w.lim@lgresearch.ai,MIR tasks -> music generation,Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> expression and performative aspects of music,No,We focus on extracting symbolic loop phrases using an audio-domain loop detector and generating them using an autoregressive model trained by discrete latent features.,No,Yes,No,000048.pdf,https://archives.ismir.net/ismir2022/paper/000048.pdf,https://drive.google.com/open?id=1gCIBEdycNqh6biSEXvnEr_PPw8jC0cGf,https://drive.google.com/open?id=1BQ5-xEjzL7cMUjiljAp3d2DkGyK9qwvo,https://drive.google.com/open?id=16wdCJIQK8wnkfsSp3h9t4lwWIcA8mxuQ,,https://slack.com/app_redirect?channel=C04CGCU2S0M,p4-02-han
11,2,4,2,"In-person, in Bengaluru",FALSE,Automatic music mixing with deep learning and out-of-domain data,"Music mixing traditionally involves recording instruments in the form of clean, individual tracks and blending them into a final mixture using audio effects and expert knowledge (e.g., a mixing engineer). The automation of music production tasks has become an emerging field in recent years, where rule-based methods and machine learning approaches have been explored. Nevertheless, the lack of dry or clean instrument recordings limits the performance of such models, which is still far from professional human-made mixes. We explore whether we can use out-of-domain data such as wet or processed multitrack music recordings and repurpose it to train supervised deep learning models that can bridge the current gap in automatic mixing quality. To achieve this we propose a novel data preprocessing method that allows the models to perform automatic music mixing. We also redesigned a listening test method for evaluating music mixing systems. We validate our results through such subjective tests using highly experienced mixing engineers as participants.",Marco A Martinez Ramirez,marco.martinez@sony.com,"Marco A Martinez Ramirez (Sony Group Corporation, Tokyo, Japan)*; WeiHsiang Liao (Sony Group Corporation, Tokyo, Japan); Chihiro Nagashima (Sony Group Corporation, Tokyo, Japan); Giorgio Fabbro (Sony Europe B.V., Stuttgart, Germany); Stefan Uhlich (Sony Europe B.V., Stuttgart, Germany); Yuki Mitsufuji (Sony Group Corporation, Tokyo, Japan)","Martinez Ramirez, Marco A*; Liao, WeiHsiang; Nagashima, Chihiro; Fabbro, Giorgio; Uhlich, Stefan; Mitsufuji, Yuki",marco.martinez@sony.com*; weihsiang.liao@sony.com; chihiro.nagashima@sony.com; giorgio.fabbro@sony.com; stefan.uhlich@sony.com; Yuhki.Mitsufuji@sony.com,Domain knowledge -> machine learning/artificial intelligence for music,"Applications -> performance, and production; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> musical affect, emotion and mood",No,We explore whether we can use wet or processed multitrack music data and repurpose it to train supervised deep learning models that perform automatic music mixing.,No,Yes,No,000049.pdf,https://archives.ismir.net/ismir2022/paper/000049.pdf,https://drive.google.com/open?id=1cEgcVYs0PxgsXHdJx5sLOpxW6GivpY4J,https://drive.google.com/open?id=11_9PlJlhKdYVEOvm8YGOuW3FeBJ-VZpo,https://drive.google.com/open?id=102P91LePzzQTV-zdeul4BX8q5REUsdWH,https://drive.google.com/open?id=1KiYN6bkTKZE9MSxfUj7z5ZrRtScF9jul,https://slack.com/app_redirect?channel=C04CGCTK9NZ,p4-03-ramirez
139,2,4,3,Virtually,FALSE,Music-STAR: a Style Translation system for Audio-based Re-instrumentation,"Music style translation aims to generate variations of existing pieces of music by altering the style-related characteristics of the original piece while content, such as the melody, remains unchanged. These alterations could involve timbre translation, re-harmonization, or music rearrangement. Previous studies have achieved promising results utilizing time-frequency and symbolic music representations. Music style translation on raw audio has also been investigated and applied to single-instrument pieces. Although processing raw audio is more challenging, it provides richer information about timbres, dynamics, and articulations.
In this paper, we introduce Music-STAR, the first audio-based translation system that translates the existing instruments in a piece into a set of target instruments without using source separation. To conduct our experiments, we also present an audio dataset that contains two-track pieces performed by two instrument sets alongside their stems. We carry out subjective and objective evaluations to compare Music-STAR with a variety of baseline methods and demonstrate its superiority.",Mahshid Alinoori,mahshida@yorku.ca,"Mahshid Alinoori (Department of Electrical Engineering and Computer Science, York University, Canada)*; Vassilios Tzerpos (Department of Electrical Engineering and Computer Science, York University, Canada)","Alinoori, Mahshid*; Tzerpos, Vassilios",mahshid.alinoori@gmail.com*; bil@cse.yorku.ca,MIR tasks -> music synthesis and transformation,"Musical features and properties -> musical style and genre; Musical features and properties -> timbre, instrumentation, and singing voice",No,"This paper introduces Music-STAR, the first audio-based multi-instrument music translation system that translates the existing instruments in a piece into a set of target instruments without using source separation.",No,Yes,No,000050.pdf,https://archives.ismir.net/ismir2022/paper/000050.pdf,https://drive.google.com/open?id=1jQifWWW-m1J2KvfORdDMM97DaTpDrf28,https://drive.google.com/open?id=1gu839MnctG_GecdOT81yvT6GGovAQ8B3,https://drive.google.com/open?id=1u1jXxXFS60oERhXk2whvvYVSgTIh1sv_,,https://slack.com/app_redirect?channel=C04CCP2N0NA,p4-04-alinoori
29,2,4,4,"In-person, in Bengaluru",FALSE,Learning Unsupervised Hierarchies of Audio Concepts,"Music signals are difficult to interpret from their low-level features, perhaps even more than images: e.g. highlighting part of a spectrogram or an image is often insufficient to convey high-level ideas that are genuinely relevant to humans. In computer vision, concept learning was therein proposed to adjust explanations to the right abstraction level (e.g. detect clinical concepts from radiographs). These methods have yet to be used for MIR.

In this paper, we adapt concept learning to the realm of music, with its particularities. For instance, music concepts are typically non-independent and of mixed nature (e.g. genre, instruments, mood), unlike previous work that assumed disentangled concepts.
We propose a method to learn numerous music concepts from audio and then automatically hierarchise them to expose their mutual relationships. We conduct experiments on datasets of playlists from a music streaming service, serving as a few annotated examples for diverse concepts. Evaluations show that the mined hierarchies are aligned with both ground-truth hierarchies of concepts -- when available -- and with proxy sources of concept similarity in the general case.",Darius Afchar,darius.afchar@live.fr,Darius Afchar (Deezer Research)*; Romain Hennequin (Deezer Research); Vincent Guigue (LIP6),"Afchar, Darius*; Hennequin, Romain; Guigue, Vincent",darius.afchar@live.fr*; rhennequin@deezer.com; vincent.guigue@lip6.fr,Domain knowledge -> representations of music,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; Musical features and properties -> musical affect, emotion and mood; Musical features and properties -> musical style and genre",No,"We propose to learn meaningful music concepts (eg. genre and mood) in a few-shot setting to interpret audio signals; and then, building on top of this, we derive a hierarchy of the learned concepts in an unsupervised manner to organise and navigate them.",Yes,Yes,No,000051.pdf,https://archives.ismir.net/ismir2022/paper/000051.pdf,https://drive.google.com/open?id=1-iaBvVq0MRFBdm1wnl5TlOpkKodct-YD,https://drive.google.com/open?id=12qKnF-rdZocB5ibGX2GXm0kIHP2GnBJo,https://drive.google.com/open?id=1kkweuOto1zjTcZVj8ttOSRMOUTxA0jIZ,https://docs.google.com/presentation/d/1Q1tzCZICsSHZp70Y2dBBnGtpZvy432lxZwd8bwWJj5E/,https://slack.com/app_redirect?channel=C04CMQU38A0,p4-05-afchar
240,2,4,5,Virtually,FALSE,Multi-objective Hyper-parameter Optimization of Behavioral Song Embeddings,"Song embeddings are a key component of most music recommendation engines. 
In this work, we study the hyper-parameter optimization of behavioral song embeddings based on Word2Vec on a selection of downstream tasks, namely next-song recommendation, false neighbor rejection, and artist and genre clustering. We present new optimization objectives and metrics to monitor the effects of hyper-parameter optimization. We show that single-objective optimization can cause side effects on the non optimized metrics and propose a simple multi-objective optimization to mitigate these effects.
We find that next-song recommendation quality of Word2Vec is anti-correlated with song popularity, and we show how song embedding optimization can balance performance across different popularity levels.
We then show potential positive downstream effects on the task of play prediction.
Finally, we provide useful insights on the effects of training dataset scale by testing hyper-parameter optimization on an industry-scale dataset.",Massimo Quadrana,mquadrana@apple.com,Massimo Quadrana (Apple)*; Antoine Larreche-Mouly (Apple); Matthias Mauch (Apple),"Quadrana, Massimo*; Larreche-Mouly, Antoine; Mauch, Matthias",mquadrana@apple.com*; alarreche@apple.com; mmauch@apple.com,Applications -> music recommendation and playlist generation,"Applications -> music retrieval systems; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> evaluation metrics; Human-centered MIR -> personalization; Human-centered MIR -> user behavior analysis and mining",No,We show that single-objective hyper-parameter optimization of behavioral song embeddings based on Word2Vec can cause side effects on the non optimized metrics and propose a simple multi-objective optimization to mitigate these effects.,No,Yes,No,000052.pdf,https://archives.ismir.net/ismir2022/paper/000052.pdf,https://drive.google.com/open?id=14JKPPbprtzNGzgWYc6WZyF0KVfnLyrid,https://drive.google.com/open?id=1EsCrX2A77sjsveov_RMPYHBWpGplILMh,https://drive.google.com/open?id=1g-nyma06cOyMbIZJVsAVUt8jZHwrTZbx,,https://slack.com/app_redirect?channel=C04D92HMC80,p4-06-quadrana
70,2,4,6,"In-person, in Bengaluru",FALSE,ATEPP: A Dataset of Automatically Transcribed Expressive Piano Performance,"Computational models of expressive piano performance rely on attributes like tempo, timing, dynamics and pedalling. Despite some promising models for performance assessment and performance rendering, results are limited by the scale, breadth and uniformity of existing datasets. In this paper, we present ATEPP, a dataset that contains 1000 hours of performances of standard piano repertoire by 49 world-renowned pianists, organized and aligned by compositions and movements for comparative studies. Scores in MusicXML format are also available for around half of the tracks. We first evaluate and verify the use of transcribed MIDI for representing expressive performance with a listening evaluation that involves recent transcription models. Then, the process of sourcing and curating the dataset is outlined, including composition entity resolution and a pipeline for audio matching and solo filtering. Finally, we conduct baseline experiments for performer identification and performance rendering on our datasets, demonstrating its potential in generalizing expressive features of individual performing style.",Huan Zhang,huan.zhang@qmul.ac.uk,Huan Zhang (Queen Mary University of London)*; Jingjing Tang (Queen Mary University of London); Syed RM Rafee (Queen Mary University of London); Simon Dixon (Queen Mary University of London); George Fazekas (Queen Mary University of London); Geraint A. Wiggins (Vrije Universiteit Brussel),"Zhang, Huan*; Tang, Jingjing; Rafee, Syed RM; Dixon, Simon; Fazekas, George; Wiggins, Geraint A.",huan.zhang@qmul.ac.uk*; jingjing.tang@qmul.ac.uk; s.rafee@qmul.ac.uk; s.e.dixon@qmul.ac.uk; george.fazekas@qmul.ac.uk; geraint.wiggins@qmul.ac.uk,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Applications -> performance, and production; Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> expression and performative aspects of music",No,A novel dataset dedicated to computational expressive piano performance research.,Yes,Yes,No,000053.pdf,https://archives.ismir.net/ismir2022/paper/000053.pdf,https://drive.google.com/open?id=1jtKPOklu0VSGfQkXaO-yKqbHxI2ixNx2,https://drive.google.com/open?id=1oFKridMHFoDZDUAY-0XbxtDUReuLOdOa,https://drive.google.com/open?id=1ClnMf16jVdvey5wwSAMy2ezVOQHJfZDP,https://docs.google.com/presentation/d/1gRhSk8a7vxiCFjmSZXZDv2s0D40YplBm/edit?usp=sharing&ouid=107643094600257118451&rtpof=true&sd=true,https://slack.com/app_redirect?channel=C04CCP21PDL,p4-07-zhang
180,2,4,7,Virtually,FALSE,PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription,"Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of the paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.",Kejun Zhang,zhangkejun@zju.edu.cn,Chen Zhang (Zhejiang University); Jiaxing Yu (Zhejiang University); LuChin Chang (Zhejiang University ); Xu Tan (Microsoft Research Asia); Jiawei Chen (South China University of Technology); Tao Qin (Microsoft Research Asia); Kejun Zhang (Zhejiang University)*,"Zhang, Chen; Yu, Jiaxing; Chang, LuChin; Tan, Xu; Chen, Jiawei; Qin, Tao; Zhang, Kejun*",zc99@zju.edu.cn; yujxzju@gmail.com; changluchin@gmail.com; xuta@microsoft.com; csjiaweichen@mail.scut.edu.cn; taoqin@microsoft.com; zhangkejun@zju.edu.cn*,MIR fundamentals and methodology -> lyrics and other textual data,"MIR fundamentals and methodology -> web mining, and natural language processing",No,We provide an **easy** data augmentation pipeline for ALT task without a complex training process and the usage of speech-singing paired data.,Yes,No,No,000054.pdf,https://archives.ismir.net/ismir2022/paper/000054.pdf,https://drive.google.com/open?id=1w1zfWAEhYI4oJtVEmwWf-TxZ_Qd5wVMz,https://drive.google.com/open?id=1YIm3VNvWdeZ5Ljjj69gAD0OeLf59VQJo,https://drive.google.com/open?id=1WtUm8foit1SlCh8cf41aqw6iXoPMs3N6,,https://slack.com/app_redirect?channel=C04D92H794G,p4-08-zhang
107,2,4,8,"In-person, in Bengaluru",FALSE,Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures,"Standard evaluation metrics such as the Inception score and Fréchet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics in response to control-parameter variations for audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures.",Chitralekha Gupta,chitralekha@u.nus.edu,Chitralekha Gupta (National University of Singapore)*; Yize Wei (National University of Singapore); Zequn Gong (National University of Singapore); Purnima Kamath (National University of Singapore); Zhuoyao Li (National University of Singapore); Lonce Wyse (National University of Singapore),"Gupta, Chitralekha*; Wei, Yize; Gong, Zequn; Kamath, Purnima; Li, Zhuoyao; Wyse, Lonce",chitralekha@u.nus.edu*; yize.wei@u.nus.edu; zequn.gong@u.nus.edu; purnima.kamath@u.nus.edu; zhuoyaoli@u.nus.edu; lonce.wyse@nus.edu.sg,"Evaluation, datasets, and reproducibility -> evaluation metrics","Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> music synthesis and transformation",No,We present a systematic study of the sensitivity of some of the existing audio quality evaluation metrics as well as some potential ones such as Gram-matrix based metrics to parameter variations in audio textures.,No,No,No,000055.pdf,https://archives.ismir.net/ismir2022/paper/000055.pdf,https://drive.google.com/open?id=1AYr3JCRtjSpidj6Yuk76Lsnf8VRPVO0l,https://drive.google.com/open?id=1ehr0qU4earuZdO-FcDTswOleeAB4yK-x,https://drive.google.com/open?id=1mzUe0z4BUL0n6f8aHTP-MU0TyQkdd8VA,https://docs.google.com/presentation/d/1ACoN7UmTNeIwm_j77CX4N1ijqmEkTj-ZKkoMu6DYRNw/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CY0MDC8H,p4-09-gupta
256,2,4,9,Virtually,FALSE,Stability of Symbolic Feature Group Importance in the Context of Multi-Modal Music Classification,"Multi-modal music classification creates supervised models trained on features from different sources (modalities): the audio signal, the score, lyrics, album covers, expert tags, etc. A concept of “multi-group feature importance” not only helps to measure the individual relevance of features of a feature type under investigation (such as the instruments present in a piece), but also serves to quantify the potential for further improving classification by adding features from other feature types or extracted from different kinds of sources, based on a multi-objective analysis of feature sets after evolutionary feature selection. In this study, we investigate the stability of feature group importance when different classification methods and different measures of classification quality are applied. Since musical scores are particularly helpful in deriving semantically meaningful, robust genre characteristics, we focus on the feature groups analyzed by the jSymbolic feature extraction software, which describe properties associated with instrumentation, basic pitch statistics, melody, chords, tempo, and other rhythmic aspects. These symbolic features are analyzed in the context of musical information drawn from five other modalities, and experiments are conducted involving two datasets, one small and one large. The results show that, although some feature groups can remain similarly important compared to others, differences can also be evident in various applications, and can depend on the particular classifier and evaluation measure being used. Insights drawn from this type of analysis can potentially be helpful in effectively matching specific features or feature groups to particular classifiers and evaluation measures in future feature-based MIR research.",Igor Vatolkin,igor.vatolkin@udo.edu,"Igor Vatolkin (Department of Computer Science, TU Dortmund University)*; Cory McKay (Department of Liberal and Creative Arts, Marianopolis College)","Vatolkin, Igor*; McKay, Cory",igor.vatolkin@udo.edu*; cory.mckay@mail.mcgill.ca,MIR fundamentals and methodology -> multimodality,"MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> timbre, instrumentation, and singing voice",No,Measurement of importance of symbolic features within a multi-modal music classification framework,No,Yes,No,000056.pdf,https://archives.ismir.net/ismir2022/paper/000056.pdf,https://drive.google.com/open?id=1314qVAvj9ehZlgm29k6YJxI8IXqaM4WY,https://drive.google.com/open?id=1kAQDz6jpiOgdMTHXW6RqKIc9QJpora1n,https://drive.google.com/open?id=148fVwwzWbAoqYcnP70H2tsNiCnHAgekv,,https://slack.com/app_redirect?channel=C04CKB9UD7C,p4-10-vatolkin
138,2,4,10,"In-person, in Bengaluru",FALSE,Multi-pitch Estimation meets Microphone Mismatch: Applicability of Domain Adaptation,"The performance of machine learning (ML) models is known to be affected by discrepancies between training (source) and real-world (target) data distributions. This problem is referred to as domain shift and is commonly approached using domain adaptation (DA) methods. As one relevant scenario, automatic piano transcription algorithms in music learning applications potentially suffer from domain shift since pianos are recorded in different acoustic conditions using various devices. Yet, most currently available datasets for piano transcription only cover ideal recording situations with high-quality microphones.
 Consequently, a transcription model trained on these datasets will face a mismatch between source and target data in real-world scenarios.
 To address this issue, we employ a recently proposed dataset which includes annotated piano recordings covering typical real-life recording settings for a piano learning application on mobile devices.
 We first quantify the influence of the domain shift on the performance of a deep learning-based piano multi-pitch estimation (MPE) algorithm.
 Then, we employ and evaluate four unsupervised DA methods to reduce domain shift.
 Our results show that the studied MPE model is surprisingly robust to domain shift in microphone mismatch scenarios and the DA methods do not notably improve the transcription performance.",Franca Bittner,franca.bittner@idmt.fraunhofer.de,"Franca Bittner (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)*; Marcel Gonzalez (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany); Maike L Richter (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany); Hanna Lukashevich (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany); Jakob Abeßer (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)","Bittner, Franca*; Gonzalez, Marcel; Richter, Maike L; Lukashevich, Hanna; Abeßer, Jakob",franca.bittner@idmt.fraunhofer.de*; marcel.gonzalez@tu-ilmenau.de; maike.richter@tu-ilmenau.de; hanna.lukashevich@idmt.fraunhofer.de; jakob.abesser@idmt.fraunhofer.de,Domain knowledge -> machine learning/artificial intelligence for music,"Applications -> music training and education; Evaluation, datasets, and reproducibility -> MIR tasks; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music transcription and annotation",No,"Multi-pitch estimation models for automatic piano transcription might be robust to domain shift caused by a microphone mismatch, so domain adaptation methods do not improve their performance significantly.",Yes,Yes,No,000057.pdf,https://archives.ismir.net/ismir2022/paper/000057.pdf,https://drive.google.com/file/d/1fx80IzmtDUmcRxWpdmd-Oj8U_ThZstyN,https://drive.google.com/file/d/1xnO0ud5zxHf6ZBYlMxvzoBtQ7vE02LuN/view?usp=share_link,https://drive.google.com/file/d/13wnMsMPB2DJ7cwYpi2jeAzSWAF135vg_/view?usp=share_link,https://drive.google.com/file/d/1qrJZRBqcJV1sBEq5lGmZ47o8vrkHDcXW/view?usp=share_link,https://slack.com/app_redirect?channel=C04D92GS2G0,p4-11-bittner
300,2,4,11,Virtually,FALSE,Melody transcription via generative pre-training,"Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in *melody transcription* is building methods which can handle broad audio containing any number of instrument ensembles and musical styles---existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data---we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build a system capable of transcribing human-readable lead sheets directly from music audio.",Chris Donahue,cdonahue@cs.stanford.edu,Chris Donahue (Stanford University)*; John Thickstun (Stanford University); Percy Liang (Stanford University),"Donahue, Chris*; Thickstun, John; Liang, Percy",cdonahue@cs.stanford.edu*; jthickstun@stanford.edu; pliang@cs.stanford.edu,MIR tasks -> music transcription and annotation,"Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; Evaluation, datasets, and reproducibility -> MIR tasks; Evaluation, datasets, and reproducibility -> novel datasets and use cases",No,We substantially improve performance on the task of melody transcription of arbitrary Western music audio by combining generative pre-training with a new dataset.,No,Yes,No,000058.pdf,https://archives.ismir.net/ismir2022/paper/000058.pdf,https://drive.google.com/open?id=1VRQPvGYsS5NxvdY-iUQXwhvAh8AQnaCh,https://drive.google.com/open?id=1IpcYQkxfDpC45iZHAkdzBFczyTVzNKWr,https://drive.google.com/open?id=1JHATdmEjh9NaN2r2NZsa1UhP9qEP_pfA,,https://slack.com/app_redirect?channel=C04CCP4146A,p4-12-donahue
187,2,4,12,"In-person, in Bengaluru",FALSE,Source Separation of Piano Concertos with Test-Time Adaptation,"Music source separation (MSS) aims at decomposing a music recording into constituent sources, such as a lead instrument and the accompaniment. Despite the difficulties in MSS due to the high correlation of musical sources in time and frequency, deep neural networks (DNNs) have led to substantial improvements to accomplish this task. For training supervised machine learning models such as DNNs, isolated sources are required. In the case of popular music, one can exploit open-source datasets which involve multitrack recordings of vocals, bass, and drums. For western classical music, however, isolated sources are generally not available. In this article, we consider the case of piano concertos, which are composed for a pianist typically accompanied by an orchestra. The lack of multitrack recordings makes training supervised machine learning models for the separation of piano and orchestra challenging. To overcome this problem, we generate artificial training material by randomly mixing sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies) to train state-of-the-art DNN models for MSS. As our main contribution, we propose a test-time adaptation (TTA) procedure, which exploits random mixtures of the piano-only and orchestra-only parts in the test data to further improve the separation quality.",Yigitcan Özer,yigitcan.oezer@audiolabs-erlangen.de,"Yigitcan Özer (International Audio Laboratories Erlangen, Germany)*; Meinard Müller (International Audio Laboratories Erlangen, Germany)","Özer, Yigitcan*; Müller, Meinard",yigitcan.oezer@audiolabs-erlangen.de*; meinard.mueller@audiolabs-erlangen.de,MIR tasks -> sound source separation,"Applications -> performance, and production; Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> evaluation metrics; Musical features and properties -> structure, segmentation, and form",No,"We propose a test-time adaptation procedure, which exploits random mixtures of the piano-only and orchestra-only parts in piano concertos to finetune the separation quality of a pre-trained separation model, which is based on randomly mixed sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies).",Yes,Yes,No,000059.pdf,https://archives.ismir.net/ismir2022/paper/000059.pdf,https://drive.google.com/open?id=1RteG0EhWOO2uvE7i9jAq0xc2IqpzmGxb,https://drive.google.com/open?id=1WTzoSalucfsyY7tiqXtR9I64Kh97fJLS,https://drive.google.com/open?id=13XEZ01uTPcqKzb91k4L35nI-alK_YEa-,https://docs.google.com/presentation/d/1xCM5qn6mQMKIBk6-i8svjqew5DLlfi8O/edit?usp=sharing&ouid=102045217384450827259&rtpof=true&sd=true,https://slack.com/app_redirect?channel=C04CMQVHBRA,p4-13-özer
44,2,4,13,Virtually,FALSE,Counterpoint Error-Detection Tools for Optical Music Recognition of Renaissance Polyphonic Music,"This paper discusses part of a larger project to preserve and increase access to Guatemalan music sources written in mensural notation by using a digitization and music information retrieval (MIR) workflow to obtain both digital images and symbolic scores with editorial corrections. The workflow involves MIR tools such as optical music recognition (OMR), automatic voice alignment for mensural notation, editorial correction software, and computational counterpoint error detection. 
 In this paper, we evaluate whether the use of automatic counterpoint error-detection tools makes the correction process more efficient. The results confirm that marking illegal dissonances in the score following the rules of Renaissance counterpoint indeed makes the process of editorial correction of scribal errors in Renaissance music more efficient by reducing the time taken and improving the accuracy of such corrections. Moreover, marking the illegal dissonances in the score also allowed us to catch OMR errors that had passed through undetected at a previous stage of the workflow.",Martha E Thomae Elias,martha.thomaeelias@mail.mcgill.ca,"Martha E Thomae (Schulich School of Music, McGill University)*; Julie E Cumming (Schulich School of Music, McGill University); Ichiro Fujinaga (Schulich School of Music, McGill University)","Thomae Elias, Martha E*; Cumming, Julie; Fujinaga, Ichiro",martha.thomaeelias@mail.mcgill.ca*; julie.cumming@mcgill.ca; ichiro.fujinaga@mcgill.ca,Domain knowledge -> computational music theory and musicology,Applications -> digital libraries and archives; Applications -> music heritage and sustainability; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> optical music recognition,No,The use of computational music analysis tools allows for identifying OMR and scribal errors in Renaissance music sources more easily by reducing the time and incrementing the accuracy of such corrections.,Yes,No,No,000060.pdf,https://archives.ismir.net/ismir2022/paper/000060.pdf,https://drive.google.com/open?id=1WNvEy2FaYUJDdskqJDekvn9HB7LojNIm,https://drive.google.com/open?id=1vCGz2bH2Wg_j6FkY_DG3oXu1IcsVYCve,https://drive.google.com/open?id=1jnrQaG3C-tBPCi2wRXw8xt3Hd8X_gZ0g,,https://slack.com/app_redirect?channel=C04D92FSND6,p4-14-thomae-elias
236,2,4,14,"In-person, in Bengaluru",FALSE,A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas,"Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture. These descriptors can be correlated with texture annotations and used in different machine-learning tasks. Along with provided data, they offer promising applications in computer assisted music analysis and composition.",Louis Couturier,louis.couturier@u-picardie.fr,"Louis Couturier (MIS, Université de Picardie Jules Verne, Amiens, France)*; Louis Bigo (CRIStAL, UMR 9189 CNRS, Université de Lille, France); Florence Leve (MIS, Université de Picardie Jules Verne, Amiens, France, CRIStAL, UMR 9189 CNRS, Université de Lille, France)","Couturier, Louis*; Bigo, Louis; Leve, Florence",louis.couturier@u-picardie.fr*; louis.bigo@univ-lille.fr; Florence.Leve@u-picardie.fr,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",Domain knowledge -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music transcription and annotation; Musical features and properties,No,"We provide a texture annotation dataset of 9 movements of Piano sonata scores by W.A. Mozart, along with a statistical analysis of the dataset, offering promising further developments for computer assisted music analysis and composition.",Yes,Yes,No,000061.pdf,https://archives.ismir.net/ismir2022/paper/000061.pdf,https://drive.google.com/open?id=1UNbASOzT42sxRq9xvbMgpWhf6Ai8p0p9,https://drive.google.com/open?id=189X9J0QP8Gz6YUrBrankyss41klPHwW6,https://drive.google.com/open?id=10t3C2aus761kdF1ulIte-A9L8wJSTs8m,https://drive.google.com/open?id=1RSEkZ5MKkUlDpMEieeoWSgLGLyxDoEpH,https://slack.com/app_redirect?channel=C04CGCVHLUD,p4-15-couturier
247,2,4,15,"In-person, in Bengaluru",FALSE,Violin Etudes: A Comprehensive Dataset for f0 Estimation and Performance Analysis,"Violin performance analysis requires accurate and robust f0 estimates to give feedback on the playing accuracy. Despite the recent advancements in data-driven f0 estimators, their application to performance analysis remains a challenge due to style-specific and dataset-induced biases. In this paper, we address this problem by introducing Violin Etudes, a 27.8-hours violin performance dataset constructed with domain knowledge in instrument pedagogy and a novel automatic f0-labeling paradigm. Experimental results on unseen datasets show that the CREPE f0 estimator trained on Violin Etudes outperforms the widely-used pre-trained version trained on multiple manually-labeled datasets. Further preliminary findings suggest that (i) existing data-driven f0 estimators may overfit to equal temperament, and (ii) iterative re-labeling regularized by our novel Constrained Harmonic Resynthesis method can simultaneously enhance datasets and f0 estimators. Our dataset curation methodology is easily scalable to other instruments owing to the quantity of pedagogical data online. It also supports a range of MIR research directions thanks to the performance difficulty labels from educational institutions.",Nazif Can Tamer,nazifcan.tamer@upf.edu,"Nazif Can Tamer (Music Technology Group, Universitat Pompeu Fabra, Barcelona)*; Pedro Ramoneda (Music Technology Group, Universitat Pompeu Fabra, Barcelona); Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona)","Tamer, Nazif Can*; Ramoneda, Pedro; Serra, Xavier",nazifcan.tamer@upf.edu*; pedro.ramoneda@upf.edu; xavier.serra@upf.edu,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",Applications -> music training and education; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music transcription and annotation,No,"We present the Violin Etudes, a 27-hour solo violin dataset with automatically extracted f0 annotations, and show that an f0 estimator trained only in this data outperforms its widely-used pretrained version trained on multiple manually-labeled datasets, even its own train data.",Yes,No,No,000062.pdf,https://archives.ismir.net/ismir2022/paper/000062.pdf,https://drive.google.com/open?id=1IPSsENILVK8vxVgU3UIxr7jdaXa4igHO,https://drive.google.com/open?id=1Pe6eIW16fjgO0ETh3Q9sd_JjwJSE_KN8,https://drive.google.com/open?id=1lejNaNGIyLL-wdhandNH3zZsyMd0h_U3,https://docs.google.com/presentation/d/1vZY1jPw0EHtTtabWBwtSzRa_OxNdjTG4Y2ACjtedWsg/edit?usp=sharing,https://slack.com/app_redirect?channel=C04C4QQSSR5,p4-16-tamer
278,2,4,16,"In-person, in Bengaluru",FALSE,Checklist Models for Improved Output Fluency in Piano Fingering Prediction,"In this work we present a new approach for the task of predicting fingerings for piano music. While prior neural approaches have often treated this as a sequence tagging problem with independent predictions, we put forward a checklist system, trained via reinforcement learning, that maintains a representation of recent predictions in addition to a hidden state, allowing it to learn soft constraints on output structure. We also demonstrate that by modifying input representations --- which in prior work using neural models have often taken the form of one-hot encodings over individual keys on the piano --- to encode relative position on the keyboard to the prior note instead, we can achieve much better performance. Additionally, we reassess the use of raw per-note labeling precision as an evaluation metric, noting that it does not adequately measure the fluency, i.e. human playability, of a model's output. To this end, we compare methods across several statistics which track the frequency of adjacent finger predictions that while independently reasonable would be physically challenging to perform in sequence, and implement a reinforcement learning strategy to minimize these as part of our training loss. Finally through human expert evaluation, we demonstrate significant gains in performability directly attributable to improvements with respect to these metrics.",Nikita Srivatsan,nsrivats@cmu.edu,Nikita Srivatsan (Carnegie Mellon University)*; Taylor Berg-Kirkpatrick (UC San Diego),"Srivatsan, Nikita*; Berg-Kirkpatrick, Taylor",nsrivats@cmu.edu*; tberg@eng.ucsd.edu,Domain knowledge -> machine learning/artificial intelligence for music,"Evaluation, datasets, and reproducibility -> evaluation metrics; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music transcription and annotation; Musical features and properties -> expression and performative aspects of music",No,"We present a checklist based model for predicting fingerings of piano music, which by maintaining an explicit representation of previous predictions allows for more locally fluent outputs, and evaluate performance using new metrics which more directly assess human playability than raw precision.",Yes,Yes,No,000063.pdf,https://archives.ismir.net/ismir2022/paper/000063.pdf,https://drive.google.com/open?id=1RqjP2h5ag5vKCHu74UP_e47X0oITpCfr,https://drive.google.com/open?id=1Xg6yXeePIMzTGwW9cIYD8OrhnMTleR3o,https://drive.google.com/open?id=1T566PJrcDZKQioNyBGuZGIT0gKPtC0mt,https://drive.google.com/open?id=1nxxFcIHkySfjXN59Z6_BrjsO9DpZURmo,https://slack.com/app_redirect?channel=C04CGCVTYER,p4-17-srivatsan
182,3,5,0,Virtually,TRUE,Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations,"Reading, much like music listening, is an immersive experience that transports readers while taking them on an emotional journey. Listening to complementary music has the potential to amplify the reading experience, especially when the music is stylistically cohesive and emotionally relevant. In this paper, we propose the first fully automatic method to build a dense soundtrack for books, which can play high-quality instrumental music for the entirety of the reading duration. Our work employs a unique text processing and music weaving pipeline that determines the context and emotional composition of scenes in a chapter. This allows our method to identify and play relevant excerpts from the soundtrack of the book's movie adaptation. By relying on the movie composer's craftsmanship, our book soundtracks include expert-made motifs and other scene-specific musical characteristics. We validate the design decisions of our approach through a perceptual study. Our readers note that the book soundtrack greatly enhanced their reading experience, due to high immersiveness granted via uninterrupted and style-consistent music, and a heightened emotional state attained via high precision emotion and scene context recognition.",Jaidev Shriram,research.jaidevshriram@gmail.com,"Jaidev Shriram (International Institute of Information Technology, Hyderabad)*; Makarand Tapaswi (International Institute of Information Technology, Hyderabad); Vinoo Alluri (International Institute of Information Technology, Hyderabad)","Shriram, Jaidev*; Tapaswi, Makarand; Alluri, Vinoo",,Applications -> music retrieval systems,"Applications -> music videos, multimodal music systems; Human-centered MIR -> human-computer interaction; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing; Musical features and properties -> musical affect, emotion and mood",No,"We present a novel technique to construct a soundtrack for books that have movie adaptations using techniques in MIR (music segmentation), NLP (emotion recognition, text segmentation), and Computer Vision (book-movie alignment).",Yes,Yes,Yes,000064.pdf,https://archives.ismir.net/ismir2022/paper/000064.pdf,https://drive.google.com/open?id=1YIXPR5kcOdosuEa_NY8O6_nNAfppR4be,https://drive.google.com/open?id=12NsPyqn-BWfzNDxOujGOSN9u5OmRqfw7,https://drive.google.com/open?id=1OkDCpMGhQPCTw6-NJkomhIWc4apI_Nd2,,https://slack.com/app_redirect?channel=C04CY0N3SAD,p5-01-shriram
74,3,5,1,"In-person, in Bengaluru",FALSE,Musika! Fast Infinite Waveform Music Generation,"Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.",Marco Pasini,marco.pasini.98@gmail.com,"Marco Pasini (Institute of Computational Perception, Johannes Kepler University Linz, Austria)*; Jan Schlüter (Institute of Computational Perception, Johannes Kepler University Linz, Austria)","Pasini, Marco*; Schlüter, Jan",marco.pasini.98@gmail.com*; jan.schlueter@jku.at,MIR tasks -> music generation,"Applications -> music composition; Domain knowledge -> machine learning/artificial intelligence for music; Human-centered MIR -> human-computer interaction; MIR tasks -> music synthesis and transformation; Musical features and properties -> rhythm, beat, tempo",No,"We propose a non-autoregressive, user controllable waveform music generation system capable of generating music of arbitrary length much faster than real-time on a consumer CPU.",Yes,Yes,No,000065.pdf,https://archives.ismir.net/ismir2022/paper/000065.pdf,https://drive.google.com/open?id=1QEAvbOv9-M-xqHS2is_LZEmCn-nMRucU,https://drive.google.com/open?id=1zfEG0D_LeA7eaEgXwyjNiyY0XaKGa2OP,https://drive.google.com/open?id=13qHfRkly7cxulZcRxBlxrlHy-s1VgkYE,https://docs.google.com/presentation/d/1FMcr8xN6-g-jw3inBSzbwXHsPOjHeuOwn0AFv6wHVU0/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CGCU6FQD,p5-02-pasini
49,3,5,2,Virtually,FALSE,Symphony Generation with Permutation Invariant Language Model,"In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation.",Maosong Sun,sms@tsinghua.edu.cn,"Jiafeng Liu (Department of Music AI and Music Information Technology, Central Conservatory of Music); Yuanliang Dong (Department of Music AI and Music Information Technology, Central Conservatory of Music); Zehua Cheng (Department of Computer Science, University of Oxford); Xinran Zhang (Department of Music AI and Music Information Technology, Central Conservatory of Music); XiaoBing Li (Department of Music AI and Music Information Technology, Central Conservatory of Music); Feng Yu (Department of Music AI and Music Information Technology, Central Conservatory of Music); Maosong Sun (Department of Music AI and Music Information Technology, Central Conservatory of Music, Department of Computer Science and Technology, Tsinghua University)*","Liu, Jiafeng; Dong, Yuanliang; Cheng, Zehua; Zhang, Xinran; Li, XiaoBing; Yu, Feng; Sun, Maosong*",jiafeng.liu@mail.ccom.edu.cn; gunterdong@mail.ccom.edu.cn; zehua.cheng@cs.ox.ac.uk; zhangxr.wspn@gmail.com; lxbmusic@188.com; zygjyyf@163.com; sms@tsinghua.edu.cn*,MIR tasks -> music generation,Applications -> music composition; Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> representations of music,No,"The pioneer solution for multi-track multi-instrument symbolic music generation with 3D representation, music BPE, and large symphony dataset provided.",Yes,Yes,No,000066.pdf,https://archives.ismir.net/ismir2022/paper/000066.pdf,https://drive.google.com/open?id=1VQXKiPKq3ta7o52v7CpIYK9ezoB2GdSR,https://drive.google.com/open?id=1RzyAAP0f_Sir7ugDaOc-kCYu2yOchsXk,https://drive.google.com/open?id=1sbutrPvF0UszD1DINTysrrG_riGItu2W,,https://slack.com/app_redirect?channel=C04CKB81QV8,p5-03-sun
150,3,5,3,"In-person, in Bengaluru",FALSE,MuLan: A Joint Embedding of Music Audio and Natural Language,"Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.",Qingqing Huang,qqhuang@google.com,"Qingqing Huang (Google Research)*; Aren Jansen (Google Research); Joonseok Lee (Google Research, Seoul National University); Ravi Ganti (Google Research); Judith Yue Li (Google Research); Dan P W Ellis (Google Research)","Huang, Qingqing*; Jansen, Aren; Lee, Joonseok; Ganti, Ravi; Li, Judith Yue; Ellis, Daniel P W",qqhuang@gmail.com*; arenjansen@google.com; joonseok2010@gmail.com; gmravi@google.com; judithyueli@google.com; dpwe@google.com,MIR fundamentals and methodology -> multimodality,"MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> web mining, and natural language processing; MIR tasks -> automatic classification; MIR tasks -> indexing and querying",No,"We train a large music-text joint embedding model with cross modal contrastive learning on large scale web mined data, and explore various applications enabled by the natural language interface.",No,No,No,000067.pdf,https://archives.ismir.net/ismir2022/paper/000067.pdf,https://drive.google.com/open?id=1qL-do2sSYGYWyqtivxrNRELnzLrF9MoB,https://drive.google.com/open?id=1k29DiyZ1qC3TGAVBbv9Gox1bVCdKt30Z,https://drive.google.com/open?id=1eaa1toshAC9rwK91a2oDJuG1O4Fv3jvC,https://drive.google.com/open?id=1NwaCiAUhT9qdCtVYy_5wkhEY6Gm8lXY5,https://slack.com/app_redirect?channel=C04CY0MQK0R,p5-04-huang
64,3,5,4,Virtually,FALSE,MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks,"Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc.",Peiling Lu,peil@microsoft.com,"Peiling Lu (Microsoft Research Asia, Beijing, China)*; Xu Tan (Microsoft Research Asia, Beijing, China); Botao Yu (Nanjing University, Nanjing, China); Tao Qin (Microsoft Research Asia, Beijing, China); Sheng Zhao (Microsoft Azure Speech, Beijing, China); Tie-Yan Liu (Microsoft Research Asia, Beijing, China)","Lu, Peiling*; Tan, Xu; Yu, Botao; Qin, Tao; Zhao, Sheng; Liu, Tie-Yan",peil@microsoft.com*; xuta@microsoft.com; btyu@smail.nju.edu.cn; taoqin@microsoft.com; szhao@microsoft.com; tyliu@microsoft.com,Domain knowledge -> machine learning/artificial intelligence for music,"Applications -> music composition; MIR tasks -> music generation; Musical features and properties -> structure, segmentation, and form",No,"We develop MeloForm, a system that generates melody with musical form using expert systems and neural networks without any labelled musical form data.",No,No,No,000068.pdf,https://archives.ismir.net/ismir2022/paper/000068.pdf,https://drive.google.com/open?id=1rBjo-iaak7h15gX1VfV7ikwjGX3EJW7Z,https://drive.google.com/open?id=1BmC8AZSGL18PUxACuqRa0KW0aA9o5WfI,https://drive.google.com/open?id=1TRDSd6ecnkeVasup3eVMMeGzI-uxXEND,,https://slack.com/app_redirect?channel=C04CGCU222Z,p5-05-lu
88,3,5,5,"In-person, in Bengaluru",FALSE,Towards robust music source separation on loud commercial music,"Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process.
 Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data.",Chang-Bin Jeon,vinyne@snu.ac.kr,"Chang-Bin Jeon (Department of Intelligence and Information, Music and Audio Research Group, Seoul National University)*; Kyogu Lee (Department of Intelligence and Information, Music and Audio Research Group, Seoul National University)","Jeon, Chang-Bin*; Lee, Kyogu",vinyne@snu.ac.kr*; kglee@snu.ac.kr,MIR tasks -> sound source separation,"Evaluation, datasets, and reproducibility -> MIR tasks; Evaluation, datasets, and reproducibility -> novel datasets and use cases",No,We proposed new datasets and methods for robust music source separation.,Yes,No,No,000069.pdf,https://archives.ismir.net/ismir2022/paper/000069.pdf,https://drive.google.com/open?id=1ilduQ9Hu7bEM3-LyhIPunNf_EsImHi15,https://drive.google.com/open?id=1BVzyruwuCtVOo0mBayApzFtvGt5oXBV7,https://drive.google.com/open?id=127RJsbbf9Mc2VWbGLoWvfewfI4X4KN8w,https://drive.google.com/open?id=1sbdTQN9tK9S44zkJI-gKiJ8zMRhhKW6K,https://slack.com/app_redirect?channel=C04CK86ASM9,p5-06-jeon
116,3,5,6,Virtually,FALSE,Towards Quantifying the Strength of Music Scenes Using Live Event Data,"There are many benefits for a community when there is a vibrant local music scene (e.g., increased mental & physical well-being, increased economic activity) and there are many factors that contribute to an environment in which a live music scene can thrive (e.g., available performance spaces, helpful government policies). In this paper, we explore using an estimate of the live music event rate (LMER) as a rough indicator to measure the strength of a local music scene. We define LMER as the number of music shows per 100,000 people per year and then explore how this indicator is (or is not) correlated with 28 other socioeconomic indicators. To do this, we analyze a set of 308,051 music events from 2019 across 1,139 cities in the United States. Our findings reveal that factors related to transportation (e.g., high walkability), population (high density), economics (high employment rate), age (high proportion of individuals age 20-29), and education (bachelor’s degree or higher) are strongly correlated with having a high number of live music events. Conversely, we did not find statistically significant evidence that other indica- tors (e.g., racial diversity) are correlated.",Michael Zhou,mgz2112@columbia.edu,Michael Zhou (Columbia University)*; Andrew McGraw (University of Richmond); Douglas R Turnbull (Ithaca College),"Zhou, Michael*; McGraw, Andrew; Turnbull, Douglas R",mgz2112@columbia.edu*; amcgraw@richmond.edu; dturnbull@ithaca.edu;,"Applications -> music and health, well-being and therapy","Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> web mining, and natural language processing; Philosophical and ethical discussions -> legal and societal aspects of MIR",No,We propose using the Live Music Event Rate (LMER) to measure the strength of a local music scene.,Yes,Yes,No,000070.pdf,https://archives.ismir.net/ismir2022/paper/000070.pdf,https://drive.google.com/open?id=1h0-RY1e8olgt50_HS-BNt25DWVFn059w,https://drive.google.com/open?id=1mqBWv0DB-be4qKV11vRujzJ27MNpU4vV,https://drive.google.com/open?id=1UHf0cUF8zZQzNB5_sVVBxS9E9E4EQMFb,,https://slack.com/app_redirect?channel=C04CGCUHYSH,p5-07-zhou
237,3,5,7,"In-person, in Bengaluru",FALSE,Learning Multi-Level Representations for Hierarchical Music Structure Analysis.,"Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, we explore unsupervised learning of such representations using a contrastive approach operating at different time-scales. We evaluate the proposed system on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.",Morgan Buisson,morgan.buisson76@gmail.com,"Morgan Buisson (LTCI, Télécom Paris, Institut Polytechnique de Paris, France)*; Brian McFee (Music and Audio Research Laboratory, New York University, USA, Center of Data Science, New York University, USA); Slim Essid (LTCI, Télécom Paris, Institut Polytechnique de Paris, France); Helene C Crayencour (L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France)","Buisson, Morgan*; McFee, Brian; Essid, Slim; Crayencour, Crayencour, Hélène C.",morgan.buisson76@gmail.com*; brian.mcfee@nyu.edu; slim.essid@telecom-paristech.fr; helene.camille.crayencour@gmail.com,"Musical features and properties -> structure, segmentation, and form",Domain knowledge -> machine learning/artificial intelligence for music; Musical features and properties -> representations of music,No,"In this work, we use a contrastive learning approach at different time scales to build representations that improve music segmentation at different levels of granularity.",Yes,Yes,No,000071.pdf,https://archives.ismir.net/ismir2022/paper/000071.pdf,https://drive.google.com/open?id=1XLUMzf3uoejFSWn5jGnSPpzQ3cHKM5Jm,https://drive.google.com/open?id=1bAMp-dLazBX41bIuF7RGiZgAR6QSoRku,https://drive.google.com/open?id=1RD0vj9G3DxjdrBAkN_NotQCczqfKn7zA,https://docs.google.com/presentation/d/1j6KgEpjKJOZo83CY6JioDDtizJ2luFsJKE8YinVCR2o/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CK87J411,p5-08-buisson
246,3,5,8,Virtually,FALSE,Multi-instrument Music Synthesis with Spectrogram Diffusion,"An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fréchet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.",Curtis Hawthorne,fjord@google.com,"Curtis Hawthorne (Google Research, Brain Team)*; Ian Simon (Google Research, Brain Team); Adam Roberts (Google Research, Brain Team); Neil Zeghidour (Google Research, Brain Team); Joshua Gardner (University of Washington); Ethan Manilow (Interactive Audio Lab, Northwestern University); Jesse Engel (Google Research, Brain Team)","Hawthorne, Curtis*; Simon, Ian; Roberts, Adam; Zeghidour, Neil; Gardner, Joshua; Manilow, Ethan; Engel, Jesse",fjord@google.com*; iansimon@google.com; adarob@gmail.com; neilz@google.com; jpgard@cs.washington.edu; ethanm@u.northwestern.edu; jesseengel@google.com,MIR tasks -> music synthesis and transformation,MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing,No,"Using an encoder-decoder Transformer, with the decoder trained as a Denoising Diffusion Probabilistic Model (DDPM), enables generating audio from MIDI sequences with arbitrary combinations of instruments.",No,Yes,No,000072.pdf,https://archives.ismir.net/ismir2022/paper/000072.pdf,https://drive.google.com/open?id=1L0KNm15GdRf-LlBskmQap6iSrS167g0W,https://drive.google.com/open?id=1OSgBz7vOidgTnyugE8Up19l-hOKBJ12w,https://drive.google.com/open?id=18uC0iLawsMhkR4nGyZXCTI6NiFD_LD6c,,https://slack.com/app_redirect?channel=C04CKB9RCUA,p5-09-hawthorne
248,3,5,9,"In-person, in Bengaluru",FALSE,DDX7: Differentiable FM Synthesis of Musical Instrument Sounds,"FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source.
 On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.",Franco Caspe,f.s.caspe@qmul.ac.uk,Franco Caspe (Queen Mary University of London)*; Andrew McPherson (Queen Mary University of London); Mark Sandler (Queen Mary University of London),"Caspe, Franco*; McPherson, Andrew; Sandler, Mark",f.s.caspe@qmul.ac.uk*; a.mcpherson@qmul.ac.uk; mark.sandler@qmul.ac.uk,MIR tasks -> music synthesis and transformation,"Applications -> performance, and production; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice",No,We present a neural network architecture that learns to control an FM synthesizer to effectively reproduce sounds of selected musical instruments from a compact set of parameters.,Yes,Yes,No,000073.pdf,https://archives.ismir.net/ismir2022/paper/000073.pdf,https://drive.google.com/open?id=1tcuAoQbn79II6omDp02niDcXYi1HGVZv,https://drive.google.com/open?id=1CQMt2dW3cGfG7CLbRxTzYkRbLvKG4Rox,https://drive.google.com/open?id=1ttR-uhp_6fldGXAUTlH71VQHFSOlsBhr,https://docs.google.com/presentation/d/1_0wlq4619MN43JPRtCwZ-TdblxJOFTaKVTtTKtvh2Tg/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CCP3LHNJ,p5-10-caspe
250,3,5,10,Virtually,FALSE,Singing beat tracking with Self-supervised front-end and linear transformers,"Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction.
 Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. 
 Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies 
 also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.",Mojtaba Heydari,mheydari@ur.rochester.edu,"Mojtaba Heydari (Department of Electrical and Computer Engineering, University of Rochester, 500 Wilson Blvd, Rochester, NY 14627, USA)*; Zhiyao Duan (Department of Electrical and Computer Engineering, University of Rochester, 500 Wilson Blvd, Rochester, NY 14627, USA)","Heydari, Mojtaba*; Duan, Zhiyao",mheydari@ur.rochester.edu*; zhiyao.duan@rochester.edu,"Musical features and properties -> rhythm, beat, tempo","Musical features and properties -> timbre, instrumentation, and singing voice",No,We introduced singing vocal beat tracking as a new task and proposed 3 models to address it.,Yes,Yes,No,000074.pdf,https://archives.ismir.net/ismir2022/paper/000074.pdf,https://drive.google.com/open?id=1iIMqgx_EjeYlM0uaaSyzJSikF4m5LIfY,https://drive.google.com/open?id=1-XUh6JmUAZckJDnV7rV3_5e-E4_CLOLe,https://drive.google.com/open?id=1ru9Ksqob5mz_y2I8D37yW1mA1iSHuUBI,,https://slack.com/app_redirect?channel=C04CCP3M8TY,p5-11-heydari
277,3,5,11,"In-person, in Bengaluru",FALSE,EnsembleSet: a new high quality synthesised dataset for chamber ensemble separation,"Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.",Saurjya Sarkar,saurjya.sarkar@qmul.ac.uk,"Saurjya Sarkar (Centre for Digital Music, Queen Mary University of London, UK)*; Emmanouil Benetos (Centre for Digital Music, Queen Mary University of London, UK); Mark Sandler (Centre for Digital Music, Queen Mary University of London, UK)","Sarkar, Saurjya*; Benetos, Emmanouil; Sandler, Mark",saurjya.sarkar@qmul.ac.uk*; emmanouil.benetos@qmul.ac.uk; mark.sandler@qmul.ac.uk,MIR tasks -> sound source separation,"Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> novel datasets and use cases",No,We introduce a new synthesised chamber ensemble dataset from a highly realistic sample library and show that a model trained exclusively on the synthesised dataset is able to generalise to real world data (from URMP) and outperform SOTA for separating 2 monotimbral instruments from a mixture.,Yes,Yes,No,000075.pdf,https://archives.ismir.net/ismir2022/paper/000075.pdf,https://drive.google.com/open?id=1SVuTvv-13lWyPxHnICvtH-kVhZNfmGqa,https://drive.google.com/open?id=1jFALUuzfK3k0S-L5EdAnlNqc5IOM81xW,https://drive.google.com/open?id=1WpyTZ1dZG5NpVO46cE-1qL4Cjx7bXrG6,https://drive.google.com/open?id=1KsW8Vf0X4pBGvDmBGQlVGGmj7nnIEdKv,https://slack.com/app_redirect?channel=C04CMQW8TDJ,p5-12-sarkar
6,3,5,12,Virtually,FALSE,End-to-End Lyrics Transcription Informed by Pitch and Onset Estimation,"This paper presents an automatic lyrics transcription (ALT) method for music recordings that leverages the framewise semitone-level sung pitches estimated in a multi-task learning framework. Compared to automatic speech recognition (ASR), ALT is challenging due to the insufficiency of training data and the variation and contamination of acoustic features caused by singing expressions and accompaniment sounds. The domain adaptation approach has thus recently been taken for updating an ASR model pre-trained from sufficient speech data. In the naive application of the end-to-end approach to ALT, the internal audio-to-lyrics alignment often fails due to the time-stretching nature of singing features. To stabilize the alignment, we make use of the semi-synchronous relationships between notes and characters. Specifically, a convolutional recurrent neural network (CRNN) is used for estimating the semitone-level pitches with note onset times while eliminating the intra- and inter-note pitch variations. This estimate helps an end-to-end ALT model based on connectionist temporal classification (CTC) learn correct audio-to-character alignment and mapping, where the ALT model is trained jointly with the pitch and onset estimation model. The experimental results show the usefulness of the pitch and onset information in ALT.",Tengyu Deng,deng@sap.ist.i.kyoto-u.ac.jp,"Tengyu Deng (Graduate School of Informatics, Kyoto University, Japan)*; Eita Nakamura (Graduate School of Informatics, Kyoto University, Japan); Kazuyoshi Yoshii (Graduate School of Informatics, Kyoto University, Japan, PRESTO, Japan Science and Technology Agency, Japan)","Deng, Tengyu*; Nakamura, Eita; Yoshii, Kazuyoshi",deng@sap.ist.i.kyoto-u.ac.jp*; eita.nakamura@i.kyoto-u.ac.jp; yoshii@i.kyoto-u.ac.jp,MIR fundamentals and methodology -> lyrics and other textual data,MIR fundamentals and methodology -> music signal processing; MIR tasks -> music transcription and annotation,No,We propose an end-to-end lyrics transcription method that makes effective use of the semitone- and frame-level pitch sequence of the singing voice estimated in a multi-task learning framework.,Yes,No,No,000076.pdf,https://archives.ismir.net/ismir2022/paper/000076.pdf,https://drive.google.com/open?id=1CyuskJl7dZ2YWd0M7lAu-Ti0X3D0ahhY,https://drive.google.com/open?id=1jZ0ilSCXRSQcTWl_GQodIO2AeBHlBn2a,https://drive.google.com/open?id=1Canay8HQQGNSbK0hh6SXv1Xey8EZcph_,,https://slack.com/app_redirect?channel=C04CGCTHZJ9,p5-13-deng
275,3,5,13,"In-person, in Bengaluru",FALSE,Contrastive Audio-Language Learning for Music,"As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. 
 Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets.",Ilaria Manco,i.manco@qmul.ac.uk,"Ilaria Manco (School of EECS, Queen Mary University of London, London, U.K)*; Emmanouil Benetos (School of EECS, Queen Mary University of London, London, U.K); Elio Quinton (Music & Audio Machine Learning Lab, Universal Music Group, London, U.K.); György Fazekas (School of EECS, Queen Mary University of London, London, U.K)","Manco, Ilaria*; Benetos, Emmanouil; Quinton, Elio; Fazekas, George",i.manco@qmul.ac.uk*; emmanouil.benetos@qmul.ac.uk; elio.quinton@umusic.com; george.fazekas@qmul.ac.uk,Domain knowledge -> machine learning/artificial intelligence for music,"Applications -> music retrieval systems; Domain knowledge -> representations of music; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",No,We propose a multimodal contrastive learning method to align audio and language data for cross-modal retrieval in the music domain.,Yes,Yes,No,000077.pdf,https://archives.ismir.net/ismir2022/paper/000077.pdf,https://drive.google.com/open?id=1yGT7U1odyQAF9QjWG-MqcgRRbI713kCm,https://drive.google.com/open?id=1NV_Gdv7CIeEvzEJvrP3PLTfd4xArXf2R,https://drive.google.com/open?id=1hyhmeByyVCMBgKy6wY7zTVSTiCwVVDL2,https://drive.google.com/open?id=1STDzjEbMyPr2AhItfNmHWcbIXG5W4DSe,https://slack.com/app_redirect?channel=C04CCP3Q1GE,p5-14-manco
286,3,5,14,"In-person, in Bengaluru",FALSE,MusAV: A dataset of relative arousal-valence annotations for validation of audio models,"We present MusAV, a new public benchmark dataset for comparative validation of arousal and valence (AV) regression models for audio-based music emotion recognition. To gather the ground truth, we rely on relative judgments instead of absolute values to simplify the manual annotation process and improve its consistency. We build MusAV by gathering comparative annotations of arousal and valence on pairs of tracks, using track audio previews and metadata from the Spotify API. The resulting dataset contains 2,092 track previews covering 1,404 genres, with pairwise relative AV judgments by 20 annotators and various subsets of the ground truth based on different levels of annotation agreement. We demonstrate the use of the dataset in an example study evaluating nine models for AV regression that we train based on state-of-the-art audio embeddings and three existing datasets of absolute AV annotations. The results on MusAV offer a view of the performance of the models complementary to the metrics obtained during training and provide insights into the impact of the considered datasets and embeddings on the generalization abilities of the models.",Dmitry Bogdanov,dmitry.bogdanov@upf.edu,"Dmitry Bogdanov (Music Technology Group, Universitat Pompeu Fabra, Spain)*; Xavier Lizarraga-Seijas (Music Technology Group, Universitat Pompeu Fabra, Spain); Pablo Alonso-Jiménez (Music Technology Group, Universitat Pompeu Fabra, Spain); Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Spain)","Bogdanov, Dmitry*; Lizarraga-Seijas, Xavier; Alonso-Jiménez, Pablo; Serra, Xavier",dmitry.bogdanov@upf.edu*; xavier.lizarraga@upf.edu; pablo.alonso@upf.edu; xavier.serra@upf.edu,"Musical features and properties -> musical affect, emotion and mood","Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> automatic classification; Musical features and properties -> representations of music",No,We present a new dataset for comparative validation of arousal and valence regression models for audio-based music emotion recognition and evaluate models based on audio embeddings.,No,Yes,No,000078.pdf,https://archives.ismir.net/ismir2022/paper/000078.pdf,https://drive.google.com/open?id=1B_XMXJNMPKKEyJ6_7q35A6v44YCw4GLj,https://drive.google.com/open?id=11Hx1IAV5oxTVwrBnjsZotvbSRnBQDnd7,https://drive.google.com/open?id=1UCUzM_P69M0JjoVGW7XO6-5D6Fk7En7r,,https://slack.com/app_redirect?channel=C04C4QR62KZ,p5-15-bogdanov
136,3,5,15,Virtually,FALSE,What is missing in deep music generation? A study of repetition and structure in popular music,"Structure is one of the most essential aspects of music, and music structure is commonly indicated through repetition. However, the nature of repetition and structure in music is still not well understood, especially in the context of music generation, and much remains to be explored with Music Information Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and American) illustrate important music construction principles: (1) structure exists at multiple hierarchical levels, (2) songs use repetition and limited vocabulary so that individual songs do not follow general statistics of song collections, (3) structure interacts with rhythm, melody, harmony, and predictability, and (4) over the course of a song, repetition is not random, but follows a general trend as revealed by cross-entropy. These and other findings offer challenges as well as opportunities for deep-learning music generation and suggest new formal music criteria and evaluation methods. Music from recent music generation systems is analyzed and compared to human-composed music in our datasets, often revealing striking differences from a structural perspective.",Shuqi Dai,shuqid@cs.cmu.edu,Shuqi Dai (Carnegie Mellon University)*; Huiran Yu (Carnegie Mellon University); Roger B. Dannenberg (Carnegie Mellon University),"Dai, Shuqi*; Yu, Huiran; Dannenberg, Roger B",shuqid@cs.cmu.edu*; huiranyu@cs.cmu.edu; rbd@cs.cmu.edu,Domain knowledge -> computational music theory and musicology,"Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> evaluation methodology; MIR tasks -> music generation; Musical features and properties; Musical features and properties -> structure, segmentation, and form",No,"Analysis of repetition and structure reveals differences between real songs and those of many music generation systems, suggesting that there are important gaps to fill through new research.",Yes,Yes,No,000079.pdf,https://archives.ismir.net/ismir2022/paper/000079.pdf,https://drive.google.com/open?id=1hgpQn0Gn3lO1GYdqMVFAPGLkZizMMTs5,https://drive.google.com/open?id=1EJMLcpDBOwBrKtONt54rNgtkUNN_M7af,https://drive.google.com/open?id=1YEa_9gth2NYF3RTiSe-oW98Btgq1n9M2,,https://slack.com/app_redirect?channel=C04C4QPTMUP,p5-16-dai
282,3,5,16,Virtually,FALSE,Heterogeneous Graph Neural Network for Music Emotion Recognition,"Music emotion recognition has been a growing field of research motivated by the wealth of information that these labels express. Recognition of emotions highlights music's social and psychological functions, extending traditional applications such as style recognition or content similarity. Once musical data are intrinsically multi-modal, exploring this characteristic is usually beneficial. However, building a structure that incorporates different modalities in a unique space to represent the songs is challenging. Integrating information from related instances by learning heterogeneous graph-based representations has achieved state-of-the-art results in multiple tasks. This paper proposes structuring musical features over a heterogeneous network and learning a multi-modal representation using Graph Convolutional Networks with features extracted from audio and lyrics as inputs to handle the music emotion recognition tasks. We show that the proposed learning approach resulted in a representation with greater power to discriminate emotion labels. Moreover, our heterogeneous graph neural network classifier outperforms related works for music emotion recognition.",Angelo Cesar Mendes da Silva,angelo.mendes@usp.br,"Angelo Cesar Mendes da Silva (Universidade de São Paulo, Brazil)*; Diego Furtado Silva (Universidade Federal de São Carlos, Brazil); Ricardo Marcondes Marcacini (Universidade de São Paulo, Brazil)","Mendes da Silva, Angelo Cesar*; Silva, Diego F; Marcacini, Ricardo Marcondes",angelo.mendes@usp.br*; diegofs@ufscar.br; ricardo.marcacini@usp.br,Musical features and properties -> representations of music,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR tasks -> automatic classification; Musical features and properties -> musical affect, emotion and mood",No,We create a structure for music data using heterogeneous networks and build a new multi-modal graph-based music representation that incorporates audio and lyrics features to handle the music emotion recognition tasks.,Yes,Yes,No,000080.pdf,https://archives.ismir.net/ismir2022/paper/000080.pdf,https://drive.google.com/open?id=12eZbcXcwCJ2X3hQ0LSgTfsgTg0jxlAHA,https://drive.google.com/open?id=15CVr0PQ1kJZqpx4QYunuQ_txrgBKThxD,https://drive.google.com/open?id=1ZiECb_DmW_yo3dcsgp34xJKSEVWsxWay,,https://slack.com/app_redirect?channel=C04CKBA3ZK4,p5-17-silva
221,3,6,0,Virtually,TRUE,"And what if two musical versions don't share melody, harmony, rhythm, or lyrics ?","Version identification (VI) has seen substantial progress over the past few years. On the one hand, the introduction of the metric learning paradigm has favored the emergence of scalable yet accurate VI systems. On the other hand, using features focusing on specific aspects of musical pieces, such as melody, harmony, or lyrics, yielded interpretable and promising performances. In this work, we build upon these recent advances and propose a metric learning-based system systematically leveraging four dimensions commonly admitted to convey musical similarity between versions: melodic line, harmonic structure, rhythmic patterns, and lyrics. We describe our deliberately simple model architecture, and we show in particular that an approximated representation of the lyrics is an efficient proxy to discriminate between versions and non-versions. We then describe how these features complement each other and yield new state-of-the-art performances on two publicly available datasets. We finally suggest that a VI system using a combination of melodic, harmonic, rhythmic and lyrics features could theoretically reach the optimal performances obtainable on these datasets.",Mathilde Abrassart,abrassart@ircam.fr,"Mathilde Abrassart (Ircam Amplify)*; Guillaume Doras (Ircam, Sorbonne Université, CNRS, STMS Lab)","Abrassart, Mathilde*; Doras, Guillaume",abrassart@ircam.fr*; doras@ircam.fr,Applications -> music retrieval systems,"MIR tasks -> fingerprinting; MIR tasks -> similarity metrics; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> melody and motives; Musical features and properties -> rhythm, beat, tempo",No,"A novel metric learning system leveraging melodic line, harmonic structure, rhythm patterns, and lyrics yields new start-of-the-art performances on two publicly available datasets.",Yes,Yes,Yes,000081.pdf,https://archives.ismir.net/ismir2022/paper/000081.pdf,https://drive.google.com/open?id=19Kw2x2o6CKQbzixgXQ-uJczXDDKYMgls,https://drive.google.com/open?id=1pxf_e-xXTWLnKSAi2drT_RyDzgVg49In,https://drive.google.com/open?id=1UkcTwl0KdkQzXc7PBnQ64CubKwDzlsof,,https://slack.com/app_redirect?channel=C04CKB9HE6N,p6-01-abrassart
262,3,6,1,"In-person, in Bengaluru",FALSE,A diffusion-inspired training strategy for singing voice extraction in the waveform domain,"Notable progress in music source separation has been achieved using multi-branch networks that operate on both temporal and spectral domains. However, such networks tend to be complex and heavy-weighted. In this work, we tackle the task of singing voice extraction from polyphonic music signals in an end-to-end manner using an approach inspired by the training procedure of denoising diffusion models. We perform unconditional signal modelling to gradually convert an input mixture signal to the corresponding singing voice or accompaniment. We use fewer parameters than the state-of-the-art models while operating on the waveform domain, bypassing phase-related problems. More concisely, we train a non-causal WaveNet using a diffusion-inspired strategy improving the said network for singing voice extraction and obtaining performance comparable to the end-to-end state-of-the-art on MUSDB18. We further report results on a non-MUSDB-overlapping version of MedleyDB and the multi-track audio of the Saraga Carnatic dataset showing good generalization, and run perceptual tests of our approach. Code, models, and audio examples are made available.",Genís Plaja-Roglans,GEN√çS PLAJA,"Genís Plaja-Roglans (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)*; Marius Miron (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain); Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)","Plaja-Roglans, Genís*; Miron, Marius; Serra, Xavier",genis.plaja@upf.edu*; miron.marius@upf.edu; xavier.serra@upf.edu,MIR tasks -> sound source separation,Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music synthesis and transformation,No,The denoising diffusion process can be adapted to perform end-to-end singing voice extraction and obtain comparable results to the state-of-the-art.,Yes,No,No,000082.pdf,https://archives.ismir.net/ismir2022/paper/000082.pdf,https://drive.google.com/open?id=1w9UVHiHcwKdzZ8TT-5Zmac-_89Ss5LJA,https://drive.google.com/open?id=11ai_HSMAujukTAmFJtxvGAGOJlrveXw3,https://drive.google.com/open?id=1vSjJiUd4n7ih5a6HI03JQuXENYjppeVK,https://docs.google.com/presentation/d/1PmQ3k2m8OQJ45HM5BFQxAK5--CXVnKLcVD1D5e8mhPM/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CK87QV5Z,p6-02-plaja-roglans
23,3,6,2,Virtually,FALSE,A Model You Can Hear: Audio Identification with Playable Prototypes,"Machine learning techniques have proved useful for classifying and analyzing audio content. However, recent methods typically rely on abstract and high-dimensional representations that are difficult to interpret. Inspired by transformation-invariant approaches developed for image and 3D data, we propose an audio identification model based on learnable spectral prototypes. Equipped with dedicated transformation networks, these prototypes can be used to cluster and classify input audio samples from large collections of sounds. Our model can be trained with or without supervision and reaches state-of-the-art results for speaker and instrument identification, while remaining easily interpretable. The code is available at: https://github.com/romainloiseau/a-model-you-can-hear",Romain Loiseau,romain.loiseau@enpc.fr,"Romain Loiseau (LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France, LASTIG, Univ Gustave Eiffel, IGN, ENSG)*; Baptiste Bouvier (STMS Lab, UMR 9912 IRCAM, CNRS, Sorbonne University, Paris, France); Yann Teytaut (STMS Lab, UMR 9912 IRCAM, CNRS, Sorbonne University, Paris, France); Elliot Vincent (LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France, INRIA and DIENS ENS-PSL, CNRS, INRIA); Mathieu Aubry (LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France); Loic Landrieu (LASTIG, Univ Gustave Eiffel, IGN, ENSG)","Loiseau, Romain*; Bouvier, Baptiste; Teytaut, Yann; Vincent, Elliot; Aubry, Mathieu; Landrieu, Loic",romain.loiseau@enpc.fr*; baptiste.bouvier@ircam.fr; yann.teytaut@ircam.fr; elliot_vincent@orange.fr; mathieu.aubry@enpc.fr; loic.landrieu@ign.fr,Domain knowledge -> machine learning/artificial intelligence for music,Domain knowledge -> representations of music; MIR tasks -> automatic classification; MIR tasks -> fingerprinting,No,"In this paper, we propose adapting the transformation-invariant clustering paradigm to the audio domain in both supervised and unsupervised settings, resulting in an audio identification model, based on prototypical sounds that can be heard directly, that produces state-of-the-art results while remaining easily interpretable.",Yes,No,No,000083.pdf,https://archives.ismir.net/ismir2022/paper/000083.pdf,https://drive.google.com/open?id=1BLk3gCxIA5v3gg7R85oFgR0VjQiORYsi,https://drive.google.com/open?id=1-__8LS0fV_lMuqKd-itjhjHuZDWhXv5Z,https://drive.google.com/open?id=1vuqO4CRkbaMJ7ns8dU86I1QjXzRkobpV,,https://slack.com/app_redirect?channel=C04D92FNZFS,p6-03-loiseau
155,3,6,3,Virtually,FALSE,An Exploration of Generating Sheet Music Images,"Many previous works in recent years have explored various forms of music generation. These works have focused on generating either raw audio waveforms or symbolic music. In this work, we explore the feasibility of generating sheet music images, which is often the primary form in which musical compositions are notated for other musicians. Using the PrIMuS dataset as a testbed, we explore five different sequence-based approaches for generating lines of sheet music: generating sequences of (a) pixel columns, (b) image patches, (c) visual word tokens, (d) semantic tokens, and (e) XML-based tags. We show sample generated images, discuss the practical challenges and problems with each approach, and give our recommendation on the most promising paths to explore in the future.",Timothy Tsai,ttsai@g.hmc.edu,Marcos Acosta (Harvey Mudd College); Irmak Bukey (Pomona College); TJ Tsai (Harvey Mudd College)*,"Acosta, Marcos; Bukey, Irmak; Tsai, T J*",mdacosta@g.hmc.edu; ibab2018@mymail.pomona.edu; ttsai@g.hmc.edu*,MIR tasks -> music generation,Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; Musical features and properties -> representations of music,No,We explore five different sequence-based approaches for generating raw sheet music images.,Yes,No,No,000084.pdf,https://archives.ismir.net/ismir2022/paper/000084.pdf,https://drive.google.com/open?id=1_Xb4reQEJYgEqL23vcAGHihgt-BtF8dg,https://drive.google.com/open?id=1w-yGPZwZWKnE9cfP8Xr7tyJFonVG3IBT,https://drive.google.com/open?id=1_IjN18zJOHnLkpkqAAN0tmBa-43Djhk5,https://drive.google.com/open?id=1geEESUvbodOGBtB5R5mmYY5upbkTEkDc,https://slack.com/app_redirect?channel=C04CGCUVCF7,p6-04-tsai
32,3,6,4,Virtually,FALSE,HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription,"While neural network models are making significant progress in piano transcription, they are becoming more resource-consuming due to requiring larger model size and more computing power. In this paper, we attempt to apply more prior about piano to reduce model size and improve the transcription performance. The sound of a piano note contains various overtones, and the pitch of a key does not change over time. To make full use of such latent information, we propose HPPNet that using the Harmonic Dilated Convolution to capture the harmonic structures and the Frequency Grouped Recurrent Neural Network to model the pitch-invariance over time. Experimental results on the MAESTRO dataset show that our piano transcription system achieves state-of-the-art performance both in frame and note scores (frame F1 93.15%, note F1 97.18%). Moreover, the model size is much smaller than the previous state-of-the-art deep learning models.",Weixing Wei,wxwei20@fudan.edu.cn,"Weixing Wei (School of Computer Science and Technology, Fudan University, China)*; Peilin Li (School of Computer Science and Technology, Fudan University, China); Yi Yu (Digital Content and Media Sciences Research Division, National Institute of Informatics, Japan); Wei Li (School of Computer Science and Technology, Fudan University, China, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, China)","Wei, Weixing*; Li, Peilin; Yu, Yi; Li, Wei",wxwei20@fudan.edu.cn*; plli21@m.fudan.edu.cn; yiyu@nii.ac.jp; weili-fudan@fudan.edu.cn,MIR tasks -> music transcription and annotation,MIR fundamentals and methodology -> music signal processing,No,A lightweight piano transcription model with state-of-the-art performance.,Yes,Yes,No,000085.pdf,https://archives.ismir.net/ismir2022/paper/000085.pdf,https://drive.google.com/open?id=1I90r4HFTX8UAOfV2yAc_09WyudcfwMSr,https://drive.google.com/open?id=1Eq2_j_EX6vD1OWsRKF0gN7MsM0HTS_2h,https://drive.google.com/open?id=1eYUDlPexj0t4QIfC-DOAR-dCSQmK8_Yi,,https://slack.com/app_redirect?channel=C04CCP1KUMC,p6-05-wei
45,3,6,5,"In-person, in Bengaluru",FALSE,Generating music with sentiment using Transformer-GANs,"The field of Automatic Music Generation has seen significant progress thanks to the advent of Deep Learning. However, most of these results have been produced by unconditional models, which lack the ability to interact with their users, not allowing them to guide the generative process in meaningful and practical ways. Moreover, synthesizing music that remains coherent across longer timescales while still capturing the local aspects that make it sound ``realistic'' or human-like is still challenging. This is due to the large computational requirements needed to work with long sequences of data, and also to limitations imposed by the training schemes that are often employed. In this paper, we propose a
 generative model of symbolic music conditioned by data retrieved from human sentiment. The model is a Transformer-GAN trained with labels that correspond to different configurations of the valence and arousal dimensions that quantitatively represent human affective states. We try to tackle both of the problems above by employing an efficient linear version of Attention and using a Discriminator both as a tool to improve the overall quality of the generated music and its ability to follow the conditioning signals.",Pedro LT Neves,p185770@dac.unicamp.br,Pedro L T Neves (State University of Campinas)*; José E F Novo Junior (State University of Campinas); João B Florindo (State University of Campinas),"Neves, Pedro L T*; Fornari, José; Florindo, João B",p185770@dac.unicamp.br*; fornari@unicamp.br; florindo@unicamp.br,MIR tasks -> music generation,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> musical affect, emotion and mood",No,We use a Generative Adversarial Network consisting of Transformer models to create symbolic music conditioned by sentiment.,Yes,Yes,No,000086.pdf,https://archives.ismir.net/ismir2022/paper/000086.pdf,https://drive.google.com/open?id=1vuuOH693beSdcybc3gwGs1T3vkQKzc2S,https://drive.google.com/open?id=1MWbEl2v-OICw0a3JI60Tv_o1qwus2wTs,https://drive.google.com/open?id=1PcUIEJovwkmsHcxwbgoYYu0ap8cY81tG,https://docs.google.com/presentation/d/1SUFGfG8X22Pk8Lc9QAF1RcfnW1LfyfyPagE30DYRh0g/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CY0LP7GR,p6-06-neves
78,3,6,6,Virtually,FALSE,Improving Choral Music Separation through Expressive Synthesized Data from Sampled Instruments,"Choral music separation refers to the task of extracting tracks of voice parts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of datasets has impeded research on this topic as previous work has only been able to train and evaluate models on a few minutes of choral music data due to copyright issues and dataset collection difficulties. In this paper, we investigate the use of synthesized training data for the source separation task on real choral music. We make three contributions: first, we provide an automated pipeline for synthesizing choral music data from sampled instrument plugins within controllable options for instrument expressiveness. This produces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset and one can easily synthesize additional data. Second, we conduct an experiment to evaluate multiple separation models on available choral music separation datasets from previous work. To the best of our knowledge, this is the first experiment to comprehensively evaluate choral music separation. Third, experiments demonstrate that the synthesized choral data is of sufficient quality to improve the model's performance on real choral music datasets. This provides additional experimental statistics and data support for the choral music separation study.",Ke Chen,knutchen@ucsd.edu,"Ke Chen (UC San Diego, USA)*; Hao-Wen Dong (UC San Diego, USA); Yi Luo (Tencent AI Lab, China); Julian McAuley (UC San Diego, USA); Taylor Berg-Kirkpatrick (UC San Diego, USA); Miller Puckette (UC San Diego, USA); Shlomo Dubnov (UC San Diego, USA)","Chen, Ke*; Dong, Hao-Wen; Luo, Yi; McAuley, Julian; Berg-Kirkpatrick, Taylor; Puckette, Miller; Dubnov, Shlomo",knutchen@ucsd.edu*; hwdong@ucsd.edu; yl3364@columbia.edu; jmcauley@ucsd.edu; tberg@ucsd.edu; msp@ucsd.edu; sdubnov@ucsd.edu,MIR tasks,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> music signal processing; MIR tasks -> sound source separation",No,"We present an automated pipeline to synthesize choral music data, with a comprehensive experiment to improve the choral music separation performance on real-world choral music datasets.",Yes,No,No,000087.pdf,https://archives.ismir.net/ismir2022/paper/000087.pdf,https://drive.google.com/open?id=1ciUlGPKpwnnfuHqjVQTW5wd60Lu0KvaS,https://drive.google.com/open?id=178fDe_sW05aT3I_3BEMm1eQNvY1w14v9,https://drive.google.com/open?id=1AIWjC2LXb3ssw-uQjiMRgwuIUBgw1uuX,,https://slack.com/app_redirect?channel=C04CGCU7561,p6-07-chen
131,3,6,7,Virtually,FALSE,Ethics of Singing Voice Synthesis: Perceptions of Users and Developers,"Singing Voice Synthesis (SVS) has recently garnered much attention as its quality has improved vastly with the use of artificial intelligence (AI), creating many opportunities for supporting music creators and listeners. Recently, there have been growing concerns about ethical issues related to AI development in general, and to AI-based SVS development specifically. Many questions remain unexplored about how to ethically develop and use such technology. In this paper, we investigate the perception of ethical issues related to SVS from the perspectives of two different groups: the general public and developers. We collected 3,075 user comments from YouTube videos showcasing various uses of SVS as part of a mainstream variety show. Additionally, we interviewed six researchers developing SVS technology. Through thematic analysis, we identify and discuss three different aspects related to ethical issues in SVS development, highlighting the similarities and differences between the perspectives of the general public and developers: (1) Use scenarios, (2) Attitudes towards development, and (3) Meaning of ""Creativity"", and (4) Concerns about human rights, intellectual property (IP) and legal issues.",Jin Ha Lee,jinhalee@uw.edu,"Kyungyun Lee (Gaudio Lab, Seoul, Korea); Gladys Hitt (University of Washington, Seattle, USA); Emily Terada (University of Washington, Seattle, USA); Jin Ha Lee (University of Washington, Seattle, USA)*","Lee, Kyungyun; Hitt, Gladys; Terada, Emily; Lee, Jin Ha*",mo@gaudiolab.com; hittg@uw.edu; eterada@uw.edu; jinhalee@uw.edu*,Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR to,Human-centered MIR -> human-computer interaction,No,We investigate and compare the perceptions of users and developers on ethical issues related to singing voice synthesis through thematic analysis of online user comments and developer interviews.,Yes,Yes,No,000088.pdf,https://archives.ismir.net/ismir2022/paper/000088.pdf,https://drive.google.com/open?id=1_uqARNN5oTzBJCH11H30LAkpH2jwBR4G,https://drive.google.com/open?id=1HFb7a5n-xNRCbaSpia4km_fgw-oCZx37,https://drive.google.com/open?id=1nUxZUs3vxTbxh7ZB4VjAXBQYcAlt7U5Z,,https://slack.com/app_redirect?channel=C04CKB8QSP4,p6-08-lee
80,3,6,8,Virtually,FALSE,Emotion-driven Harmonisation And Tempo Arrangement of Melodies Using Transfer Learning,"We propose and assess deep learning models for harmonic and tempo arrangement generation given melodies and emotional constraints. A dataset of 4000 symbolic scores and emotion labels was gathered by expanding the HTPD3 dataset with mood tags from last.fm and allmusic.com. We explore how bi-directional LSTM and Transformer encoder architectures can learn relationships between symbolic melodies, chord progressions, tempo, and expressed emotions, with and without a transfer learning strategy leveraging symbolic music data without emotion labels. Three emotion annotation summarisation methods based on the Arousal/Valence (AV) representation are compared: Emotion Average, Emotion Surface, and Emotion Category. 20 participants (average age: 30.2, 7 females and 13 males from Japan) rated how well generated accompaniments matched melodies (musical coherence) as well as perceived emotions for 75 arrangements corresponding to combinations of models and emotion summarisation methods. Musical coherence and match between target and perceived emotions were highest when melodies were encoded using a BLSTM model with transfer learning. The proposed method generates emotion-driven harmonic/tempo arrangements in a fast way, a keen advantage compared to state of the art. Applications of this work include AI-based composition assistant and live interactive music systems for entertainment such as video games.",Takuya Takahashi,takahashi@uec.ac.jp,"Takuya Takahashi (Centre for Digital Music, Queen Mary University of London)*; Mathieu Barthet (Centre for Digital Music, Queen Mary University of London)","Takahashi, Takuya*; Barthet, Mathieu",takahashi@uec.ac.jp*; m.barthet@qmul.ac.uk,MIR tasks -> music generation,"Applications -> gaming, augmented/virtual reality; Applications -> music composition; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> musical affect, emotion and mood; Musical features and properties -> rhythm, beat, tempo",No,Musical arrangements based on emotions.,Yes,Yes,No,000089.pdf,https://archives.ismir.net/ismir2022/paper/000089.pdf,https://drive.google.com/open?id=1vEyuNN9_Ol9nW987ZlxJ8qZh8BY7jCBu,https://drive.google.com/open?id=1vIQuBKX69giJ73EStghxKlxwThSTIiam,https://drive.google.com/open?id=1th5cZjVh8_6Bb20PH70JcPgVzIrrPM2j,,https://slack.com/app_redirect?channel=C04CY0M5ZBK,p6-09-takahashi
195,3,6,9,"In-person, in Bengaluru",FALSE,Using Activation Functions for Improving Measure-Level Audio Synchronization,"Audio synchronization aims at aligning multiple recordings of the same piece of music. Traditional synchronization approaches are often based on dynamic time warping using chroma features as an input representation. Previous work has shown how one can integrate onset cues into this pipeline for improving the alignment's temporal accuracy. Furthermore, recent work based on deep neural networks has led to significant improvements for learning onset, beat, and downbeat activation functions. However, for music with soft onsets and abrupt tempo changes, these functions may be unreliable, leading to unstable results. As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline. We show that this approach improves the temporal accuracy thanks to the activation cues while inheriting the robustness of the traditional synchronization approach. Conducting experiments based on string quartet recordings, we evaluate our combined approach where we transfer measure annotations from a reference recording to a target recording.",Yigitcan Özer,yigitcan.oezer@audiolabs-erlangen.de,"Yigitcan Özer (International Audio Laboratories Erlangen, Germany)*; Matej Ištvánek (Brno University of Technology, Brno, Czech Republic); Vlora Arifi-Müller (International Audio Laboratories Erlangen, Germany); Meinard Müller (International Audio Laboratories Erlangen, Germany)","Özer, Yigitcan*; Ištvánek, Matej; Arifi-Müller, Vlora; Müller, Meinard",yigitcan.oezer@audiolabs-erlangen.de*; matej.istvanek@vut.cz; vlora.arifi-mueller@audiolabs-erlangen.de; meinard.mueller@audiolabs-erlangen.de,"MIR tasks -> alignment, synchronization, and score following","Musical features and properties -> musical style and genre; Musical features and properties -> representations of music; Musical features and properties -> rhythm, beat, tempo",No,"As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline to improve the temporal accuracy thanks to the onset-, beat-, and downbeat-based activation cues while inheriting the robustness of the traditional synchronization approach.",Yes,Yes,No,000090.pdf,https://archives.ismir.net/ismir2022/paper/000090.pdf,https://drive.google.com/open?id=1O7TNpNkLw1zlSaIxeuR2xH-CRpf39F9C,https://drive.google.com/open?id=1Vdd92SN5zEyv8jSpFMuPQJbGX-PZknYt,https://drive.google.com/open?id=1YSwLNC-Z6C3L80mlEKbRc9HPt4QKVPGm,https://drive.google.com/open?id=1ft6177a_kdSLkVF1c0qzzDT64Pyym9kE,https://slack.com/app_redirect?channel=C04D92HA7RN,p6-10-özer
196,3,6,10,Virtually,FALSE,A deep learning method for melody extraction from a polyphonic symbolic music representation,"The task of identifying melodic lines in a polyphonic piece is a known active research topic in the symbolic and audio domain. Its importance has attracted the interest of researchers focusing on Music Information Retrieval and musicological applications and achieving high results is a common goal in industrial applications. The distinction of a melody in a written score can be a challenging task, however improvements have been reported in recent years using deep learning methods. In this paper, we present a lightweight deep bidirectional LSTM model for identifying the most salient melodic line of a music piece using handcrafted features without requiring the input score to be separated into multiple parts. We evaluate our model to measure the effectiveness of several data augmentation techniques and to compare performance to other state-of-the-art models. We also identify the features importance and evaluate their incremental contribution on the model performance using evaluation metrics. Results on the POP909 dataset show that our model approximates or outperforms current state of the art models trained on the same dataset, based on different implemented metrics and observations.",Katerina Kosta,katerina.kosta@bytedance.com,Katerina Kosta (ByteDance)*; Wei Tsung Lu (ByteDance); Gabriele Medeot (ByteDance); Pierre Chanquion (ByteDance),"Kosta, Katerina*; Lu, Wei Tsung; Medeot, Gabriele; Chanquion, Pierre",katerina.kosta@bytedance.com*; weitsung.lu@bytedance.com; gabriele.medeot@bytedance.com; pierre.chanquion@bytedance.com,Musical features and properties -> melody and motives,MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification,No,"We present a bidirectional LSTM model for melody extraction from a polyphonic symbolic music score trained on handcrafted features, without requiring the input score to be separated into multiple parts. Results on the POP909 dataset show that our model outperforms current state of the art models trained on the same dataset.",No,Yes,No,000091.pdf,https://archives.ismir.net/ismir2022/paper/000091.pdf,https://drive.google.com/file/d/1q1HAq03MtNNx2tqb0KQXLQWHWpojYKTg,https://drive.google.com/file/d/1spdzWUhSXptwi4Nw1rXXTEhss8f50Cm1/view?usp=share_link,https://drive.google.com/file/d/1XdHgdldt8Es5aGmjd86hluqNDrtyR2fc/view?usp=share_link,,https://slack.com/app_redirect?channel=C04D92HB52L,p6-11-kosta
209,3,6,11,"In-person, in Bengaluru",FALSE,A Reproducibility Study on User-centric MIR Research and Why it is Important,"Reproducibility of results is a central pillar of scientific work. In music information retrieval research, this is widely acknowledged and practiced by the community by re-implementing algorithms and re-validating machine learning experiments. In this paper, we argue for an increased need to also reproduce the results and findings of user studies, including qualitative work, especially since these often lay the foundations and serve as justification for choices taken in algorithmic design and optimization criteria. As an example, we attempt to reproduce the study by Kim et al. presented in the RecSys (2020) paper ''Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations.'' By repeating this study on how interpersonal relationships can affect a user's assessment of music recommendations on a new sample of n=142 participants, we can largely confirm and support the validity of the original results. At the same time, we extend the analysis and also observe differences with regards to adoption rates between different channels as well as different factors that influences the adoption rate. From this specific reproducibility study, we conclude that potential cultural differences should be accounted for more explicitly in future studies and that systems development should be more explicitly connected to its intended target audience.",Peter Knees,peter.knees@tuwien.ac.at,"Peter Knees (Faculty of Informatics, TU Wien, Austria, School of Music, Georgia Institute of Technology, USA)*; Bruce Ferwerda (Department of Computer Science and Informatics, Jönköping University, Sweden); Andreas Rauber (Faculty of Informatics, TU Wien, Austria); Sebastian Strumbelj (Faculty of Informatics, TU Wien, Austria); Annabel Resch (Faculty of Informatics, TU Wien, Austria); Laurenz Tomandl (Faculty of Informatics, TU Wien, Austria); Valentin Bauer (Faculty of Informatics, TU Wien, Austria); Fung Yee Tang (Faculty of Informatics, TU Wien, Austria); Josip Bobinac (Faculty of Informatics, TU Wien, Austria); Amila Ceranic (Faculty of Informatics, TU Wien, Austria); Riad Dizdar (Faculty of Informatics, TU Wien, Austria)","Knees, Peter*; Ferwerda, Bruce; Rauber, Andreas; Strumbelj, Sebastian; Resch, Annabel; Tomandl, Laurenz; Bauer, Valentin; Tang, Fung Yee; Bobinac, Josip; Ceranic, Amila; Dizdar, Riad",peter.knees@tuwien.ac.at*; bruce.ferwerda@ju.se; rauber@ifs.tuwien.ac.at; e12007910@student.tuwien.ac.at; e119142@student.tuwien.ac.at; e1326545@student.tuwien.ac.at; valentin.bauer@tuwien.ac.at; e51850916@student.tuwien.ac.at; josip.bobinac@tuwien.ac.at; e12046164@student.tuwien.ac.at; e12046165@student.tuwien.ac.at,"Evaluation, datasets, and reproducibility -> reproducibility",Human-centered MIR,No,"Reproducibility of results is a central pillar of scientific work, as an example we attempt to reproduce the qualitative study by Kim et al. presented in the RecSys 2020 paper ``Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations''",No,Yes,No,000092.pdf,https://archives.ismir.net/ismir2022/paper/000092.pdf,https://drive.google.com/open?id=1wi6KSaA3kqqAr3917K4KWxxV-XJiketH,https://drive.google.com/open?id=1MdamLjENVz249t-oqgq47-TCkuSaaMT3,https://drive.google.com/open?id=1TsjCIMzXbuDW0QgA9MvSxMp6WvkJUFWh,https://drive.google.com/open?id=1QhVU-XqjDhPivnBxO1ASk7pRMQNerMBI,https://slack.com/app_redirect?channel=C04C4QQFBCP,p6-12-knees
225,3,6,12,Virtually,FALSE,Music Separation Enhancement with Generative Modeling,"Despite phenomenal progress in recent years, state-of-the-art music separation systems produce source estimates with significant perceptual shortcomings, such as adding extraneous noise or removing harmonics. We propose a post-processing model (the Make it Sound Good (MSG) post-processor) to enhance the output of music source separation systems. We apply our post-processing model to state-of-the-art waveform-based and spectrogram-based music source separators, including a separator unseen by MSG during training. Our analysis of the errors produced by source separators shows that waveform models tend to introduce more high-frequency noise, while spectrogram models tend to lose transients and high frequency content. We introduce objective measures to quantify both kinds of errors and show MSG improves the source reconstruction of both kinds of errors. Crowdsourced subjective evaluations demonstrate that human listeners prefer source estimates of bass and drums that have been post-processed by MSG.",Noah Schaffer,noahschaffer2022@u.northwestern.edu,"Noah Schaffer (Interactive Audio Lab, Northwestern University, Evanston IL, USA)*; Boaz Cogan (Interactive Audio Lab, Northwestern University, Evanston IL, USA); Ethan Manilow (Interactive Audio Lab, Northwestern University, Evanston IL, USA); Max Morrison (Interactive Audio Lab, Northwestern University, Evanston IL, USA); Prem Seetharaman (Descript, Inc); Bryan Pardo (Interactive Audio Lab, Northwestern University, Evanston IL, USA)","Schaffer, Noah*; Cogan, Boaz; Manilow, Ethan; Morrison, Max; Seetharaman, Prem; Pardo, Bryan",noahschaffer2022@u.northwestern.edu*; boazcogan@gmail.com; ethanm@u.northwestern.edu; morrimax@u.northwestern.edu; prem@descript.com; pardo@northwestern.edu,MIR tasks -> sound source separation,"Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> MIR tasks; MIR tasks -> music generation; MIR tasks -> music synthesis and transformation",No,Analyzing issues with source separation systems and proposing a post-processor to fix those issues,Yes,Yes,No,000093.pdf,https://archives.ismir.net/ismir2022/paper/000093.pdf,https://drive.google.com/open?id=1ePaU2wBytZx_pqJb8uouxlJS9BRk9LLO,https://drive.google.com/open?id=10uCuqwwsDUCyYSMBhXU_VJlzouwMmuIn,https://drive.google.com/open?id=19Weiy20EdE8xvO9KfWOHvJ-fB5VlQzxw,,https://slack.com/app_redirect?channel=C04CKB9JE2E,p6-13-schaffer
211,3,6,13,"In-person, in Bengaluru",FALSE,SampleMatch: Drum Sample Retrieval by Musical Context,"Modern digital music production typically involves combining numerous acoustic elements to compile a piece of music. Important types of such elements are drum samples, which determine the characteristics of the percussive components of the piece. Artists must use their aesthetic judgement to assess whether a given drum sample fits the current musical context. However, selecting drum samples from a potentially large library is tedious and may interrupt the creative flow. In this work, we explore the automatic drum sample retrieval based on aesthetic principles learned from data. As a result, artists can rank the samples in their library by fit to some musical context at different stages of the production process (i.e., by fit to incomplete song mixtures). To this end, we use contrastive learning to maximize the score of drum samples originating from the same song as the mixture. We conduct a listening test to determine whether the human ratings match the automatic scoring function. We also perform objective quantitative analyses to evaluate the efficacy of our approach.",Stefan Lattner,stefan.lattner@sony.com,"Stefan Lattner (Sony Computer Science Laboratories, Paris, France)*","Lattner, Stefan*",stefan.lattner@sony.com,MIR tasks -> indexing and querying,"Applications -> performance, and production; Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> similarity metrics; Musical features and properties -> timbre, instrumentation, and singing voice",No,Contrastive learning is used for automatic conditional drum sample selection in music production.,No,No,No,000094.pdf,https://archives.ismir.net/ismir2022/paper/000094.pdf,https://drive.google.com/open?id=1v8NaHzFdP4gTEFX6ElPx52-Byh-x87BW,https://drive.google.com/open?id=1Ljv0GV74Jpg-ELBpO--bIfc4py3qCh0E,https://drive.google.com/open?id=1L7h56UFGymGdMdZd6tHjL_0n6aRO3ltF,https://docs.google.com/presentation/d/1o89Z9XIr83eNjI9HHV5JqYLTcnUSNoii0VeKeJfZgpA/edit?usp=sharing,https://slack.com/app_redirect?channel=C04C4QQG07R,p6-14-lattner
134,3,6,14,"In-person, in Bengaluru",FALSE,"A Transformer-Based ""Spellchecker"" for Detecting Errors in OMR Output","The outputs of Optical Music Recognition (OMR) systems require time-consuming human correction. Given that most of the errors induced by OMR processes appear ""non-musical"" to humans, we propose that the time to correct errors may be reduced by marking all symbols on a score that are musically unlikely, allowing the human to focus their attention accordingly. Using a dataset of Romantic string quartets, we train a variant of the Transformer network architecture on the task of classifying each symbol of an optically-recognized musical piece in symbolic format as correct or erroneous, based on whether a manual correction of the piece would require an insertion, deletion, or replacement of a symbol at that location. Since we have a limited amount of data with real OMR errors, we employ extensive data augmentation to add errors into training data in a way that mimics how OMR would modify the score. Our best-performing models achieve 99% recall and 50% precision on this error-detection task.",Timothy de Reuse,timothy.dereuse@mcgill.ca,"Timothy de Reuse (Centre for Interdisciplinary Research in Music Media and Technology, McGill University)*; Ichiro Fujinaga (Centre for Interdisciplinary Research in Music Media and Technology, McGill University)","de Reuse, Timothy*; Fujinaga, Ichiro",timothy.dereuse@mcgill.ca*; ichiro.fujinaga@mcgill.ca,MIR tasks -> optical music recognition,Applications -> music retrieval systems; Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; MIR fundamentals and methodology -> symbolic music processing,No,"Optical Music Recognition requires human correction; we train a machine-learning model to highlight things in a score that look musically ""wrong,"" so that the human can spend more time correcting those things and less time checking parts of a score that are already correct.",Yes,Yes,No,000095.pdf,https://archives.ismir.net/ismir2022/paper/000095.pdf,https://drive.google.com/open?id=1iOUbYiJZQgZeDPZfF72fr_Lhykj7WhNQ,https://drive.google.com/open?id=1BHbdECdT3ATAqOPD9BkQE6-8YJcc8aLu,https://drive.google.com/open?id=1QFjVEhTbOEl5Y4T9OOw4-X4R1ikWhZ6O,https://drive.google.com/open?id=1DvddSprUCi39K_LQ6F5kCPrC5HupuUF-,https://slack.com/app_redirect?channel=C04CMQV3Y76,p6-15-reuse
281,3,6,15,"In-person, in Bengaluru",FALSE,"""More than words"": Linking Music Preferences and Moral Values through Lyrics","This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song’s overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression ap- proaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (.20 ≤ r ≤ .30) than values of empathy and equality (.08 ≤ r ≤ .11), while basic demographic variables only account for a small part in the models’ explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.",Vjosa Preniqi,v.preniqi@qmul.ac.uk,"Vjosa Preniqi (Centre for Digital Music, Queen Mary University of London, London UK)*; Kyriaki Kalimeri (ISI Foundation, Turin, Italy); Charalampos Saitis (Centre for Digital Music, Queen Mary University of London, London UK)","Preniqi, Vjosa*; Kalimeri, Kyriaki; Saitis, Charalampos",v.preniqi@qmul.ac.uk*; kyariaki.kalimeri@isi.it; c.saitis@qmul.ac.uk,Human-centered MIR,Applications -> music recommendation and playlist generation; Human-centered MIR -> personalization; Human-centered MIR -> user behavior analysis and mining; Human-centered MIR -> user modeling; MIR fundamentals and methodology -> lyrics and other textual data,No,Peoples‚Äô worldviews and moral values are reflected in their music preferences as modelled through lyrics.,Yes,Yes,No,000096.pdf,https://archives.ismir.net/ismir2022/paper/000096.pdf,https://drive.google.com/open?id=1MMI5RbZdgjhQOavn7Slxzysb2RQ_Tza1,https://drive.google.com/open?id=1anqS2_P7AJ8Ft9JvpfWblzDDsGNmeF4T,https://drive.google.com/open?id=1IjadVnjYR-DHiQV8XFQ7aAAJOCNFHwjS,https://drive.google.com/open?id=1tnSL4_DzkqzizNKNXHMwV_cwU1sCPO6K,https://slack.com/app_redirect?channel=C04CMQWA9M2,p6-16-preniqi
156,4,7,0,Virtually,FALSE,A unified model for zero-shot singing voice conversion and synthesis,"Recent advances in deep learning not only facilitate the implementation of zero-shot singing voice synthesis (SVS) and singing voice conversion (SVC) tasks but also provide the opportunity to unify these two tasks into one generalized model. In this paper, we propose such a model that generate the singing voice of any target singer from any source singing content in either text or audio format. The model incorporates self-supervised joint training of the phonetic encoder and the acoustic encoder, with an audio-to-phoneme alignment process in each training step, such that these encoders map the audio and text data respectively into a shared, temporally aligned, and singer agnostic latent space. The target singer’s latent representations encoded at different granularity levels are all trained to match the source latent representations sequentially with the attention mechanisms in the decoding stage. This enables the model to generate unseen target singer’s voice with fine-grained resolution from either text or audio sources. Both objective and subjective experiments confirmed that the proposed model is competitive with the state-of-the-art SVC and SVS methods.",Li Su,lisu@iis.sinica.edu.tw,"Jui-Te Wu (NTU-AS Data Science Degree Program, National Taiwan University, Taiwan); Jun-You Wang (Department of Computer Science and Information Engineering, National Taiwan University, Taiwan); Jyh-Shing Roger Jang (NTU-AS Data Science Degree Program, National Taiwan University, Taiwan, Department of Computer Science and Information Engineering, National Taiwan University, Taiwan); Li Su (NTU-AS Data Science Degree Program, National Taiwan University, Taiwan, Institute of Information Science, Academia Sinica, Taiwan)*","Wu, Jui-Te; Wang, Jun-You; Jang, Jyh-Shing Roger; Su, Li*",bryan051003@gmail.com; junyou.wang@mirlab.org; jang@csie.ntu.edu.tw; lisu@iis.sinica.edu.tw*,MIR tasks -> music synthesis and transformation,"MIR tasks -> music generation; Musical features and properties -> timbre, instrumentation, and singing voice",No,We propose a unified model that combines the singing voice conversion and singing voice synthesis together.,Yes,No,No,000097.pdf,https://archives.ismir.net/ismir2022/paper/000097.pdf,https://drive.google.com/open?id=1sDYV4eSrxppXdXHjsHriKtQ7hSQTcNcY,https://drive.google.com/open?id=1BL5gPSod8aX-uNeCwpZfPT92T6_EfxGp,https://drive.google.com/open?id=1aQqBJoPXKXWcyTLnThakkirPRCfUIW6O,https://drive.google.com/open?id=1AqY3RHVKv0P8PLe9bYHogobZKGap7MbM,https://slack.com/app_redirect?channel=C04CGCV0NSZ,p7-01-su
168,4,7,1,Virtually,FALSE,Semantic Control of Generative Musical Attributes,"Deep generative neural networks have been successful in tasks such as composing novel music and rendering expressive performance. Controllability is essential for building creative tools from such models. Recent work in this area has focused on disentangled latent space representations, but this is only part of the solution. Efficient control of semantic attributes must handle non-linearities and holes that occur in latent spaces, whilst minimising unwanted changes to other attributes. This paper introduces SeNT-Gen, a neural traversal algorithm that uses a secondary neural network to model the complex relationships between latent codes and musical attributes. This enables precise editing of semantic attributes that adapts to context. We demonstrate the method using the dMelodies dataset, and show strong performance for several VAE models.",Stewart Greenhill,s.greenhill@deakin.edu.au,"Stewart Greenhill (Applied Artificial Intelligence Institute, Deakin University, Australia)*; Majid Abdolshah (Applied Artificial Intelligence Institute, Deakin University, Australia); Vuong Le (Applied Artificial Intelligence Institute, Deakin University, Australia); Sunil Gupta (Applied Artificial Intelligence Institute, Deakin University, Australia); Svetha Venkatesh (Applied Artificial Intelligence Institute, Deakin University, Australia)","Greenhill, Stewart*; Abdolshah, Majid; Le, Vuong; Gupta, Sunil; Venkatesh, Svetha",s.greenhill@deakin.edu.au*; m.abdolshah@deakin.edu.au; vuong.le@deakin.edu.au; sunil.gupta@deakin.edu.au; svetha.venkatesh@deakin.edu.au,Domain knowledge -> machine learning/artificial intelligence for music,Human-centered MIR -> human-computer interaction; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation,No,"Generative AI can show astounding creativity, but to be useful to musicians we need to provide appropriate interactive control over the process, using methods like those introduced in this paper.",No,Yes,No,000098.pdf,https://archives.ismir.net/ismir2022/paper/000098.pdf,https://drive.google.com/open?id=1g0hfLPDSrY5tHMNUzKEJMMKjWwwlYTvo,https://drive.google.com/open?id=1VE7uZf3V3GZZ-noD7BPZwcjn1AZuO7bt,https://drive.google.com/open?id=1i7t5YjaI3T3odMJxnVerf1qYCD8AWxpN,,https://slack.com/app_redirect?channel=C04CY0N0YL9,p7-02-greenhill
243,4,7,2,"In-person, in Bengaluru",FALSE,Music Representation Learning Based on Editorial Metadata from Discogs,"This paper revisits the idea of music representation learning supervised by editorial metadata, contributing to the state of the art in two ways. First, we exploit the public editorial metadata available on Discogs, an extensive community-maintained music database containing information about artists, releases, and record labels. Second, we use a contrastive learning setup based on COLA, different from previous systems based on triplet loss. We train models targeting several associations derived from the metadata and experiment with stacked combinations of learned representations, evaluating them on standard music classification tasks. Additionally, we consider learning all the associations jointly in a multi-task setup. We show that it is possible to improve the performance of current self-supervised models by using inexpensive metadata commonly available in music collections, producing representations comparable to those learned on classification setups. We find that the resulting representations based on editorial metadata outperform a system trained with music style tags available in the same large-scale dataset, which motivates further research using this type of supervision. Additionally, we give insights on how to preprocess Discogs metadata to build training objectives and provide public pre-trained models.",Pablo Alonso-Jiménez,pablo.alonso@upf.edu,"Pablo Alonso-Jiménez (Music Technology Group, Universitat Pompeu Fabra)*; Xavier Serra (Music Technology Group, Universitat Pompeu Fabra); Dmitry Bogdanov (Music Technology Group, Universitat Pompeu Fabra)","Alonso-Jiménez, Pablo*; Serra, Xavier; Bogdanov, Dmitry",pablo.alonso@upf.edu*; xavier.serra@upf.edu; dmitry.bogdanov@upf.edu,Musical features and properties -> representations of music,"MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR tasks -> automatic classification; MIR tasks -> similarity metrics; Musical features and properties -> musical affect, emotion and mood; Musical features and properties -> musical style and genre",No,We use editorial metadata from Discogs to generate similarity targets and train contrastive learning models that show very competitive performance in different music classification tasks.,Yes,No,No,000099.pdf,https://archives.ismir.net/ismir2022/paper/000099.pdf,https://drive.google.com/open?id=1nEt3Z7jRysFimuJRPF0NZmeMY6qlXXWs,https://drive.google.com/open?id=1Q-OGckt9Dzd2R9cSzved_8uRG0xA0lZ3,https://drive.google.com/open?id=12uQCBAM3xQi9miVwq6GsAa2bossFb2cK,https://docs.google.com/presentation/d/1rrsO7pFRSVnQK1m6VSzSESkE395DKDLla5H6v9_RxR8/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CGCVKU69,p7-03-alonso-jiménez
59,4,7,3,Virtually,FALSE,Melody Infilling with User-Provided Structural Context,"This paper proposes a novel Transformer-based model for music score infilling, to generate a music passage that fills in the gap between given past and future contexts. While existing infilling approaches can generate a passage that connects smoothly locally with the given contexts, they do not take into account the musical form or structure of the music and may therefore generate overly smooth results. To address this issue, we propose a structure-aware conditioning approach that employs a novel attention-selecting module to supply user-provided structure-related information to the Transformer for infilling. With both objective and subjective evaluations, we show that the proposed model can harness the structural information effectively and generate melodies in the style of pop of higher quality than the two existing structure-agnostic infilling models.",Chih-Pin Tan,p76091551@gs.ncku.edu.tw,"Chih-Pin Tan (National Cheng Kung University, Academia Sinica)*; Alvin W Y Su (National Cheng Kung University); Yi-Hsuan Yang (Academia Sinica, Taiwan AI Labs)","Tan, Chih-Pin*; Su, Alvin W Y; Yang, Yi-Hsuan",p76091551@gs.ncku.edu.tw*; alvinsu@mail.ncku.edu.tw; yang@citi.sinica.edu.tw,Applications -> music composition,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation; Musical features and properties -> structure, segmentation, and form",No,We show that structure information should be incorporated when performing symbolic-domain music score infilling/inpainting.,No,Yes,No,000100.pdf,https://archives.ismir.net/ismir2022/paper/000100.pdf,https://drive.google.com/open?id=1zdgG8FxH4w_tnBdKcxbIU2EOC2XqdRKR,https://drive.google.com/open?id=1QirWM-r5jIpZG40qeOcsAuOZsgtokM9S,https://drive.google.com/open?id=1917A7wfbJZPRKUrzRzKE3yn-yoxMhKRm,,https://slack.com/app_redirect?channel=C04CGCTVCP7,p7-04-tan
7,4,7,4,"In-person, in Bengaluru",FALSE,Robust Melody Track Identification in Symbolic Music,"Melody tracks are worthy of special attention in the field of symbolic music information retrieval (MIR) because they contribute more towards music perception than many other musical components. However, many existing symbolic MIR systems neglect melody track identification (MTI) and are thus less effective. Existing MTI methods are also not robust and perform poorly on MIDI files representing music of unusual genres, arrangements, or formats. To address this problem, we propose a CNN-Transformer-based MTI model designed to robustly identify a single melody track for a given MIDI file. As this process can take a sizable amount of time for long songs, we also use a sparse Transformer to speed up attention computation. Our experiments show that our proposed model outperforms state-of-the-art (SOTA) algorithms in accuracy and can also benefit downstream MIR tasks.",Ye Wang,wangye@comp.nus.edu.sg,Xichu Ma (National University of Singapore); Xiao Liu (National University of Singapore); Bowen Zhang (National University of Singapore); Ye Wang (National University of Singapore)*,"Ma, Xichu; Liu, Xiao; Zhang, Bowen; Wang, Ye*",ma_xichu@u.nus.edu; liuxiao@u.nus.edu; zbowen@comp.nus.edu; wangye@comp.nus.edu.sg*,MIR fundamentals and methodology -> symbolic music processing,"Domain knowledge -> cognitive MIR; Domain knowledge -> machine learning/artificial intelligence for music; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> music transcription and annotation; Musical features and properties -> melody and motives",No,"An accurate, efficient, and robust melody track identification model for symbolic music.",Yes,Yes,No,000101.pdf,https://archives.ismir.net/ismir2022/paper/000101.pdf,https://drive.google.com/open?id=1_FbTmND6no8vptyIFsC9rjI6deCjxDTp,https://drive.google.com/open?id=1k_xWVpieuYbnYrqcKtBMJnsyr4DcU7qr,https://drive.google.com/open?id=1ruzDpJPC5nQGbRYWyvqYmQQa9Kz4r-X8,,https://slack.com/app_redirect?channel=C04CGCTJME1,p7-05-wang
12,4,7,5,Virtually,FALSE,Tracking the Evolution of a Band's Live Performances over Decades,"Evolutionary studies have become a dominant thread in the analysis of large audio collections. Such corpora usually consist of musical pieces by various composers or bands and the studies usually focus on identifying general historical trends in harmonic content or music production techniques. In this paper we present a comparable study that examines the music of a single band whose publicly available live recordings span three decades. We first discuss the opportunities and challenges faced when working with single-artist and live-music datasets and introduce solutions for audio feature validation and outlier detection. We then investigate how individual songs vary over time and identify general performance trends using a new approach based on relative feature values, which improves accuracy for features with a large variance. Finally, we validate our findings by juxtaposing them with descriptions posted in online forums by experienced listeners of the band's large following.",Florian Thalmann,thalm007@umn.edu,"Florian Thalmann (Graduate School of Informatics, Kyoto University, Japan)*; Eita Nakamura (Graduate School of Informatics, Kyoto University, Japan); Kazuyoshi Yoshii (Graduate School of Informatics, Kyoto University, Japan)","Thalmann, Florian*; Nakamura, Eita; Yoshii, Kazuyoshi",thalm007@umn.edu*; eita.nakamura@i.kyoto-u.ac.jp; yoshii@i.kyoto-u.ac.jp,Domain knowledge -> computational music theory and musicology,"Applications -> digital libraries and archives; Applications -> performance, and production; Evaluation, datasets, and reproducibility -> MIR tasks; Evaluation, datasets, and reproducibility -> novel datasets and use cases",No,We can use evolutionary methods to study the music of a single band.,No,Yes,No,000102.pdf,https://archives.ismir.net/ismir2022/paper/000102.pdf,https://drive.google.com/open?id=1uAL5bgP8TUTwTyMguU2bOLfHMUR7G_hI,https://drive.google.com/open?id=1DhB8xAaKrGiFcr2MtjLFHg34QWcnOAuu,https://drive.google.com/open?id=1r_zMQ2KOCtuHusNoEpiRSdO8ddrcLTTE,,https://slack.com/app_redirect?channel=C04CMQU1H8U,p7-06-thalmann
308,4,7,6,"In-person, in Bengaluru",FALSE,Evaluating Generative Audio Systems and Their Metrics,"Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems.",Ashvala Vinay,ashvala@gatech.edu,"Ashvala Vinay (Center for Music Technology, Georgia Institute of Technology)*; Alexander Lerch (Center for Music Technology, Georgia Institue of Technology)","Vinay, Ashvala*; Lerch, Alexander",ashvala@gatech.edu*; alexander.lerch@gatech.edu,"Evaluation, datasets, and reproducibility","Evaluation, datasets, and reproducibility -> evaluation methodology; MIR tasks -> music synthesis and transformation",No,Generative audio systems driven by deep learning techniques have paved the way for a new era of sound synthesis. The rapid advancements over the last few years in these techniques have outpaced the development of metrics to evaluate the audio quality of these systems. We evaluate networks on a consistent set of metrics and ran a listening study to validate what the metrics might suggest perceptually and found that the listener ratings do not match the metrics.,Yes,Yes,No,000103.pdf,https://archives.ismir.net/ismir2022/paper/000103.pdf,https://drive.google.com/open?id=1joOvb5Bd7qxAYR2GlteXaKU9pyuXr8KE,https://drive.google.com/open?id=13DiY5H3CC_5P4QrXjT-u-zNktZsntn1m,https://drive.google.com/open?id=1To34pJg900ZjGQis0n6C4a2E7amV3eP7,https://drive.google.com/open?id=1FRdNEbbNOg9LZB57SKZeyb10FV1h0sYa,https://slack.com/app_redirect?channel=C04CKBAAB42,p7-07-vinay
53,4,7,7,Virtually,FALSE,Representation Learning for the Automatic Indexing of Sound Effects Libraries,"Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness.",Alison B Ma,ama67@gatech.edu,"Alison B Ma (Music Informatics Group, Georgia Institute of Technology)*; Alexander Lerch (Music Informatics Group, Georgia Institute of Technology)","Ma, Alison B*; Lerch, Alexander",ama67@gatech.edu*; alexander.lerch@gatech.edu,MIR tasks -> automatic classification,Applications -> music retrieval systems; Domain knowledge -> machine learning/artificial intelligence for music; Domain knowledge -> representations of music; MIR tasks -> indexing and querying; Musical features and properties -> representations of music,No,We introduce a new pre-trained representation for automatic sound effects library classification and show the effectiveness of cross-dataset training over metric learning for this task.,Yes,No,No,000104.pdf,https://archives.ismir.net/ismir2022/paper/000104.pdf,https://drive.google.com/open?id=1EJsKWAm3a3JCiTGhYx1XUokQwjSvViE6,https://drive.google.com/open?id=1L7ZphOctH5PbeJFUlwRlcIxD_by_ELHc,https://drive.google.com/open?id=1SySGsZHnjCCG5fM87XeVEVxC8e24MdFO,,https://slack.com/app_redirect?channel=C04CGCTUN13,p7-08-ma
67,4,7,8,"In-person, in Bengaluru",FALSE,"Concept-Based Techniques for ""Musicologist-Friendly"" Explanations in Deep Music Classifiers","Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.",Katharina Hoedt,katharina.hoedt@jku.at,"Francesco Foscarin (Institute of Computational Perception, Johannes Kepler University Linz, Austria); Katharina Hoedt (Institute of Computational Perception, Johannes Kepler University Linz, Austria)*; Verena Praher (Institute of Computational Perception, Johannes Kepler University Linz, Austria); Arthur Flexer (Institute of Computational Perception, Johannes Kepler University Linz, Austria); Gerhard Widmer (Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria)","Foscarin, Francesco; Hoedt, Katharina*; Praher, Verena; Flexer, Arthur; Widmer, Gerhard",francesco.foscarin@jku.at; katharina.hoedt@jku.at*; verena.praher@jku.at; arthur.flexer@jku.at; gerhard.widmer@jku.at,Domain knowledge -> computational music theory and musicology,Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification; Musical features and properties -> musical style and genre,No,High-level musical concepts can be used to develop human-friendly explanations of a composer classification system.,No,No,No,000105.pdf,https://archives.ismir.net/ismir2022/paper/000105.pdf,https://drive.google.com/open?id=1JQR--TXENiS7hXgS169ktY1NeSnckSiY,https://drive.google.com/open?id=1t-4k-R3PCNSYQaxR1-AGLuss9M7nPnJH,https://drive.google.com/open?id=1Zl2_LH5Qda6dPIte_zJW6_I2XJFx6fMh,https://docs.google.com/presentation/d/1-nGwScKwvkBVEu-rHS2-ZxcvitiKBO7sZkQ5llJUZJQ/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CKB87ESW,p7-09-hoedt
92,4,7,9,Virtually,FALSE,Verse versus Chorus: Structure-aware Feature Extraction for Lyrics-based Genre Recognition,"The aim of lyrics-based genre recognition is to automatically determine the genre of a given song based on its lyrics. Previous approaches for this task have commonly used textual features extracted from the entirety of a song's lyrics, neglecting the inherent structure of lyrics consisting of, for instance, verses and choruses. Therefore, we pose the hypothesis that features extracted from different parts of the lyrics can have significantly different predictive power. To test this hypothesis, we perform a series of experiments to determine whether models trained on features taken from verses and choruses perform differently for genre recognition. Our experiments indeed confirm our hypothesis, showing that generally, using features extracted from verses leads to higher performance than features extracted from choruses. Digging deeper, we found that this is especially true for pop and rap songs. Rock songs show the opposite effect, with features extracted from choruses performing better than those taken from verses.",Maximilian Mayerl,maximilian.mayerl@uibk.ac.at,"Maximilian Mayerl (Department of Computer Science, Leopold-Franzens-Universität Innsbruck, Austria)*; Stefan Brandl (Institute of Computational Perception, Johannes Kepler Universität Linz, Austria, Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria); Günther Specht (Department of Computer Science, Leopold-Franzens-Universität Innsbruck, Austria); Markus Schedl (Institute of Computational Perception, Johannes Kepler Universität Linz, Austria, Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria); Eva Zangerle (Department of Computer Science, Leopold-Franzens-Universität Innsbruck, Austria)","Mayerl, Maximilian*; Brandl, Stefan; Specht, Günther; Schedl, Markus; Zangerle, Eva",maximilian.mayerl@uibk.ac.at*; stefan.brandl@jku.at; guenther.specht@uibk.ac.at; markus.schedl@jku.at; eva.zangerle@uibk.ac.at,MIR tasks -> automatic classification,"Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> lyrics and other textual data; Musical features and properties -> musical style and genre; Musical features and properties -> structure, segmentation, and form",No,"Features extracted from different structural parts of a song lead to different performance for lyrics-based genre recognition, with features taken from verses performing significantly better for rap and pop, while features extracted from choruses give better performance for rock.",Yes,Yes,No,000106.pdf,https://archives.ismir.net/ismir2022/paper/000106.pdf,https://drive.google.com/open?id=185Rq0MoJFw_uwaQbNknlj8kHgEqnQaa3,https://drive.google.com/open?id=131Jy3tK0QjPWhBP6ujYRX6UeCFx2gSZ-,https://drive.google.com/open?id=1q_GOzN6TfcbPvrh-izQyB-x6osnQekOB,,https://slack.com/app_redirect?channel=C04CCP29X6J,p7-10-mayerl
94,4,7,10,"In-person, in Bengaluru",FALSE,Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription,"Automatic speech recognition (ASR) has progressed significantly in recent years due to the emergence of large-scale datasets and the self-supervised learning (SSL) paradigm. However, as its counterpart problem in the singing domain, the development of automatic lyric transcription (ALT) suffers from limited data and degraded intelligibility of sung lyrics. To fill in the performance gap between ALT and ASR, we attempt to exploit the similarities between speech and singing. In this work, we propose a transfer-learning-based ALT solution that takes advantage of these similarities by adapting wav2vec 2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of transfer learning by exploring the influence of different transfer starting points. We further enhance the performance by extending the original CTC model to a hybrid CTC/attention model. Our method surpasses previous approaches by a large margin on various ALT benchmark datasets. Further experiments show that, with even a tiny proportion of training data, our method still achieves competitive performance.",Longshen Ou,longshen@comp.nus.edu.sg,"Longshen Ou (School of Computing, National University of Singapore)*; Xiangming Gu (School of Computing, National University of Singapore); Ye Wang (School of Computing, National University of Singapore)","Ou, Longshen*; Gu, Xiangming; Wang, Ye",longshen@comp.nus.edu.sg*; xiangming@comp.nus.edu.sg; wangye@comp.nus.edu.sg,MIR fundamentals and methodology -> lyrics and other textual data,"Applications -> music retrieval systems; Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> web mining, and natural language processing; MIR tasks -> music transcription and annotation; Musical features and properties -> timbre, instrumentation, and singing voice",No,"This work proposes a method to transfer speech representation knowledge to singing domain, achieves state-of-the-art lyric transcription performance on various ALT benchmark datasets, as well as demonstrates the potential of low-resource lyric transcription.",Yes,No,No,000107.pdf,https://archives.ismir.net/ismir2022/paper/000107.pdf,https://drive.google.com/open?id=1J02o1LVVDJPfS9xwjtyrktHLDnvdyqWx,https://drive.google.com/open?id=1tDh_pupJdv1rGg5ImIf7nWHJs4sh4FIA,https://drive.google.com/open?id=1aJwe52UdKRS_Dc0pn_uVIAwazJX14ErS,https://docs.google.com/presentation/d/1uY6CkrwKhhRNuOgA4wpc8irDYDCgONvB/edit?usp=sharing&ouid=114426413689815620648&rtpof=true&sd=true,https://slack.com/app_redirect?channel=C04CY0MB30R,p7-11-ou
152,4,7,11,Virtually,FALSE,A Novel Dataset and Deep Learning Benchmark for Classical Music Form Recognition and Analysis,"Automated computational analysis schemes for Western classical music analysis based on form and hierarchical structure have not received much attention in the literature so far. One reason, of course, is the paucity of labeled datasets — which, if available, could be used to train machine learning approaches. Dataset curation cannot be crowdsourced; one needs trained musicians to devote sizable effort to carry out such annotations. Further, such an analysis is not simple for beginners; obtaining labeled data that can capture the nuances of a musician's reasoning acquired over years of practice is fraught with challenges. To this end, we provide a system for computational analysis of classical music, both for machine learning and music researchers. First, we introduce a labeled dataset containing 200 classical music pieces annotated by form and phrases. Then, by leveraging this dataset, we show that deep learning-based methods can be used to learn Form Classification as well as Phrase Analysis and Classification, for which few (if any) results have been reported yet. Taken together, we provide the community with a unique dataset as well as a toolkit needed to analyze classical music structure, which can be used or extended to drive applications in both commercial and educational settings.",Daniel Szelogowski,szelogowdj19@uww.edu,Daniel Szelogowski (University of Wisconsin - Whitewater)*; Lopamudra Mukherjee (University of Wisconsin - Whitewater); Benjamin Whitcomb (University of Wisconsin - Whitewater),"Szelogowski, Daniel*; Mukherjee, Lopamudra; Whitcomb, Benjamin",szelogowdj19@uww.edu*; mukherjl@uww.edu; whitcomb@uww.edu,Domain knowledge -> machine learning/artificial intelligence for music,"Applications -> music training and education; Domain knowledge -> computational music theory and musicology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> automatic classification; Musical features and properties -> structure, segmentation, and form",No,"We present a new publicly available dataset for musical form recognition and analysis seeking to correct errors of previous commonly used datasets (e.g., SALAMI), as well as a set of hybrid deep learning architectures for automatically analyzing classical music by large-form classification and phrase classification/analysis using onset detection methods.",Yes,Yes,No,000108.pdf,https://archives.ismir.net/ismir2022/paper/000108.pdf,https://drive.google.com/open?id=18EScDB4ill2V1GnNDaIY7Q-A0WNeTfX4,https://drive.google.com/open?id=1QkUF-gWX4l1BCbu-JWLE9b10Yy_bjY-3,https://drive.google.com/open?id=1lmMrdSbxvlxc-tqR6j3qDgo3UBYNca-0,,https://slack.com/app_redirect?channel=C04C4QQ3295,p7-12-szelogowski
228,4,7,12,"In-person, in Bengaluru",FALSE,BAF: An audio fingerprinting dataset for broadcast monitoring,"Audio Fingerprinting (AFP) is a well-studied problem in music information retrieval for various use-cases e.g. content-based copy detection, DJ-set monitoring, and music excerpt identification. However, AFP for continuous broadcast monitoring (e.g. for TV & Radio), where music is often in the background, has not received much attention despite its importance to the music industry. In this paper (1) we present BAF, the first public dataset for music monitoring in broadcast. It contains 74 hours of production music from Epidemic Sound and 57 hours of TV audio recordings. Furthermore, BAF provides cross-annotations with exact matching timestamps between Epidemic tracks and TV recordings. Approximately, 80% of the total annotated time is background music. (2) We benchmark BAF with public state-of-the-art AFP systems, together with our proposed baseline PeakFP: a simple, non-scalable AFP algorithm based on spectral peak matching. In this benchmark, none of the algorithms obtain a F1-score above 47%, pointing out that further research is needed to reach the AFP performance levels in other studied use cases. The dataset, baseline, and benchmark framework are open and available for research.",Guillem Cortès,cortes.sebastia@gmail.com,"Guillem Cortès (BMAT Licensing S.L., Barcelona, MTG, Universitat Pompeu Fabra, Barcelona)*; Alex Ciurana (BMAT Licensing S.L., Barcelona); Emilio Molina (BMAT Licensing S.L., Barcelona); Marius Miron (MTG, Universitat Pompeu Fabra, Barcelona); Owen Meyers (Epidemic Sound, Stockholm); Joren Six (IPEM, Ghent University, Ghent); Xavier Serra (MTG, Universitat Pompeu Fabra, Barcelona)","Cortès, Guillem*; Ciurana, Alex; Molina, Emilio; Miron, Marius; Meyers, Owen; Six, Joren; Serra, Xavier",cortes.sebastia@gmail.com*; aciurana@bmat.com; emolina@bmat.com; miron.marius@gmail.com; owen.meyers@epidemicsound.com; joren.six@ugent.be; xavier.serra@upf.edu,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Applications -> music retrieval systems; Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> reproducibility; MIR tasks -> fingerprinting; MIR tasks -> indexing and querying",No,New open audio fingerprinting dataset for broadcast monitoring,Yes,No,No,000109.pdf,https://archives.ismir.net/ismir2022/paper/000109.pdf,https://drive.google.com/open?id=15uRhIXqDebPcLCC3bRqZovkL_kXCoJqn,https://drive.google.com/open?id=1f0-EYH2ANBqVEvrBTI30GlU3-Of04UHr,https://drive.google.com/open?id=1svXtCf014Mohn9EVK_fx1Cu0cgKTAMmm,https://docs.google.com/presentation/d/11RYnGOyjfsh0VFmLpOYzj_lNym9tMRdiTly-T6XlFE4/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CK87EZJP,p7-13-cortès
173,4,7,13,"In-person, in Bengaluru",FALSE,Cadence Detection in Symbolic Classical Music using Graph Neural Networks.,"Cadences are complex structures that have been driving music from the beginning of contrapuntal polyphony until today. Detecting such structures is vital for numerous MIR tasks such as musicological analysis, key detection, music segmentation, and others. However, automatic cadence detection remains a challenging task mainly because it involves a combination of high-level musical elements like harmony, voice leading, and rhythm. In this work, we present a graph representation of symbolic scores as an intermediate means to solve the cadence detection task. We approach cadence detection as an imbalanced node classification problem using a Graph Convolutional Network. We obtain results that are at least on par with the state of the art, and we present a model capable of making predictions at multiple levels of granularity, from individual notes to beats, thanks to the fine-grained, note-by-note representation. Moreover, our experiments suggest that graph convolution is able to learn non-local features that assist in cadence detection, freeing us from the need of having to devise specialized features that encode non-local context. We argue that this general approach to modeling musical scores and classification tasks has a number of potential advantages, beyond the specific recognition task presented here.",Emmanouil Karystinaios,emmanouil.karystinaios@jku.at,"Emmanouil Karystinaios (Institute of Computational Perception, Johannes Kepler University Linz, Austria)*; Gerhard Widmer (Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria)","Karystinaios, Emmanouil*; Widmer, Gerhard",emmanouil.karystinaios@jku.at*; gerhard.widmer@jku.at,MIR fundamentals and methodology -> symbolic music processing,"Musical features and properties -> harmony, chords and tonality; Musical features and properties -> representations of music; Musical features and properties -> structure, segmentation, and form",No,"A simple, general graph representation of musical scores, in combination with Convolutional Graph Neural Networks, provide the basis for an automatic cadence detection model with state-of-the-art recognition performance and some promising general properties for score analysis tasks.",Yes,Yes,No,000110.pdf,https://archives.ismir.net/ismir2022/paper/000110.pdf,https://drive.google.com/open?id=1s0aQoGik8UHNbfbYkMA22QsRFL_VRc83,https://drive.google.com/open?id=1dNTgTRl7adkUEQ0Q0oLxwNtpRIEwegpE,https://drive.google.com/open?id=1hyu6MSPAUDVVMy2O1nCxT-jiSRzcruUP,https://docs.google.com/presentation/d/1ZfNZ9CeghGcER0SpQ3XayhbO7HwdQZAHtbCVXTxOBek/edit?usp=sharing,https://slack.com/app_redirect?channel=C04CK8741KM,p7-14-karystinaios
73,4,7,14,"In-person, in Bengaluru",FALSE,Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation,"The variational auto-encoder has become a leading framework for symbolic music generation, and a popular research direction is to study how to effectively control the generation process. A straightforward way is to control a model using different conditions during inference. However, in music practice, conditions are usually sequential (rather than simple categorical labels), involving rich information that overlaps with the learned representation. Consequently, the decoder gets confused about whether to ""listen to"" the latent representation or the condition, and sometimes just ignores the condition. To solve this problem, we leverage domain adversarial training to disentangle the representation from condition cues for better control. Specifically, we propose a condition corruption objective that uses the representation to denoise a corrupted condition. Minimized by a discriminator and maximized by the VAE encoder, this objective adversarially induces a condition-invariant representation. In this paper, we focus on the task of melody harmonization to illustrate our idea, while our methodology can be generalized to other controllable generative tasks. Demos and experiments show that our methodology facilitates not only condition-invariant representation learning but also higher-quality controllability compared to baselines.",Jingwei Zhao,jzhao@u.nus.edu,"Jingwei Zhao (Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)*; Gus Xia (Music X Lab, NYU Shanghai, MBZUAI); Ye Wang (School of Computing, NUS, Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)","Zhao, Jingwei*; Xia, Gus; Wang, Ye",jzhao@u.nus.edu*; gxia@nyu.edu; wangye@comp.nus.edu.sg,MIR tasks,Applications -> music composition; Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation; MIR tasks -> music synthesis and transformation,No,This work generalizes domain adversarial training to controllable music generation with complex sequential conditions.,Yes,No,No,000111.pdf,https://archives.ismir.net/ismir2022/paper/000111.pdf,https://drive.google.com/open?id=15PMfRRlWmV8zeUGbZzxfPbC5dNUui0ze,https://drive.google.com/open?id=1h29abkAL9gGQ6lU3d7MWZQME7Qv5TsJS,https://drive.google.com/open?id=1oU3Ai680ktJf5S80ZMWoA-e0HmMieXsg,https://docs.google.com/presentation/d/1yEoaAJ8_a1S2-AdcsS5faCKUimrTMJPQ/edit?usp=sharing&ouid=115661709969705131614&rtpof=true&sd=true,https://slack.com/app_redirect?channel=C04CMQUGT6Y,p7-15-zhao
206,4,7,15,Virtually,FALSE,Modeling perceptual loudness of piano tone: theory and applications,"The relationship between perceptual loudness and physical attributes of sound is an important subject in both computer music and psychoacoustics. Early studies of “equal-loudness contour” can trace back to the 1920s and the measured loudness with respect to intensity and frequency has been revised many times since then. However, most studies merely focus on synthesized sound, and the induced theories on natural tones with complex timbre has rarely been justified. To this end, we investigate both theory and applications of natural-tone loudness perception in this paper via modeling piano tone. The theory part contains: 1) an accurate measurement of piano-tone equal-loudness contour of pitches, and 2) a machine-learning model capable of inferring loudness purely based on spectral features trained on human subject measurements. As for the application, we apply our theory to piano control transfer, in which we adjust the MIDI velocities on two different player pianos (in different acoustic environments) to achieve the same perceptual effect. Experiments show that both of our theoretical loudness modeling and the corresponding performance control transfer algorithm significantly outperform their baselines.",Yang Qu,yangqu7-c@my.cityu.edu.hk,"Yang Qu (Music X Lab, NYU Shanghai, City University of Hong Kong)*; Yutian Qin (Music X Lab, NYU Shanghai, New York University); Lecheng Chao (Music X Lab, NYU Shanghai); Hangkai Qian (Music X Lab, NYU Shanghai); Ziyu Wang (Music X Lab, NYU Shanghai, Mohamed Bin Zayed University of Artificial Intelligence); Gus Xia (Music X Lab, NYU Shanghai, Mohamed Bin Zayed University of Artificial Intelligence)","Qu, Yang*; Qin, Yutian; Chao, Lecheng; Qian, Hangkai; Wang, Ziyu; Xia, Gus",yangqu7-c@my.cityu.edu.hk*; yq2120@nyu.edu; lc4087@nyu.edu; hq443@nyu.edu; ziyu.wang@nyu.edu; gxia@nyu.edu,Domain knowledge -> music acoustics,"Applications -> performance, and production; Domain knowledge -> cognitive MIR; Domain knowledge -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice",No,We study the perceptual loudness of piano tone through psychoacoustic experiments and propose a piano tone loudness model for various downstream applications.,Yes,Yes,No,000112.pdf,https://archives.ismir.net/ismir2022/paper/000112.pdf,https://drive.google.com/open?id=1wrtsC8vVVCiZ0EWm4dmK8-qGRKx4MN6H,https://drive.google.com/open?id=1urtGL6vD4F9q53fyjM31PZi1CCb_R9KO,https://drive.google.com/open?id=1hLUl4F8WoLbLtUodBp3GPjEVx5a_3mWe,,https://slack.com/app_redirect?channel=C04CKB9E4F4,p7-16-qu
317,4,7,16,"In-person, in Bengaluru",FALSE,On the Impact and Interplay of Input Representations and Network Architectures for Automatic Music Tagging,"Automatic music tagging systems have once more gained relevance over the last years, not least through their use in applications such as music recommender systems.
 State-of-the-art systems are based on a variant of convolutional neural networks (CNNs) and use some type of time-frequency audio representation as input, in a fitting combination to predict semantic tags available through expert or crowd-based annotation. 
 In this work we systematically compare five widely used audio input representations (STFT, CQT, Mel spectrograms, MFCCs, and raw audio waveform) using five established convolutional neural network architectures (MusicCNN, VGG16, ResNet, a Squeeze and Excitation Network (SeNet), as well as a newly proposed MusicCNN variant using dilated convolutions) for the task of music tag prediction.
 Performance of all factor combinations are measured on two distinct tagging datasets, namely MagnaTagATune and MTG Jamendo.
 A two-way ANOVA shows that both input representation and model architecture significantly impact the classification results. Despite differently sized input representations and practical impact on model training, we find that using STFT as input representations provides the best results overall and on specific tag categories (genre, instrument, mood), while other representations show less consistent behavior in these regards.
 Furthermore, the proposed dilated convolutional architecture shows significant performance improvements for all input representations except raw waveform.",Richard Vogl,richard.vogl@tuwien.ac.at,"Maximilian Damböck (Faculty of Informatics, TU Wien, Austria); Richard Vogl (Faculty of Informatics, TU Wien, Austria)*; Peter Knees (Faculty of Informatics, TU Wien, Austria, School of Music, Georgia Institute of Technology, USA)","Damböck, Maximilian; Vogl, Richard*; Knees, Peter",maxi.damboeck@gmail.com; richard.vogl@tuwien.ac.at*; peter.knees@tuwien.ac.at,Domain knowledge -> machine learning/artificial intelligence for music,MIR tasks -> automatic classification; Musical features and properties -> musical style and genre,No,A full-factorial analysis of five neural network architectures and five input representations for music tagging shows that a dilated convolutional network using STFT features performs best.,Yes,Yes,No,000113.pdf,https://archives.ismir.net/ismir2022/paper/000113.pdf,https://drive.google.com/open?id=1d14BKzh77h8vwRgw7KP9vP-E6P-nAGpV,https://drive.google.com/open?id=1Rve9CLZFElIha7IOBKh554H9iklMqpXQ,https://drive.google.com/open?id=1usXebeP2k8G1jDAahpoTC1A82Dy9Q77o,,https://slack.com/app_redirect?channel=C04CMQWJ0J0,p7-17-vogl